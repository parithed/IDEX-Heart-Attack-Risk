{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Dropout, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')#input your path to your file\n",
    "df.drop(['Patient ID'], axis = 1, inplace= True)\n",
    "le = LabelEncoder()\n",
    "changedColumns = ['Sex','Diet','Country','Continent','Hemisphere','Physical Activity Days Per Week']#add blood pressure when a solution is found\n",
    "for i,column in enumerate(changedColumns):\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "\n",
    "df_bp = df['Blood Pressure'].str.split('/')\n",
    "sys_bp = []\n",
    "dia_bp = []\n",
    "for bp in df_bp.to_numpy():\n",
    "  sys_bp.append(int(bp[0]))\n",
    "  dia_bp.append(int(bp[1]))\n",
    "\n",
    "df['Systolic Blood Pressure'] = sys_bp\n",
    "df['Diastolic Blood Pressure'] = dia_bp\n",
    "df.drop('Blood Pressure',axis = 1, inplace = True)\n",
    "df_cluster = df.drop(['Heart Attack Risk'],axis = 1)\n",
    "df_cluster['Blood pressure diff'] = df_cluster['Systolic Blood Pressure'] - df_cluster['Diastolic Blood Pressure']\n",
    "scaler = MinMaxScaler()\n",
    "columns = df_cluster.columns\n",
    "df_cluster = scaler.fit_transform(df_cluster)\n",
    "# df_cluster_scaled = pd.DataFrame(df_cluster, columns=df)\n",
    "df_cluster_scaled = pd.DataFrame(df_cluster, columns=columns)\n",
    "df_cluster_scaled['Heart Attack Risk'] = df['Heart Attack Risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age\n",
      "Sex\n",
      "Cholesterol\n",
      "Heart Rate\n",
      "Diabetes\n",
      "Family History\n",
      "Smoking\n",
      "Obesity\n",
      "Alcohol Consumption\n",
      "Exercise Hours Per Week\n",
      "Diet\n",
      "Previous Heart Problems\n",
      "Medication Use\n",
      "Stress Level\n",
      "Sedentary Hours Per Day\n",
      "Income\n",
      "BMI\n",
      "Triglycerides\n",
      "Physical Activity Days Per Week\n",
      "Sleep Hours Per Day\n",
      "Country\n",
      "Continent\n",
      "Hemisphere\n",
      "Systolic Blood Pressure\n",
      "Diastolic Blood Pressure\n",
      "Heart Attack Risk\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "columns = df_cluster_scaled.columns\n",
    "\n",
    "columns = ['Age', 'Sex', 'Cholesterol', 'Heart Rate', 'Diabetes', 'Family History',\n",
    "       'Smoking', 'Obesity', 'Alcohol Consumption', 'Exercise Hours Per Week',\n",
    "       'Diet', 'Previous Heart Problems', 'Medication Use', 'Stress Level',\n",
    "       'Sedentary Hours Per Day', 'Income', 'BMI', 'Triglycerides',\n",
    "       'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Country',\n",
    "       'Continent', 'Hemisphere', 'Systolic Blood Pressure', 'Diastolic Blood Pressure',\n",
    "       'Heart Attack Risk']\n",
    "for col in columns:\n",
    "    print(col)\n",
    "    sns.countplot(x=col, data=cluster_1)\n",
    "    plt.show\n",
    "    # plt.hist(col)\n",
    "    # sns.displot(\n",
    "    #         cluster_1, x= col,\n",
    "    #         binwidth=2, height=3, facet_kws=dict(margin_titles=True),\n",
    "    #     )\n",
    "# sns.countplot(x='Heart Attack Risk', data=df)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df_cluster_scaled.corr()\n",
    "sns.heatmap(corr) #this is a correlation heatmap. if it is larger than 0.8 or less than -0.8, the two parameters are highly correlated, positive or negative. if is between 0.8 and 0.4, moderate corrlation. If less than 0.4 postive or negative, then it is not correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28857.742123598415, 27453.344663541255, 26596.734468110353, 25721.165350949406, 25208.662469824416, 24677.709736987745, 24244.286471899075, 23958.48111542014]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "diff_from_centroid_run1 = []\n",
    "n_trials = 10\n",
    "for i in range(2,n_trials,1):\n",
    "  # m = k_means(df_cluster_scaled, n_init = 100,n_clusters = i)\n",
    "  m = k_means(df_cluster,n_clusters=i )\n",
    "  diff_from_centroid_run1.append(m[-1])\n",
    "print(diff_from_centroid_run1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3928b8390>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(diff_from_centroid_run1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Parith/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1.52332227e-01,  5.55111512e-16,  4.85647124e-01,\n",
      "         5.10982933e-01,  6.51548673e-01,  4.75663717e-01,\n",
      "        -1.44328993e-15,  4.95575221e-01,  5.79646018e-01,\n",
      "         5.00813344e-01,  5.14380531e-01,  4.96681416e-01,\n",
      "         5.14380531e-01,  4.98279253e-01,  4.86408119e-01,\n",
      "         4.91137957e-01,  4.88577847e-01,  4.99492874e-01,\n",
      "         4.41648230e-01,  5.09218289e-01,  4.74848626e-01,\n",
      "         4.98672566e-01,  3.50663717e-01,  5.09070796e-01,\n",
      "         5.13761062e-01,  5.00916561e-01],\n",
      "       [ 5.44108163e-01,  7.70315091e-01,  5.04320659e-01,\n",
      "         4.89054726e-01,  6.43449420e-01,  5.02487562e-01,\n",
      "         1.00000000e+00,  4.44089210e-16,  6.21890547e-01,\n",
      "         5.09996013e-01,  4.95439469e-01,  1.00000000e+00,\n",
      "         4.95024876e-01,  4.93366501e-01,  5.05008650e-01,\n",
      "         4.94554107e-01,  5.14375587e-01,  4.90511727e-01,\n",
      "         4.42889718e-01,  4.99585406e-01,  5.64720258e-01,\n",
      "         4.37479270e-01,  1.66533454e-16,  4.95559241e-01,\n",
      "         5.14958541e-01,  4.91802890e-01],\n",
      "       [ 5.31030841e-01,  7.72623574e-01,  5.13609451e-01,\n",
      "         5.06007605e-01,  6.60076046e-01, -9.99200722e-16,\n",
      "         1.00000000e+00,  4.96577947e-01,  5.90114068e-01,\n",
      "         5.03699664e-01,  4.82889734e-01,  2.22044605e-16,\n",
      "         4.82129278e-01,  4.96070976e-01,  4.93168355e-01,\n",
      "         4.93700234e-01,  4.90551024e-01,  5.09358550e-01,\n",
      "         4.33174905e-01,  5.03548796e-01,  5.57894737e-01,\n",
      "         4.29353612e-01,  1.11022302e-16,  4.97634136e-01,\n",
      "         4.91513308e-01,  5.01510049e-01],\n",
      "       [ 5.37626082e-01,  7.80557541e-01,  4.90753600e-01,\n",
      "         5.06331053e-01,  6.42601858e-01,  4.97498213e-01,\n",
      "         1.00000000e+00,  5.55111512e-16,  6.01858470e-01,\n",
      "         4.98293856e-01,  5.05360972e-01,  5.08934954e-01,\n",
      "         5.20371694e-01,  4.90032563e-01,  5.02821252e-01,\n",
      "         5.01607878e-01,  4.99204316e-01,  5.01239290e-01,\n",
      "         4.27626876e-01,  5.09888015e-01,  3.84898988e-01,\n",
      "         5.79556826e-01,  1.00000000e+00,  5.05885156e-01,\n",
      "         4.96783417e-01,  5.04932094e-01],\n",
      "       [ 5.29770488e-01,  7.68565249e-01,  4.98491091e-01,\n",
      "         4.98321145e-01,  6.59697188e-01,  4.89545782e-01,\n",
      "         1.00000000e+00,  1.00000000e+00,  5.83273252e-01,\n",
      "         4.95766080e-01,  4.98197549e-01,  5.07570296e-01,\n",
      "         4.90266763e-01,  4.95233518e-01,  5.05039544e-01,\n",
      "         4.86795808e-01,  4.91268636e-01,  4.97581438e-01,\n",
      "         4.34751262e-01,  4.96034607e-01,  3.66713467e-01,\n",
      "         5.79091565e-01,  1.00000000e+00,  5.00777057e-01,\n",
      "         5.08781543e-01,  4.97363271e-01],\n",
      "       [ 5.36352374e-01,  7.87949922e-01,  5.05164319e-01,\n",
      "         4.94410910e-01,  6.37715180e-01,  1.00000000e+00,\n",
      "         1.00000000e+00,  4.94522692e-01,  6.12676056e-01,\n",
      "         4.92065920e-01,  5.12519562e-01,  1.11022302e-16,\n",
      "         4.95305164e-01,  5.07563902e-01,  5.04812327e-01,\n",
      "         4.99681939e-01,  4.82606900e-01,  5.19441904e-01,\n",
      "         4.37500000e-01,  5.01695357e-01,  5.60662219e-01,\n",
      "         4.13928013e-01,  0.00000000e+00,  5.02564771e-01,\n",
      "         5.04178404e-01,  5.00156495e-01],\n",
      "       [ 5.34820338e-01,  7.85714286e-01,  4.95646445e-01,\n",
      "         4.98979592e-01,  6.70329670e-01,  4.95290424e-01,\n",
      "         1.00000000e+00,  1.00000000e+00,  5.94191523e-01,\n",
      "         5.05298938e-01,  4.87048666e-01,  1.00000000e+00,\n",
      "         4.94505495e-01,  4.96947497e-01,  4.94759982e-01,\n",
      "         4.87995961e-01,  4.98200174e-01,  5.05365043e-01,\n",
      "         4.39560440e-01,  5.08895866e-01,  5.64818640e-01,\n",
      "         4.48037677e-01,  5.55111512e-17,  4.96110239e-01,\n",
      "         4.96091052e-01,  4.98895492e-01]]), array([3, 6, 0, ..., 4, 3, 0], dtype=int32), 24485.42796874626)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = k_means(df_cluster,n_clusters = 7,random_state=69)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1= df_cluster_scaled.iloc[np.where(best_model[1] == 0)]\n",
    "cluster_2 = df_cluster_scaled.iloc[np.where(best_model[1]==1)]\n",
    "cluster_3= df_cluster_scaled.iloc[np.where(best_model[1] == 2)]\n",
    "cluster_4 = df_cluster_scaled.iloc[np.where(best_model[1]==3)]\n",
    "cluster_5 = df_cluster_scaled.iloc[np.where(best_model[1]==4)]\n",
    "cluster_6 = df_cluster_scaled.iloc[np.where(best_model[1]==5)]\n",
    "cluster_7 = df_cluster_scaled.iloc[np.where(best_model[1]==6)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1= df.iloc[np.where(best_model[1] == 0)]\n",
    "cluster_2 = df.iloc[np.where(best_model[1]==1)]\n",
    "cluster_3= df.iloc[np.where(best_model[1] == 2)]\n",
    "cluster_4 = df.iloc[np.where(best_model[1]==3)]\n",
    "cluster_5 = df.iloc[np.where(best_model[1]==4)]\n",
    "cluster_6 = df.iloc[np.where(best_model[1]==5)]\n",
    "cluster_7 = df.iloc[np.where(best_model[1]==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Family History</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Alcohol Consumption</th>\n",
       "      <th>Exercise Hours Per Week</th>\n",
       "      <th>...</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Triglycerides</th>\n",
       "      <th>Physical Activity Days Per Week</th>\n",
       "      <th>Sleep Hours Per Day</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Hemisphere</th>\n",
       "      <th>Heart Attack Risk</th>\n",
       "      <th>Systolic Blood Pressure</th>\n",
       "      <th>Diastolic Blood Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.0</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.0</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>904.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.967920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.981195</td>\n",
       "      <td>75.768805</td>\n",
       "      <td>0.651549</td>\n",
       "      <td>0.475664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495575</td>\n",
       "      <td>0.579646</td>\n",
       "      <td>10.016840</td>\n",
       "      <td>...</td>\n",
       "      <td>28.748545</td>\n",
       "      <td>414.609513</td>\n",
       "      <td>3.533186</td>\n",
       "      <td>7.055310</td>\n",
       "      <td>9.022124</td>\n",
       "      <td>2.493363</td>\n",
       "      <td>0.350664</td>\n",
       "      <td>0.363938</td>\n",
       "      <td>135.816372</td>\n",
       "      <td>85.688053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.604478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.842989</td>\n",
       "      <td>20.924022</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>0.499684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500257</td>\n",
       "      <td>0.493889</td>\n",
       "      <td>5.692711</td>\n",
       "      <td>...</td>\n",
       "      <td>6.153228</td>\n",
       "      <td>228.871499</td>\n",
       "      <td>2.345799</td>\n",
       "      <td>1.957815</td>\n",
       "      <td>5.820074</td>\n",
       "      <td>1.599405</td>\n",
       "      <td>0.477442</td>\n",
       "      <td>0.481397</td>\n",
       "      <td>26.585984</td>\n",
       "      <td>14.840609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>...</td>\n",
       "      <td>18.016191</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.114021</td>\n",
       "      <td>...</td>\n",
       "      <td>23.455808</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.004732</td>\n",
       "      <td>...</td>\n",
       "      <td>28.442002</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.019706</td>\n",
       "      <td>...</td>\n",
       "      <td>34.180273</td>\n",
       "      <td>614.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.990822</td>\n",
       "      <td>...</td>\n",
       "      <td>39.970515</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age    Sex  Cholesterol  Heart Rate    Diabetes  Family History  \\\n",
       "count  904.000000  904.0   904.000000  904.000000  904.000000      904.000000   \n",
       "mean    28.967920    0.0   255.981195   75.768805    0.651549        0.475664   \n",
       "std      6.604478    0.0    80.842989   20.924022    0.476744        0.499684   \n",
       "min     18.000000    0.0   120.000000   40.000000    0.000000        0.000000   \n",
       "25%     23.000000    0.0   188.000000   57.000000    0.000000        0.000000   \n",
       "50%     29.000000    0.0   250.000000   77.000000    1.000000        0.000000   \n",
       "75%     35.000000    0.0   328.000000   95.000000    1.000000        1.000000   \n",
       "max     40.000000    0.0   400.000000  110.000000    1.000000        1.000000   \n",
       "\n",
       "       Smoking     Obesity  Alcohol Consumption  Exercise Hours Per Week  ...  \\\n",
       "count    904.0  904.000000           904.000000               904.000000  ...   \n",
       "mean       0.0    0.495575             0.579646                10.016840  ...   \n",
       "std        0.0    0.500257             0.493889                 5.692711  ...   \n",
       "min        0.0    0.000000             0.000000                 0.013836  ...   \n",
       "25%        0.0    0.000000             0.000000                 5.114021  ...   \n",
       "50%        0.0    0.000000             1.000000                10.004732  ...   \n",
       "75%        0.0    1.000000             1.000000                15.019706  ...   \n",
       "max        0.0    1.000000             1.000000                19.990822  ...   \n",
       "\n",
       "              BMI  Triglycerides  Physical Activity Days Per Week  \\\n",
       "count  904.000000     904.000000                       904.000000   \n",
       "mean    28.748545     414.609513                         3.533186   \n",
       "std      6.153228     228.871499                         2.345799   \n",
       "min     18.016191      30.000000                         0.000000   \n",
       "25%     23.455808     218.000000                         1.000000   \n",
       "50%     28.442002     399.000000                         4.000000   \n",
       "75%     34.180273     614.750000                         6.000000   \n",
       "max     39.970515     800.000000                         7.000000   \n",
       "\n",
       "       Sleep Hours Per Day     Country   Continent  Hemisphere  \\\n",
       "count           904.000000  904.000000  904.000000  904.000000   \n",
       "mean              7.055310    9.022124    2.493363    0.350664   \n",
       "std               1.957815    5.820074    1.599405    0.477442   \n",
       "min               4.000000    0.000000    0.000000    0.000000   \n",
       "25%               5.000000    4.000000    1.000000    0.000000   \n",
       "50%               7.000000    9.000000    3.000000    0.000000   \n",
       "75%               9.000000   14.000000    4.000000    1.000000   \n",
       "max              10.000000   19.000000    5.000000    1.000000   \n",
       "\n",
       "       Heart Attack Risk  Systolic Blood Pressure  Diastolic Blood Pressure  \n",
       "count         904.000000               904.000000                904.000000  \n",
       "mean            0.363938               135.816372                 85.688053  \n",
       "std             0.481397                26.585984                 14.840609  \n",
       "min             0.000000                90.000000                 60.000000  \n",
       "25%             0.000000               113.000000                 73.000000  \n",
       "50%             0.000000               136.000000                 86.000000  \n",
       "75%             1.000000               160.000000                 99.000000  \n",
       "max             1.000000               180.000000                110.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_scaled_describe = df_cluster_scaled.describe()\n",
    "def percentageMeanSTD(df_ryogi):\n",
    "    meanData = []\n",
    "    stdData = []\n",
    "    \n",
    "    for i in range(len(df_ryogi.columns)):\n",
    "        meanData.append((df_cluster_scaled_describe.iloc[1,i]-df_ryogi.iloc[1,i])/df_cluster_scaled_describe.iloc[1,i]*100)\n",
    "        stdData.append((df_cluster_scaled_describe.iloc[2,i]-df_ryogi.iloc[2,i])/df_cluster_scaled_describe.iloc[2,i]*100)\n",
    "    print(meanData)\n",
    "    print(stdData)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69.2843970413293, 100.0, 2.7853116906306337, -2.1333162577440423, 0.11334818580974386, 3.512936332350041, 100.0, 1.1669170751749094, 3.082654968568104, -0.02552860521502428, -3.1232347957070337, -0.17075368901290747, -3.217691617136583, -0.3313669909335497, 2.6112610210091733, 0.5322651471318468, 1.3123296491788166, 0.7912611680333013, -1.2237470890374729, -1.0518180922431442, 3.837608638931668, -2.656166583232593, 0.9711198697235492, -1.6432652935578849, -2.114564441617751, -0.29858671827587663, -1.5988900698322754]\n",
      "[68.91938682553676, 100.0, 0.02508874634797587, -1.8153630360552737, -0.09918165561406293, 0.05908794474134616, 100.0, -0.04613592204335545, -0.7292559540879585, 1.5739696072530254, 0.06623937328180338, -0.05091448872912988, -0.008805151336584435, -0.4024637530873517, -1.6653265878078076, 1.548447913970586, 2.6261919721526463, -2.2897897968253504, -2.748893160620255, 1.5417770819709853, -0.5373993372032037, -0.11540656995807595, 0.17265559442622705, -0.8956640799553179, -1.1177262558741659, -0.15344549701596663, -0.3953267052247541]\n",
      "[-9.711586391154134, -10.460990742572521, -0.952673706419915, 2.2496098986648376, 1.3550163806579547, -1.9282061912659065, -11.502735717012348, 100.0, -3.980669064522834, -1.8595478357660633, 0.6740775645906828, -101.6800920598389, 0.666292991192415, 0.6578435945273462, -1.1129413251864657, -0.15959058567608803, -3.8985470016780925, 2.575087010059114, -1.5082904371788628, 0.8597792317897236, -14.362446306949256, 9.941015650931563, 100.0, 1.054510031022541, -2.3525739429759445, 1.5262447849214364, -3.240249849826995]\n",
      "[3.511420256553109, 8.406281088188077, 0.21393798710577297, 1.128485600888718, -0.6105687711320348, -0.04439451790706755, 100.0, 100.0, 1.0599443570565228, 0.653369674232423, -0.8015616160717707, 100.0, -0.031372335655995794, 2.2509211330223864, 0.8465806038778543, 0.589329863390037, 0.6012965871176001, -0.11576782511592269, 1.1653512732097624, -0.9233466755640861, 5.293652803649299, 5.135334216476712, 100.0, -0.2268517003192545, 0.3460900759962371, -0.10384105442519172, -0.7202429213204204]\n",
      "[-7.074732438093767, -10.792020622289913, -2.812063065091721, -1.1388666204496414, -1.1939536009919398, 100.0, -11.502735717012348, 0.9669424777306, 1.3323872972896451, -0.6020022190893656, 3.1900540461545237, 100.0, 3.253976200696365, 0.11328212368596602, 1.2577251691442324, 0.01333989573550159, 0.913768241364798, -1.1682487775752692, 0.7183001401848115, 0.0732644153391468, -12.98019850150823, 11.613754383056444, 100.0, 0.6402275281289999, 2.3073738365309167, -0.417420810162292, 0.6468602410251508]\n",
      "[1.525118602027543, 8.734420098137136, -0.2189473661158943, 0.06653489323795579, 0.5055723086926013, 100.0, 100.0, -0.030400574577288512, -0.34389592017521986, -0.3935394703966576, -0.2992562212264594, 100.0, 0.031029817201609404, 0.9258501624830875, -1.0417816819007961, 0.6959915197756944, -0.29012569728466303, -0.23446376238748673, 0.3785405123284329, 2.4373581499008474, 7.095518624192465, 5.660559652914058, 100.0, -1.0885299833796709, 0.7490709796737898, -1.1631296517704899, 0.11189155413207152]\n",
      "[-8.404568021316534, -11.929728893244798, 1.7631200396364815, -1.2035162168293254, 1.484953013023912, -0.9161305694543801, -11.502735717012348, 100.0, -0.6312874557139467, 0.4776790285255839, -1.3149896754373513, -2.6420482820623943, -4.419902796216938, 1.3291509866991815, -0.674980135619978, -1.5881559292501815, -0.8341073043253532, 0.4443900392798509, 1.9898828294491238, -1.1847218937960182, 22.05346063435266, -19.306908215240597, -182.40412504028362, -1.0072066270919393, 1.2598930160635435, -1.1026174087499752, 1.2245582962025665]\n",
      "[2.517416680316282, 9.883733120699157, 0.14043242811417317, -0.18341474530865562, -0.6579539104323934, -0.038653965789559694, 100.0, 100.0, 0.12714269102010178, -0.9203957344637548, -0.15990354942069426, -0.017548735864706295, 0.05246232721194592, -1.1066872264268752, 0.15249611388066597, -1.6931428524427088, -1.378544904241039, 0.390876231729875, -1.2157973115396274, 0.777752162590606, 0.7531653338183975, -2.551624856916928, 100.0, 0.5745899566155845, -0.18934551175012115, 1.5559747948283176, 0.2451343239492585]\n",
      "[-6.820600407991338, -10.210068314409797, 0.2142633466169813, 0.397473482360867, -1.1358722880612349, 0.6969979171673483, -11.502735717012348, -99.43104233045064, 2.4761781346042064, 0.9825419618183353, 0.12113660577012554, -2.3668239438547767, 1.6210752829481268, 0.2819091317132198, -1.1191270432866123, 1.4116590980992618, 0.7688179442186347, 1.170908605954185, 0.3570064035903802, 1.564417549365536, 25.7362409490705, -19.21112995575882, -182.40412504028362, 0.012698448021847747, -1.1248408980591023, 0.41288901455445953, 3.791613427648039]\n",
      "[4.214173035022483, 8.167501283392161, 0.7306203155960984, 1.0843009654898783, 0.4806814019048344, -0.018346504928455316, 100.0, 100.0, -0.5876377179214078, -0.06896325106170963, 0.05156305893701853, -0.022365171339120707, -0.011953435506349091, -1.4030365399534606, -0.6890664340741159, -0.576658658564502, -1.7577175398358058, -1.0575467384607307, 0.946687273242687, -1.1063937711750793, 1.3921487829164583, -4.07937862573636, 100.0, 1.1081034353167243, 0.008534316096605793, -0.8851782758944715, 0.8515884820052999]\n",
      "[-8.14774299453213, -12.989775230228261, -1.121553952676092, 1.179036398725462, 2.234112630060755, -102.84722222222223, -11.502735717012348, 1.376824137054152, -2.439997742609062, 1.7215607730384455, -2.75014688144116, 100.0, 0.6100491200005146, -2.2008840214489624, -1.073633465614037, -1.198105098915034, 2.518399148234826, -3.1709936823100957, -0.2729917938993628, 0.4410700220698882, -13.54064593848977, 14.789250758576394, 100.0, -0.3442443293288678, -0.209926196628529, -0.14639844792005358, -0.04516853697313858]\n",
      "[4.048233022389736, 10.993108535775546, -0.11908596416004699, -0.5999041451512701, -0.9611101694089331, 100.0, 100.0, -0.027843100132028335, 0.6085836239091941, 0.3870045996603355, 0.15030325709185818, 100.0, -0.029576292458005367, -0.24384711154076424, 0.5423687029592632, 0.2741752355049702, 0.1863127857345069, 0.4321458559175981, 0.6373551419270844, -0.3319636547926756, 8.12951257745403, 7.496974221172772, 100.0, -1.0617909556863825, 0.28370892046900237, 0.5554472006860293, -0.04341454638105865]\n",
      "[-7.838830146639393, -12.66919138789536, 0.783691841851499, 0.26586571023593697, -2.765900998931076, -0.46828667364382637, -11.502735717012348, -99.43104233045064, 0.6506332003101977, -0.921419044538197, 2.3562695760950487, -101.6800920598389, 0.7705141206400574, -0.06321038030598185, 0.9390491880561729, 1.1685980241732226, -0.6312810341859374, -0.3750626867665998, -0.7452350833525763, -0.9878348540629828, -14.382369739686382, 7.767474156042358, 100.0, 0.944495373038597, 1.3975066804077956, 0.1060921947925556, -1.8931575865085144]\n",
      "[5.593887730044961, 10.652053605511613, -0.7563504909403637, -0.25280677663426376, 1.2583550724273358, -0.038978962066163234, 100.0, 100.0, -0.18912185160029465, -0.7156560736943886, 1.010024772562398, 100.0, -0.02806899966565518, 0.08812471053018976, 1.464700301673346, -0.2498857311369846, 1.301106078365698, 2.385565829568509, 0.038230161342616385, -2.1318334693183263, 4.789616906147508, 5.059583839167779, 100.0, 1.0755723925934266, -0.3407959579216503, 0.004929846147368226, -0.4411179260785259]\n"
     ]
    }
   ],
   "source": [
    "cluster_1_describe = cluster_1.describe()\n",
    "cluster_2_describe = cluster_2.describe()\n",
    "cluster_3_describe = cluster_3.describe()\n",
    "cluster_4_describe = cluster_4.describe()\n",
    "cluster_5_describe = cluster_5.describe()\n",
    "cluster_6_describe = cluster_6.describe()\n",
    "cluster_7_describe = cluster_7.describe()\n",
    "percentageMeanSTD(cluster_1_describe)\n",
    "percentageMeanSTD(cluster_2_describe)\n",
    "percentageMeanSTD(cluster_3_describe)\n",
    "percentageMeanSTD(cluster_4_describe)\n",
    "percentageMeanSTD(cluster_5_describe)\n",
    "percentageMeanSTD(cluster_6_describe)\n",
    "percentageMeanSTD(cluster_7_describe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meanData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(meanData)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meanData' is not defined"
     ]
    }
   ],
   "source": [
    "print(meanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each model once\n",
    "rf_1_label = np.array(cluster_1['Heart Attack Risk'])\n",
    "rf_1 = cluster_1.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_1, X_test_rf_1, y_train_rf_1, y_test_rf_1 = train_test_split(rf_1, rf_1_label,random_state=63)\n",
    "X_train_rf_1 = np.array(X_train_rf_1)\n",
    "X_test_rf_1 = np.array(X_test_rf_1)\n",
    "\n",
    "rf_2_label = np.array(cluster_2['Heart Attack Risk'])\n",
    "rf_2 = cluster_2.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_2, X_test_rf_2, y_train_rf_2, y_test_rf_2 = train_test_split(rf_2, rf_2_label,random_state=64)\n",
    "X_train_rf_2 = np.array(X_train_rf_2)\n",
    "X_test_rf_2 = np.array(X_test_rf_2)\n",
    "\n",
    "rf_3_label = np.array(cluster_3['Heart Attack Risk'])\n",
    "rf_3 = cluster_3.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_3, X_test_rf_3, y_train_rf_3, y_test_rf_3 = train_test_split(rf_3, rf_3_label,random_state=65)\n",
    "X_train_rf_3 = np.array(X_train_rf_3)\n",
    "X_test_rf_3 = np.array(X_test_rf_3)\n",
    "\n",
    "rf_4_label = np.array(cluster_4['Heart Attack Risk'])\n",
    "rf_4 = cluster_4.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_4, X_test_rf_4, y_train_rf_4, y_test_rf_4 = train_test_split(rf_4, rf_4_label,random_state=66)\n",
    "X_train_rf_4 = np.array(X_train_rf_4)\n",
    "X_test_rf_4 = np.array(X_test_rf_4)\n",
    "\n",
    "rf_5_label = np.array(cluster_5['Heart Attack Risk'])\n",
    "rf_5 = cluster_5.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_5, X_test_rf_5, y_train_rf_5, y_test_rf_5 = train_test_split(rf_5, rf_5_label,random_state=67)\n",
    "X_train_rf_5 = np.array(X_train_rf_5)\n",
    "X_test_rf_5 = np.array(X_test_rf_5)\n",
    "\n",
    "rf_6_label = np.array(cluster_6['Heart Attack Risk'])\n",
    "rf_6 = cluster_6.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_6, X_test_rf_6, y_train_rf_6, y_test_rf_6 = train_test_split(rf_6, rf_6_label,random_state=68)\n",
    "X_train_rf_6 = np.array(X_train_rf_6)\n",
    "X_test_rf_6 = np.array(X_test_rf_6)\n",
    "\n",
    "rf_7_label = np.array(cluster_7['Heart Attack Risk'])\n",
    "rf_7 = cluster_7.drop('Heart Attack Risk', axis = 1)\n",
    "X_train_rf_7, X_test_rf_7, y_train_rf_7, y_test_rf_7 = train_test_split(rf_7, rf_7_label,random_state=69)\n",
    "X_train_rf_7 = np.array(X_train_rf_7)\n",
    "X_test_rf_7 = np.array(X_test_rf_7)\n",
    "\n",
    "rf_label = np.array(df['Heart Attack Risk'])\n",
    "rf_data = df_cluster_scaled.drop('Heart Attack Risk',axis = 1)\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf= train_test_split(rf_data, rf_label,random_state=70)\n",
    "X_train_rf = np.array(X_train_rf)\n",
    "X_test_rf = np.array(X_test_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.626419</td>\n",
       "      <td>0.016289</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.552947</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.010592</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.859746</td>\n",
       "      <td>0.025674</td>\n",
       "      <td>0.032911</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.670874</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.252703</td>\n",
       "      <td>0.027096</td>\n",
       "      <td>0.022311</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.605955</td>\n",
       "      <td>0.006407</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.647174</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518484</td>\n",
       "      <td>0.019726</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.305815</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.613640</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.010133</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.657582</td>\n",
       "      <td>0.036361</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.223818</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.019071</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.860947</td>\n",
       "      <td>0.028020</td>\n",
       "      <td>0.028851</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.792798</td>\n",
       "      <td>0.037623</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.154050</td>\n",
       "      <td>0.031114</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.590734</td>\n",
       "      <td>0.019741</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.010223</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.245837</td>\n",
       "      <td>0.038209</td>\n",
       "      <td>0.019304</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.314824</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.703021</td>\n",
       "      <td>0.029814</td>\n",
       "      <td>0.026244</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.153359</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.574738</td>\n",
       "      <td>0.011219</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.123197</td>\n",
       "      <td>0.037663</td>\n",
       "      <td>0.020483</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.546024</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.027944</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.025708</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.933742</td>\n",
       "      <td>0.013932</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.174698</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6        0.626419      0.016289         0.011621        0.000278   \n",
       "3        0.552947      0.005973         0.010592        0.000233   \n",
       "8        1.859746      0.025674         0.032911        0.000969   \n",
       "24       0.670874      0.019361         0.010963        0.000355   \n",
       "7        1.252703      0.027096         0.022311        0.000463   \n",
       "21       0.605955      0.006407         0.010056        0.000144   \n",
       "15       0.647174      0.002953         0.010491        0.000173   \n",
       "0        0.518484      0.019726         0.010319        0.000870   \n",
       "16       1.305815      0.007626         0.020441        0.000309   \n",
       "12       0.613640      0.013968         0.010133        0.000282   \n",
       "5        1.657582      0.036361         0.029914        0.000737   \n",
       "22       1.223818      0.008076         0.019071        0.000402   \n",
       "23       1.860947      0.028020         0.028851        0.000348   \n",
       "20       1.792798      0.037623         0.027890        0.001219   \n",
       "19       1.154050      0.031114         0.018382        0.000715   \n",
       "18       0.590734      0.019741         0.010022        0.000722   \n",
       "17       2.010223      0.037579         0.031564        0.001277   \n",
       "13       1.245837      0.038209         0.019304        0.001130   \n",
       "25       1.314824      0.009119         0.020969        0.000589   \n",
       "11       1.703021      0.029814         0.026244        0.000502   \n",
       "10       1.153359      0.023283         0.018498        0.000648   \n",
       "9        0.574738      0.011219         0.009708        0.000593   \n",
       "4        1.123197      0.037663         0.020483        0.000349   \n",
       "2        1.546024      0.015003         0.027944        0.001088   \n",
       "1        1.025708      0.020319         0.018717        0.000261   \n",
       "14       1.933742      0.013932         0.029611        0.000927   \n",
       "26       2.174698      0.153490         0.033342        0.003099   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "6             gini              12                100   \n",
       "3             gini              11                100   \n",
       "8             gini              12                300   \n",
       "24        log_loss              12                100   \n",
       "7             gini              12                200   \n",
       "21        log_loss              11                100   \n",
       "15         entropy              12                100   \n",
       "0             gini              10                100   \n",
       "16         entropy              12                200   \n",
       "12         entropy              11                100   \n",
       "5             gini              11                300   \n",
       "22        log_loss              11                200   \n",
       "23        log_loss              11                300   \n",
       "20        log_loss              10                300   \n",
       "19        log_loss              10                200   \n",
       "18        log_loss              10                100   \n",
       "17         entropy              12                300   \n",
       "13         entropy              11                200   \n",
       "25        log_loss              12                200   \n",
       "11         entropy              10                300   \n",
       "10         entropy              10                200   \n",
       "9          entropy              10                100   \n",
       "4             gini              11                200   \n",
       "2             gini              10                300   \n",
       "1             gini              10                200   \n",
       "14         entropy              11                300   \n",
       "26        log_loss              12                300   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.004274   \n",
       "3   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.004274   \n",
       "8   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.004274   \n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.002137   \n",
       "7   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.004274   \n",
       "21  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.000000   \n",
       "15  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.002137   \n",
       "0   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.000000   \n",
       "16  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.000000   \n",
       "12  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.002137   \n",
       "5   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.002137   \n",
       "22  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.000000   \n",
       "23  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.000000   \n",
       "20  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.000000   \n",
       "19  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.000000   \n",
       "18  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.000000   \n",
       "17  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.000000   \n",
       "13  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.000000   \n",
       "25  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.000000   \n",
       "11  {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.000000   \n",
       "10  {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.000000   \n",
       "9   {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.000000   \n",
       "4   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.000000   \n",
       "2   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.000000   \n",
       "1   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.000000   \n",
       "14  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.000000   \n",
       "26  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.000000   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "6            0.002137           0.002137           0.008547   \n",
       "3            0.004274           0.004274           0.002137   \n",
       "8            0.002137           0.002137           0.000000   \n",
       "24           0.000000           0.004274           0.002137   \n",
       "7            0.000000           0.002137           0.002137   \n",
       "21           0.002137           0.002137           0.000000   \n",
       "15           0.000000           0.000000           0.000000   \n",
       "0            0.000000           0.002137           0.002137   \n",
       "16           0.000000           0.000000           0.004274   \n",
       "12           0.000000           0.000000           0.000000   \n",
       "5            0.000000           0.000000           0.000000   \n",
       "22           0.000000           0.000000           0.000000   \n",
       "23           0.000000           0.000000           0.000000   \n",
       "20           0.000000           0.000000           0.000000   \n",
       "19           0.000000           0.000000           0.000000   \n",
       "18           0.000000           0.000000           0.000000   \n",
       "17           0.000000           0.000000           0.000000   \n",
       "13           0.000000           0.000000           0.000000   \n",
       "25           0.000000           0.000000           0.000000   \n",
       "11           0.000000           0.000000           0.000000   \n",
       "10           0.000000           0.000000           0.000000   \n",
       "9            0.000000           0.000000           0.000000   \n",
       "4            0.000000           0.000000           0.000000   \n",
       "2            0.000000           0.000000           0.000000   \n",
       "1            0.000000           0.000000           0.000000   \n",
       "14           0.000000           0.000000           0.000000   \n",
       "26           0.000000           0.000000           0.000000   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "6            0.004274         0.004274        0.002341                1  \n",
       "3            0.004274         0.003846        0.000855                2  \n",
       "8            0.000000         0.001709        0.001599                3  \n",
       "24           0.000000         0.001709        0.001599                3  \n",
       "7            0.000000         0.001709        0.001599                3  \n",
       "21           0.000000         0.000855        0.001047                6  \n",
       "15           0.002137         0.000855        0.001047                6  \n",
       "0            0.000000         0.000855        0.001047                6  \n",
       "16           0.000000         0.000855        0.001709                6  \n",
       "12           0.000000         0.000427        0.000855               10  \n",
       "5            0.000000         0.000427        0.000855               10  \n",
       "22           0.000000         0.000000        0.000000               12  \n",
       "23           0.000000         0.000000        0.000000               12  \n",
       "20           0.000000         0.000000        0.000000               12  \n",
       "19           0.000000         0.000000        0.000000               12  \n",
       "18           0.000000         0.000000        0.000000               12  \n",
       "17           0.000000         0.000000        0.000000               12  \n",
       "13           0.000000         0.000000        0.000000               12  \n",
       "25           0.000000         0.000000        0.000000               12  \n",
       "11           0.000000         0.000000        0.000000               12  \n",
       "10           0.000000         0.000000        0.000000               12  \n",
       "9            0.000000         0.000000        0.000000               12  \n",
       "4            0.000000         0.000000        0.000000               12  \n",
       "2            0.000000         0.000000        0.000000               12  \n",
       "1            0.000000         0.000000        0.000000               12  \n",
       "14           0.000000         0.000000        0.000000               12  \n",
       "26           0.000000         0.000000        0.000000               12  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],#\n",
    "    'n_estimators' : [100,200,300], #\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf , y_train_rf)\n",
    "best_rf = clf.best_estimator_\n",
    "df_clf_lol = pd.DataFrame(clf.cv_results_)\n",
    "df_clf_lol.sort_values(by = 'rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_rf = pd.DataFrame(X_train_rf)\n",
    "\n",
    "# Create a DataFrame with feature importances and index\n",
    "\n",
    "allData_importantFeatures = pd.DataFrame(best_rf.feature_importances_,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "allData_importantFeatures.sort_values('importance', ascending = False, inplace=True)\n",
    "allData_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079637</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.045451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.172477</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.042809</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.233764</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.053218</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.073469</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.077457</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.073469</td>\n",
       "      <td>0.027683</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.079637      0.005991         0.003659        0.000144   \n",
       "16       0.172477      0.001978         0.006527        0.000070   \n",
       "8        0.233764      0.001105         0.009507        0.000108   \n",
       "22       0.168000      0.000747         0.006482        0.000200   \n",
       "6        0.077457      0.000468         0.004185        0.001318   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "0             gini              10                100   \n",
       "16         entropy              12                200   \n",
       "8             gini              12                300   \n",
       "22        log_loss              11                200   \n",
       "6             gini              12                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.081633   \n",
       "16  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.102041   \n",
       "8   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.061224   \n",
       "22  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.061224   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.040816   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.040816           0.040816           0.163265   \n",
       "16           0.020408           0.040816           0.122449   \n",
       "8            0.040816           0.163265           0.122449   \n",
       "22           0.061224           0.040816           0.142857   \n",
       "6            0.040816           0.081633           0.102041   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.102041         0.085714        0.045451                1  \n",
       "16           0.122449         0.081633        0.042809                2  \n",
       "8            0.020408         0.081633        0.053218                2  \n",
       "22           0.061224         0.073469        0.035583                4  \n",
       "6            0.102041         0.073469        0.027683                5  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],#\n",
    "    'n_estimators' : [100,200,300], #\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_1 , y_train_rf_1)\n",
    "best_rf_1 = clf.best_estimator_\n",
    "df_clf_lol = pd.DataFrame(clf.cv_results_)\n",
    "df_clf_lol.sort_values(by = 'rank_test_score', inplace = True)\n",
    "df_clf_lol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_1.feature_importances_\n",
    "\n",
    "\n",
    "cluster_1_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "cluster_1_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_1_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.092331</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.074763</td>\n",
       "      <td>0.028177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.100566</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.074717</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.094809</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.276918</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.010091</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>0.043582</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.190178</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.062777</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3        0.092331      0.002756         0.003999        0.000242   \n",
       "12       0.100566      0.000473         0.003604        0.000048   \n",
       "6        0.094809      0.006412         0.003881        0.000199   \n",
       "8        0.276918      0.001580         0.010091        0.000274   \n",
       "19       0.190178      0.001020         0.006490        0.000049   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "3             gini              11                100   \n",
       "12         entropy              11                100   \n",
       "6             gini              12                100   \n",
       "8             gini              12                300   \n",
       "19        log_loss              10                200   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "3   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.044776   \n",
       "12  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.029851   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.029851   \n",
       "8   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.029851   \n",
       "19  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.014925   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "3            0.089552           0.119403           0.074627   \n",
       "12           0.089552           0.134328           0.089552   \n",
       "6            0.089552           0.134328           0.044776   \n",
       "8            0.149254           0.074627           0.074627   \n",
       "19           0.119403           0.059701           0.089552   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "3            0.045455         0.074763        0.028177                1  \n",
       "12           0.030303         0.074717        0.039948                2  \n",
       "6            0.060606         0.071823        0.036987                3  \n",
       "8            0.030303         0.071732        0.043582                4  \n",
       "19           0.030303         0.062777        0.038150                5  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],#\n",
    "    'n_estimators' : [100,200,300], #\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_2 , y_train_rf_2)\n",
    "best_rf_2 = clf.best_estimator_\n",
    "df_clf_lol = pd.DataFrame(clf.cv_results_)\n",
    "df_clf_lol.sort_values(by = 'rank_test_score', inplace = True)\n",
    "df_clf_lol.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_2.feature_importances_\n",
    "\n",
    "\n",
    "cluster_1_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "cluster_1_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_1_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_3, y_train_rf_3)\n",
    "df_clf_3 = pd.DataFrame(clf.cv_results_)\n",
    "df_clf_3.sort_values(by = 'rank_test_score', inplace = True)\n",
    "df_clf_3.head()\n",
    "best_rf_3 = clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.116640</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.054741</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.099874</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.052008</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.106688</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.052008</td>\n",
       "      <td>0.025374</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098252</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.049068</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.114811</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.048903</td>\n",
       "      <td>0.023183</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "15       0.116640      0.001374         0.004145        0.000104   \n",
       "6        0.099874      0.000675         0.004050        0.000070   \n",
       "12       0.106688      0.000941         0.003810        0.000047   \n",
       "0        0.098252      0.007615         0.003911        0.000187   \n",
       "24       0.114811      0.001562         0.004080        0.000163   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "15         entropy              12                100   \n",
       "6             gini              12                100   \n",
       "12         entropy              11                100   \n",
       "0             gini              10                100   \n",
       "24        log_loss              12                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "15  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.028571   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.014286   \n",
       "12  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.014286   \n",
       "0   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.014286   \n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.042857   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "15           0.043478           0.043478           0.072464   \n",
       "6            0.057971           0.086957           0.057971   \n",
       "12           0.043478           0.072464           0.086957   \n",
       "0            0.057971           0.043478           0.072464   \n",
       "24           0.014493           0.043478           0.057971   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "15           0.085714         0.054741        0.021033                1  \n",
       "6            0.042857         0.052008        0.023661                2  \n",
       "12           0.042857         0.052008        0.025374                2  \n",
       "0            0.057143         0.049068        0.019662                4  \n",
       "24           0.085714         0.048903        0.023183                5  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clf_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_3.feature_importances_\n",
    "\n",
    "\n",
    "cluster_1_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "cluster_1_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_1_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_3.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature importances and index\n",
    "\n",
    "cluster_3_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_3_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101674</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.024252</td>\n",
       "      <td>0.010094</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.192876</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.024771</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.294165</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.010552</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100301</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.043207</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194020</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.026955</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.293342</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.013248</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100763</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.037694</td>\n",
       "      <td>0.015613</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.200776</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.307090</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.020225</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.108728</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.212403</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.324103</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.010178</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'n_e...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.114296</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.224311</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.026919</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.342893</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013477</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.040396</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.234656</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.026955</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.378911</td>\n",
       "      <td>0.020004</td>\n",
       "      <td>0.011258</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.119028</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.230471</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.007415</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.338724</td>\n",
       "      <td>0.011430</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 10, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.113035</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.037802</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.225593</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.335450</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.015766</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.116304</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.021244</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.231950</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.032360</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.353330</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>0.017950</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.101674      0.007203         0.003947        0.000222   \n",
       "1        0.192876      0.002987         0.007154        0.000271   \n",
       "2        0.294165      0.001066         0.010552        0.000124   \n",
       "3        0.100301      0.002187         0.003986        0.000075   \n",
       "4        0.194020      0.000675         0.007162        0.000041   \n",
       "5        0.293342      0.001047         0.010472        0.000043   \n",
       "6        0.100763      0.000400         0.003908        0.000027   \n",
       "7        0.200776      0.000995         0.007178        0.000028   \n",
       "8        0.307090      0.004932         0.010828        0.000194   \n",
       "9        0.108728      0.000852         0.003813        0.000017   \n",
       "10       0.212403      0.000927         0.006867        0.000061   \n",
       "11       0.324103      0.004151         0.010178        0.000313   \n",
       "12       0.114296      0.000998         0.003986        0.000119   \n",
       "13       0.224311      0.002504         0.007156        0.000250   \n",
       "14       0.342893      0.002851         0.010471        0.000058   \n",
       "15       0.116667      0.001079         0.003939        0.000080   \n",
       "16       0.234656      0.004049         0.007658        0.000825   \n",
       "17       0.378911      0.020004         0.011258        0.000192   \n",
       "18       0.119028      0.007539         0.004086        0.000174   \n",
       "19       0.230471      0.008267         0.007415        0.000419   \n",
       "20       0.338724      0.011430         0.010443        0.000234   \n",
       "21       0.113035      0.001038         0.003923        0.000034   \n",
       "22       0.225593      0.001102         0.007075        0.000047   \n",
       "23       0.335450      0.001486         0.010282        0.000087   \n",
       "24       0.116304      0.000525         0.003903        0.000036   \n",
       "25       0.231950      0.001022         0.007167        0.000074   \n",
       "26       0.353330      0.004151         0.010680        0.000257   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "0             gini              10                100   \n",
       "1             gini              10                200   \n",
       "2             gini              10                300   \n",
       "3             gini              11                100   \n",
       "4             gini              11                200   \n",
       "5             gini              11                300   \n",
       "6             gini              12                100   \n",
       "7             gini              12                200   \n",
       "8             gini              12                300   \n",
       "9          entropy              10                100   \n",
       "10         entropy              10                200   \n",
       "11         entropy              10                300   \n",
       "12         entropy              11                100   \n",
       "13         entropy              11                200   \n",
       "14         entropy              11                300   \n",
       "15         entropy              12                100   \n",
       "16         entropy              12                200   \n",
       "17         entropy              12                300   \n",
       "18        log_loss              10                100   \n",
       "19        log_loss              10                200   \n",
       "20        log_loss              10                300   \n",
       "21        log_loss              11                100   \n",
       "22        log_loss              11                200   \n",
       "23        log_loss              11                300   \n",
       "24        log_loss              12                100   \n",
       "25        log_loss              12                200   \n",
       "26        log_loss              12                300   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.027027   \n",
       "1   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.040541   \n",
       "2   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.027027   \n",
       "3   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.040541   \n",
       "4   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.027027   \n",
       "5   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.013514   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.027027   \n",
       "7   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.040541   \n",
       "8   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.054054   \n",
       "9   {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.013514   \n",
       "10  {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.000000   \n",
       "11  {'criterion': 'entropy', 'max_depth': 10, 'n_e...           0.013514   \n",
       "12  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.067568   \n",
       "13  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.027027   \n",
       "14  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.000000   \n",
       "15  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.094595   \n",
       "16  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.027027   \n",
       "17  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.040541   \n",
       "18  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.013514   \n",
       "19  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.000000   \n",
       "20  {'criterion': 'log_loss', 'max_depth': 10, 'n_...           0.000000   \n",
       "21  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.054054   \n",
       "22  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.013514   \n",
       "23  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.000000   \n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.027027   \n",
       "25  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.054054   \n",
       "26  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.054054   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.040541           0.013514           0.026667   \n",
       "1            0.054054           0.000000           0.000000   \n",
       "2            0.040541           0.013514           0.013333   \n",
       "3            0.121622           0.013514           0.013333   \n",
       "4            0.013514           0.040541           0.026667   \n",
       "5            0.040541           0.000000           0.013333   \n",
       "6            0.040541           0.013514           0.053333   \n",
       "7            0.054054           0.013514           0.053333   \n",
       "8            0.027027           0.000000           0.000000   \n",
       "9            0.013514           0.013514           0.013333   \n",
       "10           0.027027           0.000000           0.000000   \n",
       "11           0.027027           0.000000           0.013333   \n",
       "12           0.013514           0.000000           0.000000   \n",
       "13           0.040541           0.000000           0.040000   \n",
       "14           0.027027           0.000000           0.013333   \n",
       "15           0.027027           0.013514           0.053333   \n",
       "16           0.040541           0.013514           0.026667   \n",
       "17           0.027027           0.013514           0.000000   \n",
       "18           0.027027           0.013514           0.026667   \n",
       "19           0.027027           0.000000           0.013333   \n",
       "20           0.027027           0.000000           0.013333   \n",
       "21           0.040541           0.027027           0.013333   \n",
       "22           0.027027           0.000000           0.000000   \n",
       "23           0.027027           0.000000           0.013333   \n",
       "24           0.054054           0.027027           0.080000   \n",
       "25           0.027027           0.027027           0.026667   \n",
       "26           0.027027           0.000000           0.013333   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.013514         0.024252        0.010094               15  \n",
       "1            0.054054         0.029730        0.024771                8  \n",
       "2            0.027027         0.024288        0.010151               13  \n",
       "3            0.027027         0.043207        0.040477                1  \n",
       "4            0.027027         0.026955        0.008548                9  \n",
       "5            0.013514         0.016180        0.013248               19  \n",
       "6            0.054054         0.037694        0.015613                5  \n",
       "7            0.013514         0.034991        0.018184                6  \n",
       "8            0.027027         0.021622        0.020225               16  \n",
       "9            0.013514         0.013477        0.000072               22  \n",
       "10           0.013514         0.008108        0.010811               26  \n",
       "11           0.013514         0.013477        0.008547               22  \n",
       "12           0.013514         0.018919        0.025064               17  \n",
       "13           0.027027         0.026919        0.014706               11  \n",
       "14           0.027027         0.013477        0.012087               22  \n",
       "15           0.013514         0.040396        0.030754                3  \n",
       "16           0.027027         0.026955        0.008548                9  \n",
       "17           0.040541         0.024324        0.015759               12  \n",
       "18           0.013514         0.018847        0.006533               18  \n",
       "19           0.013514         0.010775        0.010103               25  \n",
       "20           0.000000         0.008072        0.010793               27  \n",
       "21           0.054054         0.037802        0.015815                4  \n",
       "22           0.027027         0.013514        0.012087               21  \n",
       "23           0.040541         0.016180        0.015766               19  \n",
       "24           0.027027         0.043027        0.021244                2  \n",
       "25           0.027027         0.032360        0.010848                7  \n",
       "26           0.027027         0.024288        0.017950               13  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],#\n",
    "    'n_estimators' : [100,200,300], #\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_4 , y_train_rf_4)\n",
    "best_rf_4 = clf.best_estimator_\n",
    "df_clf_lol = pd.DataFrame(clf.cv_results_)\n",
    "df_clf_lol.sort_values(by = 'rank_test_score', ascending = False)\n",
    "df_clf_lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_4.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature importances and index\n",
    "\n",
    "cluster_4_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "\n",
    "cluster_4_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_4_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.115343</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.039124</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102497</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'n_esti...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.028052</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.112406</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.211516</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200528</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.007396</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.022418</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "24       0.115343      0.001193         0.003906        0.000040   \n",
       "0        0.102497      0.008911         0.004008        0.000133   \n",
       "6        0.112406      0.007854         0.004313        0.000065   \n",
       "7        0.211516      0.002425         0.007747        0.000172   \n",
       "4        0.200528      0.002850         0.007396        0.000138   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "24        log_loss              12                100   \n",
       "0             gini              10                100   \n",
       "6             gini              12                100   \n",
       "7             gini              12                200   \n",
       "4             gini              11                200   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.111111   \n",
       "0   {'criterion': 'gini', 'max_depth': 10, 'n_esti...           0.041667   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.027778   \n",
       "7   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.041667   \n",
       "4   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.041667   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "24           0.028169           0.028169           0.000000   \n",
       "0            0.042254           0.028169           0.014085   \n",
       "6            0.014085           0.014085           0.014085   \n",
       "7            0.028169           0.028169           0.000000   \n",
       "4            0.014085           0.014085           0.000000   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "24           0.028169         0.039124        0.037611                1  \n",
       "0            0.014085         0.028052        0.012468                2  \n",
       "6            0.056338         0.025274        0.016413                3  \n",
       "7            0.028169         0.025235        0.013657                4  \n",
       "4            0.042254         0.022418        0.016766                5  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_5, y_train_rf_5)\n",
    "df_clf_5 = pd.DataFrame(clf.cv_results_)\n",
    "best_rf_5 = clf.best_estimator_\n",
    "df_clf_5.sort_values(by = 'rank_test_score', ascending = True, inplace = True)\n",
    "df_clf_5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_5.feature_importances_\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_3_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.109415</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.074741</td>\n",
       "      <td>0.027873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100574</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.071801</td>\n",
       "      <td>0.030129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.107471</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 11, 'n_...</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.063271</td>\n",
       "      <td>0.011847</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.063271</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.107141</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.060248</td>\n",
       "      <td>0.018792</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "24       0.109415      0.000879         0.003876        0.000099   \n",
       "6        0.100574      0.002778         0.004062        0.000127   \n",
       "21       0.107471      0.002094         0.003958        0.000187   \n",
       "3        0.093679      0.001938         0.003818        0.000113   \n",
       "15       0.107141      0.000810         0.003795        0.000079   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "24        log_loss              12                100   \n",
       "6             gini              12                100   \n",
       "21        log_loss              11                100   \n",
       "3             gini              11                100   \n",
       "15         entropy              12                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.100000   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.042857   \n",
       "21  {'criterion': 'log_loss', 'max_depth': 11, 'n_...           0.057143   \n",
       "3   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.057143   \n",
       "15  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.071429   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "24           0.085714           0.028571           0.101449   \n",
       "6            0.100000           0.085714           0.101449   \n",
       "21           0.057143           0.057143           0.057971   \n",
       "3            0.085714           0.028571           0.072464   \n",
       "15           0.085714           0.057143           0.057971   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "24           0.057971         0.074741        0.027873                1  \n",
       "6            0.028986         0.071801        0.030129                2  \n",
       "21           0.086957         0.063271        0.011847                3  \n",
       "3            0.072464         0.063271        0.019567                4  \n",
       "15           0.028986         0.060248        0.018792                5  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_6, y_train_rf_6)\n",
    "df_clf_5 = pd.DataFrame(clf.cv_results_)\n",
    "best_rf_6 = clf.best_estimator_\n",
    "df_clf_5.sort_values(by = 'rank_test_score', ascending = True, inplace = True)\n",
    "df_clf_5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_6.feature_importances_\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_3_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.108251</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>entropy</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 12, 'n_e...</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.061466</td>\n",
       "      <td>0.014604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.095819</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>gini</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 12, 'n_esti...</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.052685</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.110247</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 11, 'n_e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.049872</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089457</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 11, 'n_esti...</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.049787</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.108817</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 12, 'n_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.046931</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "15       0.108251      0.000564         0.003909        0.000144   \n",
       "6        0.095819      0.001815         0.003998        0.000176   \n",
       "12       0.110247      0.005286         0.004967        0.002286   \n",
       "3        0.089457      0.000574         0.003641        0.000111   \n",
       "24       0.108817      0.000535         0.003820        0.000028   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "15         entropy              12                100   \n",
       "6             gini              12                100   \n",
       "12         entropy              11                100   \n",
       "3             gini              11                100   \n",
       "24        log_loss              12                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "15  {'criterion': 'entropy', 'max_depth': 12, 'n_e...           0.043478   \n",
       "6   {'criterion': 'gini', 'max_depth': 12, 'n_esti...           0.028986   \n",
       "12  {'criterion': 'entropy', 'max_depth': 11, 'n_e...           0.000000   \n",
       "3   {'criterion': 'gini', 'max_depth': 11, 'n_esti...           0.028986   \n",
       "24  {'criterion': 'log_loss', 'max_depth': 12, 'n_...           0.000000   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "15           0.057971           0.058824           0.058824   \n",
       "6            0.057971           0.058824           0.044118   \n",
       "12           0.043478           0.058824           0.073529   \n",
       "3            0.043478           0.058824           0.044118   \n",
       "24           0.043478           0.073529           0.058824   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "15           0.088235         0.061466        0.014604                1  \n",
       "6            0.073529         0.052685        0.015068                2  \n",
       "12           0.073529         0.049872        0.027304                3  \n",
       "3            0.073529         0.049787        0.015166                4  \n",
       "24           0.058824         0.046931        0.025317                5  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {\n",
    "    'criterion': ['gini','entropy','log_loss'],\n",
    "    'n_estimators' : [100,200,300],\n",
    "    'max_depth' : [10,11,12]\n",
    "\n",
    "}\n",
    "clf = GridSearchCV(rf,param_grid=params,scoring = 'recall')\n",
    "clf.fit(X_train_rf_7, y_train_rf_7)\n",
    "df_clf_5 = pd.DataFrame(clf.cv_results_)\n",
    "best_rf_7 = clf.best_estimator_\n",
    "df_clf_5.sort_values(by = 'rank_test_score', ascending = True, inplace = True)\n",
    "df_clf_5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = best_rf_7.feature_importances_\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures = pd.DataFrame(importantFeatures,index = rf_data.columns ,columns=['importance'])\n",
    "\n",
    "\n",
    "\n",
    "cluster_3_importantFeatures.sort_values('importance', ascending = False, inplace = True)\n",
    "cluster_3_importantFeatures.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6963 - recall: 0.1335\n",
      "Epoch 1: val_recall improved from -inf to 0.00000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 3s 16ms/step - loss: 0.6963 - recall: 0.1335 - val_loss: 0.6837 - val_recall: 0.0000e+00\n",
      "Epoch 2/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6650 - recall: 0.0347\n",
      "Epoch 2: val_recall improved from 0.00000 to 0.00214, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 12ms/step - loss: 0.6650 - recall: 0.0347 - val_loss: 0.6684 - val_recall: 0.0021\n",
      "Epoch 3/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6579 - recall: 0.0027\n",
      "Epoch 3: val_recall did not improve from 0.00214\n",
      "165/165 [==============================] - 2s 10ms/step - loss: 0.6579 - recall: 0.0027 - val_loss: 0.6533 - val_recall: 0.0000e+00\n",
      "Epoch 4/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6547 - recall: 5.3821e-04\n",
      "Epoch 4: val_recall did not improve from 0.00214\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6547 - recall: 5.3390e-04 - val_loss: 0.6519 - val_recall: 0.0000e+00\n",
      "Epoch 5/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6542 - recall: 0.0000e+00\n",
      "Epoch 5: val_recall did not improve from 0.00214\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6553 - recall: 0.0000e+00 - val_loss: 0.6512 - val_recall: 0.0000e+00\n",
      "Epoch 6/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6545 - recall: 0.0231\n",
      "Epoch 6: val_recall improved from 0.00214 to 0.00428, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 11ms/step - loss: 0.6551 - recall: 0.0224 - val_loss: 0.6647 - val_recall: 0.0043\n",
      "Epoch 7/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6547 - recall: 0.0021\n",
      "Epoch 7: val_recall did not improve from 0.00428\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6547 - recall: 0.0021 - val_loss: 0.6513 - val_recall: 0.0000e+00\n",
      "Epoch 8/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6532 - recall: 0.0033\n",
      "Epoch 8: val_recall did not improve from 0.00428\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6539 - recall: 0.0032 - val_loss: 0.6558 - val_recall: 0.0043\n",
      "Epoch 9/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6557 - recall: 0.0290\n",
      "Epoch 9: val_recall improved from 0.00428 to 0.01071, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 11ms/step - loss: 0.6555 - recall: 0.0283 - val_loss: 0.6600 - val_recall: 0.0107\n",
      "Epoch 10/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6546 - recall: 0.0096\n",
      "Epoch 10: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6546 - recall: 0.0096 - val_loss: 0.6516 - val_recall: 0.0000e+00\n",
      "Epoch 11/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6542 - recall: 0.0059\n",
      "Epoch 11: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6542 - recall: 0.0059 - val_loss: 0.6512 - val_recall: 0.0043\n",
      "Epoch 12/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6528 - recall: 0.0048\n",
      "Epoch 12: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6532 - recall: 0.0048 - val_loss: 0.6509 - val_recall: 0.0000e+00\n",
      "Epoch 13/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6525 - recall: 0.0144\n",
      "Epoch 13: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6525 - recall: 0.0144 - val_loss: 0.6533 - val_recall: 0.0021\n",
      "Epoch 14/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6523 - recall: 0.0139\n",
      "Epoch 14: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6523 - recall: 0.0139 - val_loss: 0.6592 - val_recall: 0.0000e+00\n",
      "Epoch 15/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6558 - recall: 0.0224\n",
      "Epoch 15: val_recall did not improve from 0.01071\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6558 - recall: 0.0224 - val_loss: 0.6511 - val_recall: 0.0000e+00\n",
      "Epoch 16/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6532 - recall: 0.0172\n",
      "Epoch 16: val_recall improved from 0.01071 to 0.15632, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 11ms/step - loss: 0.6544 - recall: 0.0166 - val_loss: 0.6667 - val_recall: 0.1563\n",
      "Epoch 17/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6564 - recall: 0.0242\n",
      "Epoch 17: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6562 - recall: 0.0240 - val_loss: 0.6527 - val_recall: 0.0086\n",
      "Epoch 18/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6535 - recall: 0.0139\n",
      "Epoch 18: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6535 - recall: 0.0139 - val_loss: 0.6563 - val_recall: 0.0685\n",
      "Epoch 19/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6515 - recall: 0.0307\n",
      "Epoch 19: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6514 - recall: 0.0304 - val_loss: 0.6520 - val_recall: 0.0128\n",
      "Epoch 20/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6515 - recall: 0.0253\n",
      "Epoch 20: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6524 - recall: 0.0278 - val_loss: 0.6670 - val_recall: 0.0664\n",
      "Epoch 21/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.6531 - recall: 0.0151\n",
      "Epoch 21: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6529 - recall: 0.0149 - val_loss: 0.6524 - val_recall: 0.0150\n",
      "Epoch 22/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6556 - recall: 0.0386\n",
      "Epoch 22: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6553 - recall: 0.0384 - val_loss: 0.6576 - val_recall: 0.0214\n",
      "Epoch 23/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6530 - recall: 0.0226\n",
      "Epoch 23: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6529 - recall: 0.0240 - val_loss: 0.6596 - val_recall: 0.0857\n",
      "Epoch 24/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6521 - recall: 0.0196\n",
      "Epoch 24: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6526 - recall: 0.0192 - val_loss: 0.6509 - val_recall: 0.0193\n",
      "Epoch 25/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6517 - recall: 0.0326\n",
      "Epoch 25: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6517 - recall: 0.0326 - val_loss: 0.6518 - val_recall: 0.0086\n",
      "Epoch 26/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6530 - recall: 0.0297\n",
      "Epoch 26: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6523 - recall: 0.0294 - val_loss: 0.6598 - val_recall: 0.0214\n",
      "Epoch 27/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6548 - recall: 0.0234\n",
      "Epoch 27: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6550 - recall: 0.0230 - val_loss: 0.6519 - val_recall: 0.0128\n",
      "Epoch 28/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6572 - recall: 0.0448\n",
      "Epoch 28: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6572 - recall: 0.0448 - val_loss: 0.6602 - val_recall: 0.0278\n",
      "Epoch 29/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6519 - recall: 0.0356\n",
      "Epoch 29: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6529 - recall: 0.0347 - val_loss: 0.6634 - val_recall: 0.0086\n",
      "Epoch 30/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6519 - recall: 0.0315\n",
      "Epoch 30: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6519 - recall: 0.0315 - val_loss: 0.6515 - val_recall: 0.0021\n",
      "Epoch 31/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6517 - recall: 0.0443\n",
      "Epoch 31: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6517 - recall: 0.0443 - val_loss: 0.6780 - val_recall: 0.0535\n",
      "Epoch 32/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6509 - recall: 0.0230\n",
      "Epoch 32: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6509 - recall: 0.0230 - val_loss: 0.6557 - val_recall: 0.0471\n",
      "Epoch 33/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6546 - recall: 0.0301\n",
      "Epoch 33: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6531 - recall: 0.0294 - val_loss: 0.6590 - val_recall: 0.0193\n",
      "Epoch 34/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6514 - recall: 0.0364\n",
      "Epoch 34: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6523 - recall: 0.0368 - val_loss: 0.6576 - val_recall: 0.0364\n",
      "Epoch 35/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6527 - recall: 0.0506\n",
      "Epoch 35: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6522 - recall: 0.0497 - val_loss: 0.6524 - val_recall: 0.0171\n",
      "Epoch 36/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6511 - recall: 0.0391\n",
      "Epoch 36: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6513 - recall: 0.0390 - val_loss: 0.6623 - val_recall: 0.0707\n",
      "Epoch 37/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6514 - recall: 0.0304\n",
      "Epoch 37: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6518 - recall: 0.0294 - val_loss: 0.6534 - val_recall: 0.0086\n",
      "Epoch 38/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6495 - recall: 0.0345\n",
      "Epoch 38: val_recall did not improve from 0.15632\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6499 - recall: 0.0342 - val_loss: 0.6530 - val_recall: 0.0193\n",
      "Epoch 39/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6510 - recall: 0.0436\n",
      "Epoch 39: val_recall improved from 0.15632 to 0.20343, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 10ms/step - loss: 0.6518 - recall: 0.0464 - val_loss: 0.6791 - val_recall: 0.2034\n",
      "Epoch 40/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6514 - recall: 0.0485\n",
      "Epoch 40: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6515 - recall: 0.0481 - val_loss: 0.6616 - val_recall: 0.0514\n",
      "Epoch 41/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6556 - recall: 0.0540\n",
      "Epoch 41: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6550 - recall: 0.0534 - val_loss: 0.6578 - val_recall: 0.0257\n",
      "Epoch 42/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6518 - recall: 0.0534\n",
      "Epoch 42: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6524 - recall: 0.0534 - val_loss: 0.6548 - val_recall: 0.0471\n",
      "Epoch 43/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6488 - recall: 0.0838\n",
      "Epoch 43: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6488 - recall: 0.0838 - val_loss: 0.6622 - val_recall: 0.0450\n",
      "Epoch 44/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6545 - recall: 0.0879\n",
      "Epoch 44: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6548 - recall: 0.0870 - val_loss: 0.6610 - val_recall: 0.0450\n",
      "Epoch 45/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6503 - recall: 0.0522\n",
      "Epoch 45: val_recall did not improve from 0.20343\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6496 - recall: 0.0523 - val_loss: 0.6567 - val_recall: 0.0300\n",
      "Epoch 46/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6490 - recall: 0.0579\n",
      "Epoch 46: val_recall improved from 0.20343 to 0.22912, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 10ms/step - loss: 0.6493 - recall: 0.0566 - val_loss: 0.6759 - val_recall: 0.2291\n",
      "Epoch 47/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6478 - recall: 0.0518\n",
      "Epoch 47: val_recall did not improve from 0.22912\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6477 - recall: 0.0518 - val_loss: 0.6561 - val_recall: 0.0128\n",
      "Epoch 48/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.6460 - recall: 0.0607\n",
      "Epoch 48: val_recall did not improve from 0.22912\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6467 - recall: 0.0598 - val_loss: 0.6836 - val_recall: 0.0535\n",
      "Epoch 49/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6481 - recall: 0.0869\n",
      "Epoch 49: val_recall did not improve from 0.22912\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6490 - recall: 0.0854 - val_loss: 0.6548 - val_recall: 0.0150\n",
      "Epoch 50/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6492 - recall: 0.0439\n",
      "Epoch 50: val_recall did not improve from 0.22912\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6491 - recall: 0.0438 - val_loss: 0.6617 - val_recall: 0.1842\n",
      "Epoch 51/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6485 - recall: 0.0678\n",
      "Epoch 51: val_recall did not improve from 0.22912\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6491 - recall: 0.0662 - val_loss: 0.6564 - val_recall: 0.0385\n",
      "Epoch 52/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6450 - recall: 0.0548\n",
      "Epoch 52: val_recall improved from 0.22912 to 0.29979, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 10ms/step - loss: 0.6458 - recall: 0.0593 - val_loss: 0.6861 - val_recall: 0.2998\n",
      "Epoch 53/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6481 - recall: 0.0735\n",
      "Epoch 53: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6479 - recall: 0.0721 - val_loss: 0.6678 - val_recall: 0.0128\n",
      "Epoch 54/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6486 - recall: 0.0703\n",
      "Epoch 54: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6489 - recall: 0.0699 - val_loss: 0.6568 - val_recall: 0.0450\n",
      "Epoch 55/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6466 - recall: 0.0717\n",
      "Epoch 55: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6468 - recall: 0.0710 - val_loss: 0.6525 - val_recall: 0.0107\n",
      "Epoch 56/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6474 - recall: 0.0633\n",
      "Epoch 56: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6467 - recall: 0.0630 - val_loss: 0.6562 - val_recall: 0.0107\n",
      "Epoch 57/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6449 - recall: 0.0638\n",
      "Epoch 57: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6440 - recall: 0.0635 - val_loss: 0.6741 - val_recall: 0.0343\n",
      "Epoch 58/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6480 - recall: 0.0818\n",
      "Epoch 58: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6478 - recall: 0.0849 - val_loss: 0.6854 - val_recall: 0.2355\n",
      "Epoch 59/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6486 - recall: 0.1013\n",
      "Epoch 59: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6484 - recall: 0.0998 - val_loss: 0.6587 - val_recall: 0.0236\n",
      "Epoch 60/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6471 - recall: 0.0847\n",
      "Epoch 60: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6461 - recall: 0.0838 - val_loss: 0.6575 - val_recall: 0.0214\n",
      "Epoch 61/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.6406 - recall: 0.0714\n",
      "Epoch 61: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6403 - recall: 0.0715 - val_loss: 0.6634 - val_recall: 0.0642\n",
      "Epoch 62/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6429 - recall: 0.0979\n",
      "Epoch 62: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6421 - recall: 0.0972 - val_loss: 0.6545 - val_recall: 0.0814\n",
      "Epoch 63/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6457 - recall: 0.1112\n",
      "Epoch 63: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6470 - recall: 0.1116 - val_loss: 0.6548 - val_recall: 0.0214\n",
      "Epoch 64/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6442 - recall: 0.0818\n",
      "Epoch 64: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6438 - recall: 0.0817 - val_loss: 0.6601 - val_recall: 0.0321\n",
      "Epoch 65/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6454 - recall: 0.1079\n",
      "Epoch 65: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6445 - recall: 0.1068 - val_loss: 0.6614 - val_recall: 0.0792\n",
      "Epoch 66/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6423 - recall: 0.1037\n",
      "Epoch 66: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6413 - recall: 0.1030 - val_loss: 0.6650 - val_recall: 0.0557\n",
      "Epoch 67/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6418 - recall: 0.1217\n",
      "Epoch 67: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6419 - recall: 0.1217 - val_loss: 0.6663 - val_recall: 0.0600\n",
      "Epoch 68/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6470 - recall: 0.1314\n",
      "Epoch 68: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6472 - recall: 0.1303 - val_loss: 0.6639 - val_recall: 0.0514\n",
      "Epoch 69/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6402 - recall: 0.1261\n",
      "Epoch 69: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6402 - recall: 0.1260 - val_loss: 0.6646 - val_recall: 0.0471\n",
      "Epoch 70/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6436 - recall: 0.1236\n",
      "Epoch 70: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6432 - recall: 0.1223 - val_loss: 0.6663 - val_recall: 0.0707\n",
      "Epoch 71/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6376 - recall: 0.1488\n",
      "Epoch 71: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6383 - recall: 0.1474 - val_loss: 0.6684 - val_recall: 0.0664\n",
      "Epoch 72/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6418 - recall: 0.1282\n",
      "Epoch 72: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6416 - recall: 0.1297 - val_loss: 0.6637 - val_recall: 0.1563\n",
      "Epoch 73/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6395 - recall: 0.1400\n",
      "Epoch 73: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6398 - recall: 0.1393 - val_loss: 0.6656 - val_recall: 0.0771\n",
      "Epoch 74/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6363 - recall: 0.1437\n",
      "Epoch 74: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6349 - recall: 0.1452 - val_loss: 0.6858 - val_recall: 0.1156\n",
      "Epoch 75/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6351 - recall: 0.1656\n",
      "Epoch 75: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6349 - recall: 0.1671 - val_loss: 0.6740 - val_recall: 0.1306\n",
      "Epoch 76/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6399 - recall: 0.1699\n",
      "Epoch 76: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6407 - recall: 0.1682 - val_loss: 0.6635 - val_recall: 0.0450\n",
      "Epoch 77/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6355 - recall: 0.1449\n",
      "Epoch 77: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6360 - recall: 0.1436 - val_loss: 0.6796 - val_recall: 0.0921\n",
      "Epoch 78/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6343 - recall: 0.1718\n",
      "Epoch 78: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6338 - recall: 0.1714 - val_loss: 0.6764 - val_recall: 0.0878\n",
      "Epoch 79/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6349 - recall: 0.1673\n",
      "Epoch 79: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6353 - recall: 0.1660 - val_loss: 0.6682 - val_recall: 0.1263\n",
      "Epoch 80/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6357 - recall: 0.1963\n",
      "Epoch 80: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6357 - recall: 0.1965 - val_loss: 0.6690 - val_recall: 0.1713\n",
      "Epoch 81/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6309 - recall: 0.2061\n",
      "Epoch 81: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6311 - recall: 0.2077 - val_loss: 0.6822 - val_recall: 0.2505\n",
      "Epoch 82/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6319 - recall: 0.1837\n",
      "Epoch 82: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6313 - recall: 0.1831 - val_loss: 0.6774 - val_recall: 0.1092\n",
      "Epoch 83/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.6291 - recall: 0.2119\n",
      "Epoch 83: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6289 - recall: 0.2130 - val_loss: 0.7009 - val_recall: 0.1071\n",
      "Epoch 84/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6290 - recall: 0.2059\n",
      "Epoch 84: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6298 - recall: 0.2066 - val_loss: 0.6811 - val_recall: 0.2505\n",
      "Epoch 85/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6295 - recall: 0.2122\n",
      "Epoch 85: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6302 - recall: 0.2136 - val_loss: 0.6785 - val_recall: 0.1542\n",
      "Epoch 86/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6279 - recall: 0.2664\n",
      "Epoch 86: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6284 - recall: 0.2632 - val_loss: 0.6797 - val_recall: 0.1456\n",
      "Epoch 87/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.6292 - recall: 0.2219\n",
      "Epoch 87: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6286 - recall: 0.2248 - val_loss: 0.6864 - val_recall: 0.1285\n",
      "Epoch 88/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6227 - recall: 0.2344\n",
      "Epoch 88: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6227 - recall: 0.2344 - val_loss: 0.6810 - val_recall: 0.2034\n",
      "Epoch 89/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6249 - recall: 0.2723\n",
      "Epoch 89: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6247 - recall: 0.2707 - val_loss: 0.7136 - val_recall: 0.0557\n",
      "Epoch 90/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6206 - recall: 0.2621\n",
      "Epoch 90: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6201 - recall: 0.2605 - val_loss: 0.6949 - val_recall: 0.1156\n",
      "Epoch 91/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.6186 - recall: 0.2580\n",
      "Epoch 91: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 9ms/step - loss: 0.6189 - recall: 0.2552 - val_loss: 0.6821 - val_recall: 0.1178\n",
      "Epoch 92/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6122 - recall: 0.2456\n",
      "Epoch 92: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 2s 9ms/step - loss: 0.6122 - recall: 0.2456 - val_loss: 0.6797 - val_recall: 0.1542\n",
      "Epoch 93/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6183 - recall: 0.2604\n",
      "Epoch 93: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6178 - recall: 0.2621 - val_loss: 0.6853 - val_recall: 0.0578\n",
      "Epoch 94/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.6099 - recall: 0.2806\n",
      "Epoch 94: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6097 - recall: 0.2814 - val_loss: 0.6951 - val_recall: 0.2099\n",
      "Epoch 95/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6085 - recall: 0.2890\n",
      "Epoch 95: val_recall did not improve from 0.29979\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6088 - recall: 0.2894 - val_loss: 0.6969 - val_recall: 0.2163\n",
      "Epoch 96/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.6061 - recall: 0.3082\n",
      "Epoch 96: val_recall improved from 0.29979 to 0.35546, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(whole_df)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 2s 10ms/step - loss: 0.6052 - recall: 0.3129 - val_loss: 0.7196 - val_recall: 0.3555\n",
      "Epoch 97/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.6104 - recall: 0.3126\n",
      "Epoch 97: val_recall did not improve from 0.35546\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6109 - recall: 0.3107 - val_loss: 0.6916 - val_recall: 0.1563\n",
      "Epoch 98/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.6104 - recall: 0.3030\n",
      "Epoch 98: val_recall did not improve from 0.35546\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6107 - recall: 0.3059 - val_loss: 0.7066 - val_recall: 0.1734\n",
      "Epoch 99/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.6048 - recall: 0.3193\n",
      "Epoch 99: val_recall did not improve from 0.35546\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6065 - recall: 0.3209 - val_loss: 0.7116 - val_recall: 0.2612\n",
      "Epoch 100/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6069 - recall: 0.3225\n",
      "Epoch 100: val_recall did not improve from 0.35546\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6069 - recall: 0.3225 - val_loss: 0.7188 - val_recall: 0.2398\n",
      "Epoch 101/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.5976 - recall: 0.3448\n",
      "Epoch 101: val_recall did not improve from 0.35546\n",
      "165/165 [==============================] - 1s 8ms/step - loss: 0.6008 - recall: 0.3380 - val_loss: 0.7013 - val_recall: 0.2291\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "\n",
    "model_dense = Sequential([\n",
    "    Dense(input_shape = X_train_rf[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense.fit(X_train_rf, y_train_rf, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 2ms/step\n",
      "[[1392    0]\n",
      " [ 799    0]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "best_model = tf.keras.models.load_model('')\n",
    "y_pred_best = np.argmax(best_model.predict(X_test_rf),axis = 1)#probability\n",
    "\n",
    "confusionMatrix_best = sklearn.metrics.confusion_matrix((y_test_rf), y_pred_best)#41 percent accuracy\n",
    "print(confusionMatrix_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8133 - recall: 0.2195\n",
      "Epoch 1: val_recall improved from -inf to 0.67500, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_1)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_1)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_1)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 61ms/step - loss: 0.8133 - recall: 0.2195 - val_loss: 0.7542 - val_recall: 0.6750\n",
      "Epoch 2/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7550 - recall: 0.1919\n",
      "Epoch 2: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.7465 - recall: 0.1951 - val_loss: 0.6471 - val_recall: 0.1750\n",
      "Epoch 3/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6731 - recall: 0.2036\n",
      "Epoch 3: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6778 - recall: 0.1951 - val_loss: 0.6414 - val_recall: 0.2500\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6764 - recall: 0.2683\n",
      "Epoch 4: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6764 - recall: 0.2683 - val_loss: 0.6108 - val_recall: 0.0500\n",
      "Epoch 5/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6649 - recall: 0.0885\n",
      "Epoch 5: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6679 - recall: 0.0927 - val_loss: 0.6150 - val_recall: 0.1000\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6654 - recall: 0.1610  \n",
      "Epoch 6: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6654 - recall: 0.1610 - val_loss: 0.6018 - val_recall: 0.0500\n",
      "Epoch 7/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6777 - recall: 0.1823  \n",
      "Epoch 7: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6791 - recall: 0.1659 - val_loss: 0.6239 - val_recall: 0.0750\n",
      "Epoch 8/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6690 - recall: 0.1236  \n",
      "Epoch 8: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6712 - recall: 0.1561 - val_loss: 0.6737 - val_recall: 0.4250\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6751 - recall: 0.1415\n",
      "Epoch 9: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6751 - recall: 0.1415 - val_loss: 0.6419 - val_recall: 0.2500\n",
      "Epoch 10/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6626 - recall: 0.1848\n",
      "Epoch 10: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6629 - recall: 0.1805 - val_loss: 0.6293 - val_recall: 0.2500\n",
      "Epoch 11/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6553 - recall: 0.1314\n",
      "Epoch 11: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6685 - recall: 0.1122 - val_loss: 0.6155 - val_recall: 0.1000\n",
      "Epoch 12/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6719 - recall: 0.1798\n",
      "Epoch 12: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6708 - recall: 0.1805 - val_loss: 0.6324 - val_recall: 0.3250\n",
      "Epoch 13/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6641 - recall: 0.1406  \n",
      "Epoch 13: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6632 - recall: 0.1366 - val_loss: 0.6209 - val_recall: 0.1500\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6556 - recall: 0.0683\n",
      "Epoch 14: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6556 - recall: 0.0683 - val_loss: 0.6738 - val_recall: 0.4500\n",
      "Epoch 15/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6536 - recall: 0.1989\n",
      "Epoch 15: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6600 - recall: 0.1756 - val_loss: 0.6144 - val_recall: 0.3000\n",
      "Epoch 16/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6645 - recall: 0.1383  \n",
      "Epoch 16: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6576 - recall: 0.1317 - val_loss: 0.6224 - val_recall: 0.1500\n",
      "Epoch 17/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6537 - recall: 0.1602\n",
      "Epoch 17: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6581 - recall: 0.1561 - val_loss: 0.6480 - val_recall: 0.3000\n",
      "Epoch 18/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6425 - recall: 0.1667\n",
      "Epoch 18: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6599 - recall: 0.1463 - val_loss: 0.6065 - val_recall: 0.1000\n",
      "Epoch 19/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6583 - recall: 0.0897\n",
      "Epoch 19: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6584 - recall: 0.0780 - val_loss: 0.6154 - val_recall: 0.0500\n",
      "Epoch 20/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6466 - recall: 0.0939  \n",
      "Epoch 20: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6536 - recall: 0.1024 - val_loss: 0.6388 - val_recall: 0.3750\n",
      "Epoch 21/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6562 - recall: 0.2097\n",
      "Epoch 21: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6558 - recall: 0.2000 - val_loss: 0.6185 - val_recall: 0.1500\n",
      "Epoch 22/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6579 - recall: 0.0000e+00\n",
      "Epoch 22: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6578 - recall: 0.0000e+00 - val_loss: 0.6080 - val_recall: 0.1000\n",
      "Epoch 23/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6534 - recall: 0.0829  \n",
      "Epoch 23: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6521 - recall: 0.1024 - val_loss: 0.6322 - val_recall: 0.3500\n",
      "Epoch 24/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6542 - recall: 0.2350\n",
      "Epoch 24: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6553 - recall: 0.2293 - val_loss: 0.6371 - val_recall: 0.2000\n",
      "Epoch 25/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6571 - recall: 0.1979\n",
      "Epoch 25: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6534 - recall: 0.1854 - val_loss: 0.6117 - val_recall: 0.1000\n",
      "Epoch 26/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6529 - recall: 0.0867\n",
      "Epoch 26: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6524 - recall: 0.0927 - val_loss: 0.6221 - val_recall: 0.1750\n",
      "Epoch 27/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6457 - recall: 0.1215  \n",
      "Epoch 27: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6522 - recall: 0.1366 - val_loss: 0.6339 - val_recall: 0.2250\n",
      "Epoch 28/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6400 - recall: 0.1242\n",
      "Epoch 28: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6535 - recall: 0.1122 - val_loss: 0.6083 - val_recall: 0.1500\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6492 - recall: 0.1805\n",
      "Epoch 29: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6492 - recall: 0.1805 - val_loss: 0.6171 - val_recall: 0.2250\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6483 - recall: 0.1561\n",
      "Epoch 30: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6483 - recall: 0.1561 - val_loss: 0.6123 - val_recall: 0.0750\n",
      "Epoch 31/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6445 - recall: 0.1347\n",
      "Epoch 31: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6466 - recall: 0.1317 - val_loss: 0.6208 - val_recall: 0.2500\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6465 - recall: 0.3073\n",
      "Epoch 32: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6465 - recall: 0.3073 - val_loss: 0.6076 - val_recall: 0.1500\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6496 - recall: 0.0341\n",
      "Epoch 33: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6496 - recall: 0.0341 - val_loss: 0.6305 - val_recall: 0.0750\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6447 - recall: 0.1122  \n",
      "Epoch 34: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6447 - recall: 0.1122 - val_loss: 0.6153 - val_recall: 0.2000\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6437 - recall: 0.2195\n",
      "Epoch 35: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6437 - recall: 0.2195 - val_loss: 0.6278 - val_recall: 0.2750\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6512 - recall: 0.1707\n",
      "Epoch 36: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6512 - recall: 0.1707 - val_loss: 0.6183 - val_recall: 0.1750\n",
      "Epoch 37/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6413 - recall: 0.2240\n",
      "Epoch 37: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6431 - recall: 0.2146 - val_loss: 0.6162 - val_recall: 0.1750\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6355 - recall: 0.1073\n",
      "Epoch 38: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6355 - recall: 0.1073 - val_loss: 0.6334 - val_recall: 0.3500\n",
      "Epoch 39/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6341 - recall: 0.3118\n",
      "Epoch 39: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6317 - recall: 0.3024 - val_loss: 0.6456 - val_recall: 0.2750\n",
      "Epoch 40/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6405 - recall: 0.2951\n",
      "Epoch 40: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6324 - recall: 0.2976 - val_loss: 0.6250 - val_recall: 0.2500\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6256 - recall: 0.2683\n",
      "Epoch 41: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6256 - recall: 0.2683 - val_loss: 0.6635 - val_recall: 0.4750\n",
      "Epoch 42/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6232 - recall: 0.3222\n",
      "Epoch 42: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6290 - recall: 0.3122 - val_loss: 0.6232 - val_recall: 0.2500\n",
      "Epoch 43/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6403 - recall: 0.3673\n",
      "Epoch 43: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6404 - recall: 0.3659 - val_loss: 0.6390 - val_recall: 0.4250\n",
      "Epoch 44/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6194 - recall: 0.2541\n",
      "Epoch 44: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6187 - recall: 0.2537 - val_loss: 0.6497 - val_recall: 0.1500\n",
      "Epoch 45/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6269 - recall: 0.2513\n",
      "Epoch 45: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6221 - recall: 0.2585 - val_loss: 0.6636 - val_recall: 0.3750\n",
      "Epoch 46/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6066 - recall: 0.3068\n",
      "Epoch 46: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6134 - recall: 0.2976 - val_loss: 0.6569 - val_recall: 0.3000\n",
      "Epoch 47/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6022 - recall: 0.3591\n",
      "Epoch 47: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6045 - recall: 0.3756 - val_loss: 0.6678 - val_recall: 0.2750\n",
      "Epoch 48/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6224 - recall: 0.3676\n",
      "Epoch 48: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6211 - recall: 0.3561 - val_loss: 0.6305 - val_recall: 0.1750\n",
      "Epoch 49/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6133 - recall: 0.3388\n",
      "Epoch 49: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6131 - recall: 0.3317 - val_loss: 0.6855 - val_recall: 0.3500\n",
      "Epoch 50/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6053 - recall: 0.3867\n",
      "Epoch 50: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6066 - recall: 0.3659 - val_loss: 0.6861 - val_recall: 0.2250\n",
      "Epoch 51/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6098 - recall: 0.4420\n",
      "Epoch 51: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6096 - recall: 0.4439 - val_loss: 0.6437 - val_recall: 0.3500\n",
      "Epoch 52/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5809 - recall: 0.3311\n",
      "Epoch 52: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5979 - recall: 0.3415 - val_loss: 0.6872 - val_recall: 0.3750\n",
      "Epoch 53/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6192 - recall: 0.3871\n",
      "Epoch 53: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6150 - recall: 0.3756 - val_loss: 0.6666 - val_recall: 0.3750\n",
      "Epoch 54/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5956 - recall: 0.3667\n",
      "Epoch 54: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5986 - recall: 0.3659 - val_loss: 0.6648 - val_recall: 0.4750\n",
      "Epoch 55/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5927 - recall: 0.4130\n",
      "Epoch 55: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5914 - recall: 0.4098 - val_loss: 0.6735 - val_recall: 0.4250\n",
      "Epoch 56/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5846 - recall: 0.3916\n",
      "Epoch 56: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5878 - recall: 0.4293 - val_loss: 0.7266 - val_recall: 0.5250\n",
      "Epoch 57/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5875 - recall: 0.4740\n",
      "Epoch 57: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5945 - recall: 0.4585 - val_loss: 0.6590 - val_recall: 0.2250\n",
      "Epoch 58/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5771 - recall: 0.3616\n",
      "Epoch 58: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5819 - recall: 0.3805 - val_loss: 0.7108 - val_recall: 0.3250\n",
      "Epoch 59/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5960 - recall: 0.4303\n",
      "Epoch 59: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6120 - recall: 0.3756 - val_loss: 0.6807 - val_recall: 0.3500\n",
      "Epoch 60/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6025 - recall: 0.4365\n",
      "Epoch 60: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5966 - recall: 0.4390 - val_loss: 0.6762 - val_recall: 0.3750\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5615 - recall: 0.4732\n",
      "Epoch 61: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5615 - recall: 0.4732 - val_loss: 0.6952 - val_recall: 0.3500\n",
      "Epoch 62/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5685 - recall: 0.4663\n",
      "Epoch 62: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5706 - recall: 0.4585 - val_loss: 0.6681 - val_recall: 0.3250\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5681 - recall: 0.4293\n",
      "Epoch 63: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5681 - recall: 0.4293 - val_loss: 0.6756 - val_recall: 0.3500\n",
      "Epoch 64/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5563 - recall: 0.4536\n",
      "Epoch 64: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5576 - recall: 0.4390 - val_loss: 0.6721 - val_recall: 0.3250\n",
      "Epoch 65/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5345 - recall: 0.4773\n",
      "Epoch 65: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5473 - recall: 0.4780 - val_loss: 0.7116 - val_recall: 0.5500\n",
      "Epoch 66/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5324 - recall: 0.5508\n",
      "Epoch 66: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5267 - recall: 0.5366 - val_loss: 0.7178 - val_recall: 0.2250\n",
      "Epoch 67/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5507 - recall: 0.4946\n",
      "Epoch 67: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5476 - recall: 0.4927 - val_loss: 0.6993 - val_recall: 0.5250\n",
      "Epoch 68/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5296 - recall: 0.5028\n",
      "Epoch 68: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5359 - recall: 0.5024 - val_loss: 0.7002 - val_recall: 0.5500\n",
      "Epoch 69/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5061 - recall: 0.5722\n",
      "Epoch 69: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5133 - recall: 0.5610 - val_loss: 0.7235 - val_recall: 0.4750\n",
      "Epoch 70/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5194 - recall: 0.6000\n",
      "Epoch 70: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5133 - recall: 0.5854 - val_loss: 0.7106 - val_recall: 0.3750\n",
      "Epoch 71/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5155 - recall: 0.4944\n",
      "Epoch 71: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5184 - recall: 0.5073 - val_loss: 0.7305 - val_recall: 0.4000\n",
      "Epoch 72/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5240 - recall: 0.5580\n",
      "Epoch 72: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5269 - recall: 0.5463 - val_loss: 0.6650 - val_recall: 0.4000\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5052 - recall: 0.5659\n",
      "Epoch 73: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5052 - recall: 0.5659 - val_loss: 0.7527 - val_recall: 0.5250\n",
      "Epoch 74/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4755 - recall: 0.5818\n",
      "Epoch 74: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4843 - recall: 0.5805 - val_loss: 0.7814 - val_recall: 0.4250\n",
      "Epoch 75/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4911 - recall: 0.6056\n",
      "Epoch 75: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4892 - recall: 0.6098 - val_loss: 0.7496 - val_recall: 0.3750\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4865 - recall: 0.5756\n",
      "Epoch 76: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4865 - recall: 0.5756 - val_loss: 0.8328 - val_recall: 0.4500\n",
      "Epoch 77/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4933 - recall: 0.5714\n",
      "Epoch 77: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4931 - recall: 0.5610 - val_loss: 0.7953 - val_recall: 0.4750\n",
      "Epoch 78/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4540 - recall: 0.6257\n",
      "Epoch 78: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4623 - recall: 0.6195 - val_loss: 0.7529 - val_recall: 0.4750\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4682 - recall: 0.6439\n",
      "Epoch 79: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4682 - recall: 0.6439 - val_loss: 0.8320 - val_recall: 0.4250\n",
      "Epoch 80/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4033 - recall: 0.7588\n",
      "Epoch 80: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4481 - recall: 0.6976 - val_loss: 0.8503 - val_recall: 0.3000\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4713 - recall: 0.6829\n",
      "Epoch 81: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4713 - recall: 0.6829 - val_loss: 0.8098 - val_recall: 0.5000\n",
      "Epoch 82/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4560 - recall: 0.6568\n",
      "Epoch 82: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4554 - recall: 0.6585 - val_loss: 0.8193 - val_recall: 0.4000\n",
      "Epoch 83/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4179 - recall: 0.6902\n",
      "Epoch 83: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4257 - recall: 0.6829 - val_loss: 0.8456 - val_recall: 0.4000\n",
      "Epoch 84/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4242 - recall: 0.6757\n",
      "Epoch 84: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4173 - recall: 0.6780 - val_loss: 0.8992 - val_recall: 0.3750\n",
      "Epoch 85/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3763 - recall: 0.7738\n",
      "Epoch 85: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3865 - recall: 0.7415 - val_loss: 0.8204 - val_recall: 0.4000\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3768 - recall: 0.7659\n",
      "Epoch 86: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3768 - recall: 0.7659 - val_loss: 0.8072 - val_recall: 0.4500\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3907 - recall: 0.7610\n",
      "Epoch 87: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3907 - recall: 0.7610 - val_loss: 0.8352 - val_recall: 0.3000\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3612 - recall: 0.7463\n",
      "Epoch 88: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3612 - recall: 0.7463 - val_loss: 0.8579 - val_recall: 0.3500\n",
      "Epoch 89/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3296 - recall: 0.7744\n",
      "Epoch 89: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3264 - recall: 0.7805 - val_loss: 0.8689 - val_recall: 0.4500\n",
      "Epoch 90/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.2969 - recall: 0.8307\n",
      "Epoch 90: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3026 - recall: 0.8195 - val_loss: 0.8973 - val_recall: 0.3000\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2759 - recall: 0.8634\n",
      "Epoch 91: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2759 - recall: 0.8634 - val_loss: 0.9242 - val_recall: 0.3750\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2583 - recall: 0.8488\n",
      "Epoch 92: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2583 - recall: 0.8488 - val_loss: 0.9315 - val_recall: 0.4250\n",
      "Epoch 93/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.2936 - recall: 0.8652\n",
      "Epoch 93: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2884 - recall: 0.8488 - val_loss: 0.8760 - val_recall: 0.4250\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2660 - recall: 0.8634\n",
      "Epoch 94: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2660 - recall: 0.8634 - val_loss: 1.0139 - val_recall: 0.3750\n",
      "Epoch 95/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.2188 - recall: 0.8667\n",
      "Epoch 95: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2186 - recall: 0.8634 - val_loss: 1.0641 - val_recall: 0.4250\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1951 - recall: 0.9463\n",
      "Epoch 96: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1951 - recall: 0.9463 - val_loss: 0.9796 - val_recall: 0.3250\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1809 - recall: 0.9220\n",
      "Epoch 97: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1809 - recall: 0.9220 - val_loss: 1.0078 - val_recall: 0.4500\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1674 - recall: 0.9366\n",
      "Epoch 98: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1674 - recall: 0.9366 - val_loss: 1.0435 - val_recall: 0.6000\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1724 - recall: 0.9220\n",
      "Epoch 99: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1724 - recall: 0.9220 - val_loss: 1.0477 - val_recall: 0.4500\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1413 - recall: 0.9707\n",
      "Epoch 100: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1413 - recall: 0.9707 - val_loss: 1.0291 - val_recall: 0.4250\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1191 - recall: 0.9659\n",
      "Epoch 101: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1191 - recall: 0.9659 - val_loss: 1.0435 - val_recall: 0.3500\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1048 - recall: 0.9854\n",
      "Epoch 102: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1048 - recall: 0.9854 - val_loss: 1.0492 - val_recall: 0.4000\n",
      "Epoch 103/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.0945 - recall: 0.9867\n",
      "Epoch 103: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0969 - recall: 0.9854 - val_loss: 1.1216 - val_recall: 0.5250\n",
      "Epoch 104/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.0777 - recall: 1.0000\n",
      "Epoch 104: val_recall did not improve from 0.67500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0794 - recall: 1.0000 - val_loss: 1.0662 - val_recall: 0.3500\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "\n",
    "model_dense_1 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_1[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_1, y_train_rf_1, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 700)               18900     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 800)               560800    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1000)              801000    \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1381701 (5.27 MB)\n",
      "Trainable params: 1381701 (5.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_rf_1 = tf.keras.saving.load_model('')\n",
    "best_model_rf_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7368 - recall: 0.2190\n",
      "Epoch 1: val_recall improved from -inf to 0.03333, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 48ms/step - loss: 0.7368 - recall: 0.2190 - val_loss: 0.6318 - val_recall: 0.0333\n",
      "Epoch 2/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6858 - recall: 0.0985\n",
      "Epoch 2: val_recall did not improve from 0.03333\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6874 - recall: 0.0985 - val_loss: 0.6349 - val_recall: 0.0333\n",
      "Epoch 3/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6766 - recall: 0.1970\n",
      "Epoch 3: val_recall did not improve from 0.03333\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6744 - recall: 0.1934 - val_loss: 0.6472 - val_recall: 0.0167\n",
      "Epoch 4/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6875 - recall: 0.0865\n",
      "Epoch 4: val_recall improved from 0.03333 to 0.20000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 29ms/step - loss: 0.6873 - recall: 0.0985 - val_loss: 0.6716 - val_recall: 0.2000\n",
      "Epoch 5/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6771 - recall: 0.1680\n",
      "Epoch 5: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6738 - recall: 0.1533 - val_loss: 0.6425 - val_recall: 0.0167\n",
      "Epoch 6/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6682 - recall: 0.0341\n",
      "Epoch 6: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6695 - recall: 0.0328 - val_loss: 0.6523 - val_recall: 0.0500\n",
      "Epoch 7/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6719 - recall: 0.1203\n",
      "Epoch 7: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6714 - recall: 0.1168 - val_loss: 0.6422 - val_recall: 0.0000e+00\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6674 - recall: 0.1241  \n",
      "Epoch 8: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6674 - recall: 0.1241 - val_loss: 0.6437 - val_recall: 0.0500\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6646 - recall: 0.0511\n",
      "Epoch 9: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6646 - recall: 0.0511 - val_loss: 0.6485 - val_recall: 0.0167\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6654 - recall: 0.0425\n",
      "Epoch 10: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6643 - recall: 0.0401 - val_loss: 0.6583 - val_recall: 0.0000e+00\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6666 - recall: 0.0730\n",
      "Epoch 11: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6666 - recall: 0.0730 - val_loss: 0.6646 - val_recall: 0.0167\n",
      "Epoch 12/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6614 - recall: 0.0083\n",
      "Epoch 12: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6609 - recall: 0.0109 - val_loss: 0.6411 - val_recall: 0.0000e+00\n",
      "Epoch 13/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6582 - recall: 0.1085\n",
      "Epoch 13: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6604 - recall: 0.1204 - val_loss: 0.6433 - val_recall: 0.0000e+00\n",
      "Epoch 14/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6557 - recall: 0.0099\n",
      "Epoch 14: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6624 - recall: 0.0073 - val_loss: 0.6419 - val_recall: 0.0000e+00\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6584 - recall: 0.0292\n",
      "Epoch 15: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6584 - recall: 0.0292 - val_loss: 0.6420 - val_recall: 0.0000e+00\n",
      "Epoch 16/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6583 - recall: 0.0412\n",
      "Epoch 16: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6588 - recall: 0.0401 - val_loss: 0.6449 - val_recall: 0.0167\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6580 - recall: 0.0620\n",
      "Epoch 17: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6580 - recall: 0.0620 - val_loss: 0.6448 - val_recall: 0.0000e+00\n",
      "Epoch 18/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6556 - recall: 0.0591\n",
      "Epoch 18: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6579 - recall: 0.0547 - val_loss: 0.6435 - val_recall: 0.0000e+00\n",
      "Epoch 19/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6581 - recall: 0.0345  \n",
      "Epoch 19: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6583 - recall: 0.0328 - val_loss: 0.6482 - val_recall: 0.0000e+00\n",
      "Epoch 20/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6528 - recall: 0.0653  \n",
      "Epoch 20: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6573 - recall: 0.0803 - val_loss: 0.6583 - val_recall: 0.0333\n",
      "Epoch 21/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6572 - recall: 0.0721\n",
      "Epoch 21: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6567 - recall: 0.0657 - val_loss: 0.6479 - val_recall: 0.0000e+00\n",
      "Epoch 22/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6522 - recall: 0.1208\n",
      "Epoch 22: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6545 - recall: 0.1058 - val_loss: 0.6572 - val_recall: 0.0667\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6552 - recall: 0.0839\n",
      "Epoch 23: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6552 - recall: 0.0839 - val_loss: 0.6534 - val_recall: 0.0167\n",
      "Epoch 24/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6526 - recall: 0.1450\n",
      "Epoch 24: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6524 - recall: 0.1423 - val_loss: 0.6548 - val_recall: 0.0167\n",
      "Epoch 25/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6544 - recall: 0.1086\n",
      "Epoch 25: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6546 - recall: 0.1131 - val_loss: 0.6577 - val_recall: 0.0500\n",
      "Epoch 26/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6491 - recall: 0.1128\n",
      "Epoch 26: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6497 - recall: 0.1277 - val_loss: 0.6535 - val_recall: 0.0167\n",
      "Epoch 27/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6585 - recall: 0.1429\n",
      "Epoch 27: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6529 - recall: 0.1314 - val_loss: 0.6520 - val_recall: 0.0000e+00\n",
      "Epoch 28/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6501 - recall: 0.1896\n",
      "Epoch 28: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6519 - recall: 0.1898 - val_loss: 0.6574 - val_recall: 0.0167\n",
      "Epoch 29/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6551 - recall: 0.1388\n",
      "Epoch 29: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6563 - recall: 0.1825 - val_loss: 0.6694 - val_recall: 0.0500\n",
      "Epoch 30/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6471 - recall: 0.1744\n",
      "Epoch 30: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6534 - recall: 0.1861 - val_loss: 0.6600 - val_recall: 0.0500\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6548 - recall: 0.1752\n",
      "Epoch 31: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6548 - recall: 0.1752 - val_loss: 0.6636 - val_recall: 0.0833\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6536 - recall: 0.1934\n",
      "Epoch 32: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6536 - recall: 0.1934 - val_loss: 0.6570 - val_recall: 0.0167\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6585 - recall: 0.1953\n",
      "Epoch 33: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6612 - recall: 0.1861 - val_loss: 0.6548 - val_recall: 0.0500\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6534 - recall: 0.1606\n",
      "Epoch 34: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6534 - recall: 0.1606 - val_loss: 0.6541 - val_recall: 0.0333\n",
      "Epoch 35/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6481 - recall: 0.1604\n",
      "Epoch 35: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6489 - recall: 0.1606 - val_loss: 0.6692 - val_recall: 0.1333\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6586 - recall: 0.1496\n",
      "Epoch 36: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6586 - recall: 0.1496 - val_loss: 0.6650 - val_recall: 0.0833\n",
      "Epoch 37/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6438 - recall: 0.2230\n",
      "Epoch 37: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6438 - recall: 0.2226 - val_loss: 0.6599 - val_recall: 0.0333\n",
      "Epoch 38/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6750 - recall: 0.1452\n",
      "Epoch 38: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6762 - recall: 0.1314 - val_loss: 0.6522 - val_recall: 0.0333\n",
      "Epoch 39/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6659 - recall: 0.1396\n",
      "Epoch 39: val_recall did not improve from 0.20000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6669 - recall: 0.1350 - val_loss: 0.6482 - val_recall: 0.0000e+00\n",
      "Epoch 40/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6547 - recall: 0.2009\n",
      "Epoch 40: val_recall improved from 0.20000 to 0.60000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_2)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 27ms/step - loss: 0.6626 - recall: 0.1861 - val_loss: 0.7191 - val_recall: 0.6000\n",
      "Epoch 41/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6613 - recall: 0.2167\n",
      "Epoch 41: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6633 - recall: 0.2153 - val_loss: 0.6665 - val_recall: 0.2333\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6497 - recall: 0.1204\n",
      "Epoch 42: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6497 - recall: 0.1204 - val_loss: 0.6616 - val_recall: 0.0500\n",
      "Epoch 43/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6418 - recall: 0.1692\n",
      "Epoch 43: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6426 - recall: 0.1715 - val_loss: 0.6676 - val_recall: 0.0500\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6428 - recall: 0.1825\n",
      "Epoch 44: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6428 - recall: 0.1825 - val_loss: 0.6753 - val_recall: 0.1667\n",
      "Epoch 45/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6396 - recall: 0.2772\n",
      "Epoch 45: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6418 - recall: 0.2701 - val_loss: 0.6853 - val_recall: 0.1333\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6387 - recall: 0.2737\n",
      "Epoch 46: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6387 - recall: 0.2737 - val_loss: 0.6949 - val_recall: 0.2667\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6382 - recall: 0.3102\n",
      "Epoch 47: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6382 - recall: 0.3102 - val_loss: 0.6970 - val_recall: 0.2667\n",
      "Epoch 48/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6266 - recall: 0.2000\n",
      "Epoch 48: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6380 - recall: 0.2445 - val_loss: 0.6882 - val_recall: 0.1167\n",
      "Epoch 49/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6251 - recall: 0.2911\n",
      "Epoch 49: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6330 - recall: 0.2701 - val_loss: 0.6802 - val_recall: 0.1333\n",
      "Epoch 50/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6490 - recall: 0.2412\n",
      "Epoch 50: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6369 - recall: 0.2518 - val_loss: 0.6734 - val_recall: 0.1333\n",
      "Epoch 51/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6682 - recall: 0.2085  \n",
      "Epoch 51: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6538 - recall: 0.2299 - val_loss: 0.6621 - val_recall: 0.0500\n",
      "Epoch 52/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6563 - recall: 0.3168\n",
      "Epoch 52: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6629 - recall: 0.3029 - val_loss: 0.6580 - val_recall: 0.0500\n",
      "Epoch 53/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6520 - recall: 0.2756\n",
      "Epoch 53: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6527 - recall: 0.2591 - val_loss: 0.6699 - val_recall: 0.0667\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6298 - recall: 0.2591\n",
      "Epoch 54: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6298 - recall: 0.2591 - val_loss: 0.6737 - val_recall: 0.1000\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6231 - recall: 0.2555\n",
      "Epoch 55: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6231 - recall: 0.2555 - val_loss: 0.6818 - val_recall: 0.0833\n",
      "Epoch 56/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6139 - recall: 0.3081\n",
      "Epoch 56: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6211 - recall: 0.3285 - val_loss: 0.6844 - val_recall: 0.0500\n",
      "Epoch 57/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6266 - recall: 0.2990\n",
      "Epoch 57: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6260 - recall: 0.2664 - val_loss: 0.7239 - val_recall: 0.1833\n",
      "Epoch 58/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6304 - recall: 0.2525\n",
      "Epoch 58: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6439 - recall: 0.2518 - val_loss: 0.6750 - val_recall: 0.2333\n",
      "Epoch 59/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6516 - recall: 0.3267\n",
      "Epoch 59: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6541 - recall: 0.3175 - val_loss: 0.6827 - val_recall: 0.1667\n",
      "Epoch 60/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.6223 - recall: 0.2714\n",
      "Epoch 60: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6193 - recall: 0.2591 - val_loss: 0.6980 - val_recall: 0.1833\n",
      "Epoch 61/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6135 - recall: 0.4029\n",
      "Epoch 61: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6082 - recall: 0.3723 - val_loss: 0.6937 - val_recall: 0.0333\n",
      "Epoch 62/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.5982 - recall: 0.2913\n",
      "Epoch 62: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6058 - recall: 0.2956 - val_loss: 0.6879 - val_recall: 0.2167\n",
      "Epoch 63/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5940 - recall: 0.3701\n",
      "Epoch 63: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.6034 - recall: 0.3540 - val_loss: 0.7068 - val_recall: 0.2167\n",
      "Epoch 64/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6050 - recall: 0.3582\n",
      "Epoch 64: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6039 - recall: 0.3577 - val_loss: 0.6923 - val_recall: 0.2833\n",
      "Epoch 65/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5968 - recall: 0.3529\n",
      "Epoch 65: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5922 - recall: 0.3759 - val_loss: 0.7430 - val_recall: 0.2667\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6003 - recall: 0.4088\n",
      "Epoch 66: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6003 - recall: 0.4088 - val_loss: 0.7468 - val_recall: 0.3000\n",
      "Epoch 67/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5923 - recall: 0.4189\n",
      "Epoch 67: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5954 - recall: 0.4161 - val_loss: 0.7510 - val_recall: 0.2000\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5726 - recall: 0.4307\n",
      "Epoch 68: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5726 - recall: 0.4307 - val_loss: 0.7608 - val_recall: 0.2500\n",
      "Epoch 69/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5709 - recall: 0.4465\n",
      "Epoch 69: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5717 - recall: 0.4416 - val_loss: 0.7523 - val_recall: 0.1333\n",
      "Epoch 70/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.6106 - recall: 0.3680\n",
      "Epoch 70: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6118 - recall: 0.3686 - val_loss: 0.7164 - val_recall: 0.2500\n",
      "Epoch 71/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5727 - recall: 0.4302\n",
      "Epoch 71: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5734 - recall: 0.4343 - val_loss: 0.7385 - val_recall: 0.0833\n",
      "Epoch 72/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5720 - recall: 0.4685\n",
      "Epoch 72: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5705 - recall: 0.4818 - val_loss: 0.7660 - val_recall: 0.3000\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5587 - recall: 0.4927\n",
      "Epoch 73: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5587 - recall: 0.4927 - val_loss: 0.7728 - val_recall: 0.3000\n",
      "Epoch 74/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5472 - recall: 0.5376\n",
      "Epoch 74: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5492 - recall: 0.5328 - val_loss: 0.8092 - val_recall: 0.2000\n",
      "Epoch 75/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5364 - recall: 0.5240\n",
      "Epoch 75: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5304 - recall: 0.5219 - val_loss: 0.7580 - val_recall: 0.2000\n",
      "Epoch 76/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5229 - recall: 0.5208\n",
      "Epoch 76: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5260 - recall: 0.5182 - val_loss: 0.7877 - val_recall: 0.2667\n",
      "Epoch 77/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.5149 - recall: 0.5522\n",
      "Epoch 77: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5131 - recall: 0.5511 - val_loss: 0.8030 - val_recall: 0.1833\n",
      "Epoch 78/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5134 - recall: 0.5919\n",
      "Epoch 78: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5219 - recall: 0.5657 - val_loss: 0.7986 - val_recall: 0.2833\n",
      "Epoch 79/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4959 - recall: 0.5817\n",
      "Epoch 79: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5038 - recall: 0.5693 - val_loss: 0.8482 - val_recall: 0.2000\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5072 - recall: 0.5766\n",
      "Epoch 80: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5072 - recall: 0.5766 - val_loss: 0.8305 - val_recall: 0.2167\n",
      "Epoch 81/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4990 - recall: 0.5858\n",
      "Epoch 81: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.5007 - recall: 0.5803 - val_loss: 0.8112 - val_recall: 0.3000\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4880 - recall: 0.5693\n",
      "Epoch 82: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4880 - recall: 0.5693 - val_loss: 0.8241 - val_recall: 0.2333\n",
      "Epoch 83/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4609 - recall: 0.6505\n",
      "Epoch 83: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4550 - recall: 0.6496 - val_loss: 0.8374 - val_recall: 0.2333\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4376 - recall: 0.6825\n",
      "Epoch 84: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4376 - recall: 0.6825 - val_loss: 0.9191 - val_recall: 0.1167\n",
      "Epoch 85/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.4672 - recall: 0.6089\n",
      "Epoch 85: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.4616 - recall: 0.6168 - val_loss: 0.8926 - val_recall: 0.2333\n",
      "Epoch 86/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.4870 - recall: 0.6162\n",
      "Epoch 86: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.4703 - recall: 0.6241 - val_loss: 0.9045 - val_recall: 0.1667\n",
      "Epoch 87/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3848 - recall: 0.7933\n",
      "Epoch 87: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3951 - recall: 0.7409 - val_loss: 0.9071 - val_recall: 0.2500\n",
      "Epoch 88/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3437 - recall: 0.7874\n",
      "Epoch 88: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3590 - recall: 0.7774 - val_loss: 0.9385 - val_recall: 0.4333\n",
      "Epoch 89/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3572 - recall: 0.7873\n",
      "Epoch 89: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3564 - recall: 0.7920 - val_loss: 1.0045 - val_recall: 0.1667\n",
      "Epoch 90/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3280 - recall: 0.7934\n",
      "Epoch 90: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3337 - recall: 0.7737 - val_loss: 1.0173 - val_recall: 0.1500\n",
      "Epoch 91/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2954 - recall: 0.8238\n",
      "Epoch 91: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3052 - recall: 0.8212 - val_loss: 1.0001 - val_recall: 0.3167\n",
      "Epoch 92/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2738 - recall: 0.8676\n",
      "Epoch 92: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2701 - recall: 0.8723 - val_loss: 1.0138 - val_recall: 0.2500\n",
      "Epoch 93/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2221 - recall: 0.9352\n",
      "Epoch 93: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2364 - recall: 0.8978 - val_loss: 1.0818 - val_recall: 0.2333\n",
      "Epoch 94/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2249 - recall: 0.9310\n",
      "Epoch 94: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2310 - recall: 0.9088 - val_loss: 1.1372 - val_recall: 0.2333\n",
      "Epoch 95/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.1901 - recall: 0.9502\n",
      "Epoch 95: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1963 - recall: 0.9453 - val_loss: 1.0913 - val_recall: 0.3167\n",
      "Epoch 96/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.1711 - recall: 0.9657\n",
      "Epoch 96: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1696 - recall: 0.9599 - val_loss: 1.1880 - val_recall: 0.2500\n",
      "Epoch 97/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.1473 - recall: 0.9749\n",
      "Epoch 97: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1445 - recall: 0.9708 - val_loss: 1.2277 - val_recall: 0.1500\n",
      "Epoch 98/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.1181 - recall: 0.9767\n",
      "Epoch 98: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1180 - recall: 0.9781 - val_loss: 1.2137 - val_recall: 0.3333\n",
      "Epoch 99/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.0930 - recall: 0.9957\n",
      "Epoch 99: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0947 - recall: 0.9927 - val_loss: 1.2593 - val_recall: 0.2833\n",
      "Epoch 100/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.0771 - recall: 0.9953\n",
      "Epoch 100: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0788 - recall: 0.9964 - val_loss: 1.3190 - val_recall: 0.2833\n",
      "Epoch 101/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.0623 - recall: 0.9954\n",
      "Epoch 101: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0634 - recall: 0.9964 - val_loss: 1.3465 - val_recall: 0.3000\n",
      "Epoch 102/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.0531 - recall: 1.0000\n",
      "Epoch 102: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0529 - recall: 1.0000 - val_loss: 1.4046 - val_recall: 0.2667\n",
      "Epoch 103/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.0425 - recall: 1.0000\n",
      "Epoch 103: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0430 - recall: 1.0000 - val_loss: 1.4138 - val_recall: 0.2000\n",
      "Epoch 104/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.0357 - recall: 1.0000\n",
      "Epoch 104: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0357 - recall: 1.0000 - val_loss: 1.4170 - val_recall: 0.2167\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.0319 - recall: 1.0000\n",
      "Epoch 105: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0319 - recall: 1.0000 - val_loss: 1.4705 - val_recall: 0.2333\n",
      "Epoch 106/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.0275 - recall: 1.0000\n",
      "Epoch 106: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0276 - recall: 1.0000 - val_loss: 1.4922 - val_recall: 0.2667\n",
      "Epoch 107/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.0227 - recall: 1.0000\n",
      "Epoch 107: val_recall did not improve from 0.60000\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0234 - recall: 1.0000 - val_loss: 1.5015 - val_recall: 0.2167\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "\n",
    "model_dense_2 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_2[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense.fit(X_train_rf_2, y_train_rf_2, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6772 - recall: 0.2482\n",
      "Epoch 1: val_recall improved from -inf to 0.05479, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 45ms/step - loss: 0.6772 - recall: 0.2482 - val_loss: 0.6641 - val_recall: 0.0548\n",
      "Epoch 2/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6663 - recall: 0.0310  \n",
      "Epoch 2: val_recall did not improve from 0.05479\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6663 - recall: 0.0288 - val_loss: 0.6676 - val_recall: 0.0548\n",
      "Epoch 3/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6575 - recall: 0.1205\n",
      "Epoch 3: val_recall did not improve from 0.05479\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6559 - recall: 0.1079 - val_loss: 0.6510 - val_recall: 0.0000e+00\n",
      "Epoch 4/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.6646 - recall: 0.1025\n",
      "Epoch 4: val_recall did not improve from 0.05479\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.6616 - recall: 0.0971 - val_loss: 0.6625 - val_recall: 0.0000e+00\n",
      "Epoch 5/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6565 - recall: 0.0952  \n",
      "Epoch 5: val_recall did not improve from 0.05479\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6504 - recall: 0.0899 - val_loss: 0.6639 - val_recall: 0.0548\n",
      "Epoch 6/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.6495 - recall: 0.0944\n",
      "Epoch 6: val_recall did not improve from 0.05479\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6509 - recall: 0.0791 - val_loss: 0.6641 - val_recall: 0.0137\n",
      "Epoch 7/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6449 - recall: 0.0560\n",
      "Epoch 7: val_recall improved from 0.05479 to 0.09589, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 27ms/step - loss: 0.6455 - recall: 0.0540 - val_loss: 0.6813 - val_recall: 0.0959\n",
      "Epoch 8/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.6459 - recall: 0.0340\n",
      "Epoch 8: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6462 - recall: 0.0432 - val_loss: 0.6745 - val_recall: 0.0822\n",
      "Epoch 9/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6467 - recall: 0.1245\n",
      "Epoch 9: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6461 - recall: 0.1187 - val_loss: 0.6678 - val_recall: 0.0685\n",
      "Epoch 10/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6409 - recall: 0.0571\n",
      "Epoch 10: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6440 - recall: 0.0504 - val_loss: 0.6657 - val_recall: 0.0548\n",
      "Epoch 11/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6394 - recall: 0.0522\n",
      "Epoch 11: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6413 - recall: 0.0540 - val_loss: 0.6701 - val_recall: 0.0822\n",
      "Epoch 12/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6432 - recall: 0.1185\n",
      "Epoch 12: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6435 - recall: 0.1151 - val_loss: 0.6632 - val_recall: 0.0411\n",
      "Epoch 13/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6331 - recall: 0.0512\n",
      "Epoch 13: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6425 - recall: 0.0504 - val_loss: 0.6718 - val_recall: 0.0685\n",
      "Epoch 14/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6458 - recall: 0.1331\n",
      "Epoch 14: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6431 - recall: 0.1331 - val_loss: 0.6655 - val_recall: 0.0685\n",
      "Epoch 15/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6401 - recall: 0.0882\n",
      "Epoch 15: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6393 - recall: 0.0863 - val_loss: 0.6644 - val_recall: 0.0548\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6397 - recall: 0.1475\n",
      "Epoch 16: val_recall did not improve from 0.09589\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6397 - recall: 0.1475 - val_loss: 0.6581 - val_recall: 0.0274\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6397 - recall: 0.0324  \n",
      "Epoch 17: val_recall improved from 0.09589 to 0.10959, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 24ms/step - loss: 0.6397 - recall: 0.0324 - val_loss: 0.6719 - val_recall: 0.1096\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6405 - recall: 0.0612\n",
      "Epoch 18: val_recall did not improve from 0.10959\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6405 - recall: 0.0612 - val_loss: 0.6600 - val_recall: 0.0685\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6361 - recall: 0.1007\n",
      "Epoch 19: val_recall did not improve from 0.10959\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6361 - recall: 0.1007 - val_loss: 0.6633 - val_recall: 0.0685\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6377 - recall: 0.1043\n",
      "Epoch 20: val_recall did not improve from 0.10959\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6377 - recall: 0.1043 - val_loss: 0.6591 - val_recall: 0.0411\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6342 - recall: 0.1295\n",
      "Epoch 21: val_recall did not improve from 0.10959\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6342 - recall: 0.1295 - val_loss: 0.6645 - val_recall: 0.0548\n",
      "Epoch 22/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6370 - recall: 0.1077\n",
      "Epoch 22: val_recall improved from 0.10959 to 0.12329, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 25ms/step - loss: 0.6366 - recall: 0.1151 - val_loss: 0.6777 - val_recall: 0.1233\n",
      "Epoch 23/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6396 - recall: 0.1984\n",
      "Epoch 23: val_recall did not improve from 0.12329\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6430 - recall: 0.1978 - val_loss: 0.6668 - val_recall: 0.0959\n",
      "Epoch 24/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6514 - recall: 0.1377\n",
      "Epoch 24: val_recall did not improve from 0.12329\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6481 - recall: 0.1367 - val_loss: 0.6560 - val_recall: 0.0274\n",
      "Epoch 25/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6327 - recall: 0.0870\n",
      "Epoch 25: val_recall improved from 0.12329 to 0.13699, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 25ms/step - loss: 0.6354 - recall: 0.1115 - val_loss: 0.6657 - val_recall: 0.1370\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6388 - recall: 0.2014\n",
      "Epoch 26: val_recall did not improve from 0.13699\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6388 - recall: 0.2014 - val_loss: 0.6664 - val_recall: 0.0548\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6403 - recall: 0.1187\n",
      "Epoch 27: val_recall did not improve from 0.13699\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6403 - recall: 0.1187 - val_loss: 0.6824 - val_recall: 0.0548\n",
      "Epoch 28/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6306 - recall: 0.1828\n",
      "Epoch 28: val_recall improved from 0.13699 to 0.21918, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 24ms/step - loss: 0.6332 - recall: 0.1871 - val_loss: 0.6877 - val_recall: 0.2192\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6348 - recall: 0.1115\n",
      "Epoch 29: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6348 - recall: 0.1115 - val_loss: 0.6703 - val_recall: 0.1233\n",
      "Epoch 30/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6332 - recall: 0.1931\n",
      "Epoch 30: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6307 - recall: 0.1942 - val_loss: 0.6678 - val_recall: 0.0959\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6282 - recall: 0.1511\n",
      "Epoch 31: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6282 - recall: 0.1511 - val_loss: 0.6805 - val_recall: 0.1370\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6189 - recall: 0.2050\n",
      "Epoch 32: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6189 - recall: 0.2050 - val_loss: 0.6693 - val_recall: 0.0411\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6130 - recall: 0.1871\n",
      "Epoch 33: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6130 - recall: 0.1871 - val_loss: 0.6696 - val_recall: 0.1644\n",
      "Epoch 34/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6224 - recall: 0.2421\n",
      "Epoch 34: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6145 - recall: 0.2410 - val_loss: 0.6600 - val_recall: 0.0411\n",
      "Epoch 35/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6238 - recall: 0.2036\n",
      "Epoch 35: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6205 - recall: 0.2050 - val_loss: 0.6970 - val_recall: 0.1096\n",
      "Epoch 36/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.6301 - recall: 0.2379\n",
      "Epoch 36: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6250 - recall: 0.2374 - val_loss: 0.6771 - val_recall: 0.1507\n",
      "Epoch 37/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6340 - recall: 0.2910\n",
      "Epoch 37: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6371 - recall: 0.2806 - val_loss: 0.6900 - val_recall: 0.0685\n",
      "Epoch 38/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6254 - recall: 0.1880\n",
      "Epoch 38: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6251 - recall: 0.2194 - val_loss: 0.6845 - val_recall: 0.1370\n",
      "Epoch 39/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6235 - recall: 0.1870\n",
      "Epoch 39: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6236 - recall: 0.1942 - val_loss: 0.6888 - val_recall: 0.0822\n",
      "Epoch 40/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6112 - recall: 0.2190\n",
      "Epoch 40: val_recall did not improve from 0.21918\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6094 - recall: 0.2230 - val_loss: 0.6723 - val_recall: 0.1096\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6092 - recall: 0.2374\n",
      "Epoch 41: val_recall improved from 0.21918 to 0.35616, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 24ms/step - loss: 0.6092 - recall: 0.2374 - val_loss: 0.7374 - val_recall: 0.3562\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6192 - recall: 0.2806\n",
      "Epoch 42: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6192 - recall: 0.2806 - val_loss: 0.7105 - val_recall: 0.0548\n",
      "Epoch 43/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6336 - recall: 0.1882\n",
      "Epoch 43: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6341 - recall: 0.1871 - val_loss: 0.6768 - val_recall: 0.0822\n",
      "Epoch 44/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6183 - recall: 0.2943\n",
      "Epoch 44: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6212 - recall: 0.3022 - val_loss: 0.6779 - val_recall: 0.0959\n",
      "Epoch 45/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.6266 - recall: 0.1918\n",
      "Epoch 45: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6252 - recall: 0.2050 - val_loss: 0.6869 - val_recall: 0.1781\n",
      "Epoch 46/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6103 - recall: 0.2636\n",
      "Epoch 46: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6120 - recall: 0.2554 - val_loss: 0.6790 - val_recall: 0.0822\n",
      "Epoch 47/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6047 - recall: 0.2993\n",
      "Epoch 47: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6014 - recall: 0.2986 - val_loss: 0.6855 - val_recall: 0.1233\n",
      "Epoch 48/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.6004 - recall: 0.2679\n",
      "Epoch 48: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5951 - recall: 0.2662 - val_loss: 0.6834 - val_recall: 0.0548\n",
      "Epoch 49/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5988 - recall: 0.2794\n",
      "Epoch 49: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5994 - recall: 0.2770 - val_loss: 0.7119 - val_recall: 0.1233\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.5875 - recall: 0.2878\n",
      "Epoch 50: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5875 - recall: 0.2878 - val_loss: 0.6971 - val_recall: 0.2740\n",
      "Epoch 51/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.5965 - recall: 0.3507\n",
      "Epoch 51: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5897 - recall: 0.3417 - val_loss: 0.6934 - val_recall: 0.0822\n",
      "Epoch 52/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5819 - recall: 0.3063\n",
      "Epoch 52: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5797 - recall: 0.3165 - val_loss: 0.7093 - val_recall: 0.1096\n",
      "Epoch 53/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5947 - recall: 0.2904\n",
      "Epoch 53: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5908 - recall: 0.2950 - val_loss: 0.7306 - val_recall: 0.1096\n",
      "Epoch 54/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5959 - recall: 0.3657\n",
      "Epoch 54: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6009 - recall: 0.3633 - val_loss: 0.6931 - val_recall: 0.2740\n",
      "Epoch 55/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5725 - recall: 0.3320\n",
      "Epoch 55: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5754 - recall: 0.3237 - val_loss: 0.7120 - val_recall: 0.1233\n",
      "Epoch 56/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5662 - recall: 0.3765\n",
      "Epoch 56: val_recall did not improve from 0.35616\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.5683 - recall: 0.3705 - val_loss: 0.7018 - val_recall: 0.1233\n",
      "Epoch 57/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5559 - recall: 0.3361\n",
      "Epoch 57: val_recall improved from 0.35616 to 0.52055, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_3)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 25ms/step - loss: 0.5588 - recall: 0.3525 - val_loss: 0.7305 - val_recall: 0.5205\n",
      "Epoch 58/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5605 - recall: 0.3963\n",
      "Epoch 58: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5594 - recall: 0.4029 - val_loss: 0.7092 - val_recall: 0.1096\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.5433 - recall: 0.3669\n",
      "Epoch 59: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5433 - recall: 0.3669 - val_loss: 0.7363 - val_recall: 0.1644\n",
      "Epoch 60/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.5447 - recall: 0.4109\n",
      "Epoch 60: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5482 - recall: 0.4065 - val_loss: 0.7272 - val_recall: 0.2877\n",
      "Epoch 61/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.5303 - recall: 0.4222\n",
      "Epoch 61: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.5365 - recall: 0.4029 - val_loss: 0.7340 - val_recall: 0.2055\n",
      "Epoch 62/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.5270 - recall: 0.5000\n",
      "Epoch 62: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5239 - recall: 0.4964 - val_loss: 0.7757 - val_recall: 0.2055\n",
      "Epoch 63/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5161 - recall: 0.4762\n",
      "Epoch 63: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5120 - recall: 0.4784 - val_loss: 0.7270 - val_recall: 0.1644\n",
      "Epoch 64/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5196 - recall: 0.4661\n",
      "Epoch 64: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.5163 - recall: 0.4676 - val_loss: 0.7456 - val_recall: 0.1644\n",
      "Epoch 65/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.5086 - recall: 0.5290\n",
      "Epoch 65: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.4937 - recall: 0.5216 - val_loss: 0.8100 - val_recall: 0.0822\n",
      "Epoch 66/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.4773 - recall: 0.5400\n",
      "Epoch 66: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.4923 - recall: 0.5252 - val_loss: 0.7850 - val_recall: 0.1507\n",
      "Epoch 67/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.4682 - recall: 0.5570\n",
      "Epoch 67: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.4715 - recall: 0.5683 - val_loss: 0.7735 - val_recall: 0.1233\n",
      "Epoch 68/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.4614 - recall: 0.5132\n",
      "Epoch 68: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.4589 - recall: 0.5432 - val_loss: 0.8138 - val_recall: 0.1233\n",
      "Epoch 69/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.4612 - recall: 0.5422\n",
      "Epoch 69: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.4595 - recall: 0.5468 - val_loss: 0.8079 - val_recall: 0.3836\n",
      "Epoch 70/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.4495 - recall: 0.6044\n",
      "Epoch 70: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.4572 - recall: 0.5935 - val_loss: 0.7790 - val_recall: 0.2740\n",
      "Epoch 71/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.4467 - recall: 0.5984\n",
      "Epoch 71: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.4612 - recall: 0.6043 - val_loss: 0.8038 - val_recall: 0.3973\n",
      "Epoch 72/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.4398 - recall: 0.6653\n",
      "Epoch 72: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.4331 - recall: 0.6439 - val_loss: 0.7856 - val_recall: 0.2055\n",
      "Epoch 73/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.4045 - recall: 0.6979\n",
      "Epoch 73: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.4089 - recall: 0.6835 - val_loss: 0.7894 - val_recall: 0.1918\n",
      "Epoch 74/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.3644 - recall: 0.7149\n",
      "Epoch 74: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.3713 - recall: 0.7014 - val_loss: 0.7996 - val_recall: 0.2877\n",
      "Epoch 75/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.3278 - recall: 0.7686\n",
      "Epoch 75: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.3351 - recall: 0.7518 - val_loss: 0.8681 - val_recall: 0.2192\n",
      "Epoch 76/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.3063 - recall: 0.8057\n",
      "Epoch 76: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.3087 - recall: 0.7986 - val_loss: 0.8589 - val_recall: 0.2877\n",
      "Epoch 77/500\n",
      "20/25 [=======================>......] - ETA: 0s - loss: 0.2941 - recall: 0.7857\n",
      "Epoch 77: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.2902 - recall: 0.7734 - val_loss: 0.8858 - val_recall: 0.2740\n",
      "Epoch 78/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.2598 - recall: 0.8396\n",
      "Epoch 78: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.2647 - recall: 0.8309 - val_loss: 0.9010 - val_recall: 0.2329\n",
      "Epoch 79/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.2932 - recall: 0.7695\n",
      "Epoch 79: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.2950 - recall: 0.7662 - val_loss: 0.9142 - val_recall: 0.3014\n",
      "Epoch 80/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.2555 - recall: 0.8603\n",
      "Epoch 80: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.2566 - recall: 0.8561 - val_loss: 0.9175 - val_recall: 0.2740\n",
      "Epoch 81/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.2350 - recall: 0.8697\n",
      "Epoch 81: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.2371 - recall: 0.8633 - val_loss: 0.9582 - val_recall: 0.3288\n",
      "Epoch 82/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.1740 - recall: 0.9393\n",
      "Epoch 82: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.1777 - recall: 0.9281 - val_loss: 0.9767 - val_recall: 0.3014\n",
      "Epoch 83/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.1340 - recall: 0.9672\n",
      "Epoch 83: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.1340 - recall: 0.9640 - val_loss: 1.0280 - val_recall: 0.2877\n",
      "Epoch 84/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.1209 - recall: 0.9774\n",
      "Epoch 84: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.1202 - recall: 0.9748 - val_loss: 1.0755 - val_recall: 0.2466\n",
      "Epoch 85/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0971 - recall: 0.9680\n",
      "Epoch 85: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0967 - recall: 0.9712 - val_loss: 1.0688 - val_recall: 0.3425\n",
      "Epoch 86/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.0760 - recall: 0.9885\n",
      "Epoch 86: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0755 - recall: 0.9892 - val_loss: 1.1029 - val_recall: 0.3014\n",
      "Epoch 87/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.0526 - recall: 0.9926\n",
      "Epoch 87: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0524 - recall: 0.9928 - val_loss: 1.1504 - val_recall: 0.3014\n",
      "Epoch 88/500\n",
      "19/25 [=====================>........] - ETA: 0s - loss: 0.0390 - recall: 0.9951\n",
      "Epoch 88: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0404 - recall: 0.9964 - val_loss: 1.2014 - val_recall: 0.3288\n",
      "Epoch 89/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.0339 - recall: 0.9963\n",
      "Epoch 89: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0340 - recall: 0.9964 - val_loss: 1.2118 - val_recall: 0.2740\n",
      "Epoch 90/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0289 - recall: 1.0000\n",
      "Epoch 90: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0287 - recall: 1.0000 - val_loss: 1.2394 - val_recall: 0.3151\n",
      "Epoch 91/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0240 - recall: 1.0000\n",
      "Epoch 91: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0240 - recall: 1.0000 - val_loss: 1.2743 - val_recall: 0.2877\n",
      "Epoch 92/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0213 - recall: 1.0000\n",
      "Epoch 92: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0212 - recall: 1.0000 - val_loss: 1.3011 - val_recall: 0.2603\n",
      "Epoch 93/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.0182 - recall: 1.0000\n",
      "Epoch 93: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0182 - recall: 1.0000 - val_loss: 1.3167 - val_recall: 0.2877\n",
      "Epoch 94/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0157 - recall: 1.0000\n",
      "Epoch 94: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0154 - recall: 1.0000 - val_loss: 1.3348 - val_recall: 0.3151\n",
      "Epoch 95/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.0141 - recall: 1.0000\n",
      "Epoch 95: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0141 - recall: 1.0000 - val_loss: 1.3664 - val_recall: 0.3014\n",
      "Epoch 96/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.0125 - recall: 1.0000\n",
      "Epoch 96: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0125 - recall: 1.0000 - val_loss: 1.3689 - val_recall: 0.3014\n",
      "Epoch 97/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.0112 - recall: 1.0000\n",
      "Epoch 97: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0113 - recall: 1.0000 - val_loss: 1.3808 - val_recall: 0.2877\n",
      "Epoch 98/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0103 - recall: 1.0000\n",
      "Epoch 98: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0103 - recall: 1.0000 - val_loss: 1.3945 - val_recall: 0.2877\n",
      "Epoch 99/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0091 - recall: 1.0000\n",
      "Epoch 99: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0091 - recall: 1.0000 - val_loss: 1.4121 - val_recall: 0.3014\n",
      "Epoch 100/500\n",
      "21/25 [========================>.....] - ETA: 0s - loss: 0.0083 - recall: 1.0000\n",
      "Epoch 100: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0085 - recall: 1.0000 - val_loss: 1.4273 - val_recall: 0.3014\n",
      "Epoch 101/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.0078 - recall: 1.0000\n",
      "Epoch 101: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0079 - recall: 1.0000 - val_loss: 1.4395 - val_recall: 0.2877\n",
      "Epoch 102/500\n",
      "22/25 [=========================>....] - ETA: 0s - loss: 0.0071 - recall: 1.0000\n",
      "Epoch 102: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0073 - recall: 1.0000 - val_loss: 1.4442 - val_recall: 0.2877\n",
      "Epoch 103/500\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.0067 - recall: 1.0000\n",
      "Epoch 103: val_recall did not improve from 0.52055\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0067 - recall: 1.0000 - val_loss: 1.4713 - val_recall: 0.2877\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "\n",
    "model_dense_3 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_3[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_3, y_train_rf_3, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 700)               18900     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 800)               560800    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1000)              801000    \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1381701 (5.27 MB)\n",
      "Trainable params: 1381701 (5.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dense_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.0142 - recall: 0.9966\n",
      "Epoch 1: val_recall improved from -inf to 0.34247, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0142 - recall: 0.9966 - val_loss: 2.2370 - val_recall: 0.3425\n",
      "Epoch 2/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 0.0157 - recall: 0.9965\n",
      "Epoch 2: val_recall improved from 0.34247 to 0.38356, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_4)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 24ms/step - loss: 0.0155 - recall: 0.9966 - val_loss: 2.5055 - val_recall: 0.3836\n",
      "Epoch 3/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 0.0190 - recall: 0.9848\n",
      "Epoch 3: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0178 - recall: 0.9866 - val_loss: 2.0419 - val_recall: 0.3836\n",
      "Epoch 4/500\n",
      "22/27 [=======================>......] - ETA: 0s - loss: 0.0065 - recall: 1.0000\n",
      "Epoch 4: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0059 - recall: 1.0000 - val_loss: 2.1307 - val_recall: 0.3699\n",
      "Epoch 5/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 0.0017 - recall: 1.0000\n",
      "Epoch 5: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0017 - recall: 1.0000 - val_loss: 2.1221 - val_recall: 0.3014\n",
      "Epoch 6/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 9.2203e-04 - recall: 1.0000\n",
      "Epoch 6: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 9.1834e-04 - recall: 1.0000 - val_loss: 2.1636 - val_recall: 0.3562\n",
      "Epoch 7/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 7.1963e-04 - recall: 1.0000\n",
      "Epoch 7: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 7.0985e-04 - recall: 1.0000 - val_loss: 2.1703 - val_recall: 0.3288\n",
      "Epoch 8/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 6.2169e-04 - recall: 1.0000\n",
      "Epoch 8: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 6.1447e-04 - recall: 1.0000 - val_loss: 2.1849 - val_recall: 0.3288\n",
      "Epoch 9/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 5.3739e-04 - recall: 1.0000\n",
      "Epoch 9: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 5.4588e-04 - recall: 1.0000 - val_loss: 2.1954 - val_recall: 0.3288\n",
      "Epoch 10/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 5.0243e-04 - recall: 1.0000\n",
      "Epoch 10: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 4.9566e-04 - recall: 1.0000 - val_loss: 2.2052 - val_recall: 0.3288\n",
      "Epoch 11/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 4.5194e-04 - recall: 1.0000\n",
      "Epoch 11: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 4.5412e-04 - recall: 1.0000 - val_loss: 2.2183 - val_recall: 0.3288\n",
      "Epoch 12/500\n",
      "21/27 [======================>.......] - ETA: 0s - loss: 4.1974e-04 - recall: 1.0000\n",
      "Epoch 12: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 4.1870e-04 - recall: 1.0000 - val_loss: 2.2263 - val_recall: 0.3151\n",
      "Epoch 13/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.8948e-04 - recall: 1.0000\n",
      "Epoch 13: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.8820e-04 - recall: 1.0000 - val_loss: 2.2442 - val_recall: 0.3288\n",
      "Epoch 14/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.6286e-04 - recall: 1.0000\n",
      "Epoch 14: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.6300e-04 - recall: 1.0000 - val_loss: 2.2539 - val_recall: 0.3014\n",
      "Epoch 15/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.3836e-04 - recall: 1.0000\n",
      "Epoch 15: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.3937e-04 - recall: 1.0000 - val_loss: 2.2671 - val_recall: 0.3288\n",
      "Epoch 16/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.1846e-04 - recall: 1.0000\n",
      "Epoch 16: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.1803e-04 - recall: 1.0000 - val_loss: 2.2765 - val_recall: 0.3151\n",
      "Epoch 17/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.0497e-04 - recall: 1.0000\n",
      "Epoch 17: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.0062e-04 - recall: 1.0000 - val_loss: 2.2888 - val_recall: 0.3151\n",
      "Epoch 18/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 2.8363e-04 - recall: 1.0000\n",
      "Epoch 18: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.8357e-04 - recall: 1.0000 - val_loss: 2.2960 - val_recall: 0.3151\n",
      "Epoch 19/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.6650e-04 - recall: 1.0000\n",
      "Epoch 19: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.6877e-04 - recall: 1.0000 - val_loss: 2.3024 - val_recall: 0.3151\n",
      "Epoch 20/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.5579e-04 - recall: 1.0000\n",
      "Epoch 20: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.5502e-04 - recall: 1.0000 - val_loss: 2.3094 - val_recall: 0.3151\n",
      "Epoch 21/500\n",
      "22/27 [=======================>......] - ETA: 0s - loss: 2.4854e-04 - recall: 1.0000\n",
      "Epoch 21: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 2.4240e-04 - recall: 1.0000 - val_loss: 2.3177 - val_recall: 0.3151\n",
      "Epoch 22/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 2.3419e-04 - recall: 1.0000\n",
      "Epoch 22: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 2.3077e-04 - recall: 1.0000 - val_loss: 2.3291 - val_recall: 0.3151\n",
      "Epoch 23/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 2.2091e-04 - recall: 1.0000\n",
      "Epoch 23: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 2.2034e-04 - recall: 1.0000 - val_loss: 2.3417 - val_recall: 0.3151\n",
      "Epoch 24/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 2.1482e-04 - recall: 1.0000\n",
      "Epoch 24: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 2.1089e-04 - recall: 1.0000 - val_loss: 2.3482 - val_recall: 0.3151\n",
      "Epoch 25/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.9953e-04 - recall: 1.0000\n",
      "Epoch 25: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.0211e-04 - recall: 1.0000 - val_loss: 2.3581 - val_recall: 0.3288\n",
      "Epoch 26/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.9702e-04 - recall: 1.0000\n",
      "Epoch 26: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.9417e-04 - recall: 1.0000 - val_loss: 2.3614 - val_recall: 0.3151\n",
      "Epoch 27/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.8146e-04 - recall: 1.0000\n",
      "Epoch 27: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.8499e-04 - recall: 1.0000 - val_loss: 2.3730 - val_recall: 0.3151\n",
      "Epoch 28/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.7965e-04 - recall: 1.0000\n",
      "Epoch 28: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.7832e-04 - recall: 1.0000 - val_loss: 2.3809 - val_recall: 0.3151\n",
      "Epoch 29/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 1.7301e-04 - recall: 1.0000\n",
      "Epoch 29: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.7155e-04 - recall: 1.0000 - val_loss: 2.3872 - val_recall: 0.3151\n",
      "Epoch 30/500\n",
      "22/27 [=======================>......] - ETA: 0s - loss: 1.6125e-04 - recall: 1.0000\n",
      "Epoch 30: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.6461e-04 - recall: 1.0000 - val_loss: 2.3984 - val_recall: 0.3151\n",
      "Epoch 31/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.6041e-04 - recall: 1.0000\n",
      "Epoch 31: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.5878e-04 - recall: 1.0000 - val_loss: 2.4032 - val_recall: 0.3151\n",
      "Epoch 32/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.5101e-04 - recall: 1.0000\n",
      "Epoch 32: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.5289e-04 - recall: 1.0000 - val_loss: 2.4123 - val_recall: 0.3151\n",
      "Epoch 33/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.4752e-04 - recall: 1.0000\n",
      "Epoch 33: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.4753e-04 - recall: 1.0000 - val_loss: 2.4140 - val_recall: 0.3151\n",
      "Epoch 34/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.4346e-04 - recall: 1.0000\n",
      "Epoch 34: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.4253e-04 - recall: 1.0000 - val_loss: 2.4266 - val_recall: 0.3151\n",
      "Epoch 35/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.3536e-04 - recall: 1.0000\n",
      "Epoch 35: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.3751e-04 - recall: 1.0000 - val_loss: 2.4317 - val_recall: 0.3151\n",
      "Epoch 36/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.3490e-04 - recall: 1.0000\n",
      "Epoch 36: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.3309e-04 - recall: 1.0000 - val_loss: 2.4398 - val_recall: 0.3151\n",
      "Epoch 37/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.2998e-04 - recall: 1.0000\n",
      "Epoch 37: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.2868e-04 - recall: 1.0000 - val_loss: 2.4433 - val_recall: 0.3151\n",
      "Epoch 38/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.2645e-04 - recall: 1.0000\n",
      "Epoch 38: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.2462e-04 - recall: 1.0000 - val_loss: 2.4523 - val_recall: 0.3151\n",
      "Epoch 39/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 1.1965e-04 - recall: 1.0000\n",
      "Epoch 39: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.2054e-04 - recall: 1.0000 - val_loss: 2.4578 - val_recall: 0.3151\n",
      "Epoch 40/500\n",
      "22/27 [=======================>......] - ETA: 0s - loss: 1.1742e-04 - recall: 1.0000\n",
      "Epoch 40: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.1677e-04 - recall: 1.0000 - val_loss: 2.4645 - val_recall: 0.3151\n",
      "Epoch 41/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.1284e-04 - recall: 1.0000\n",
      "Epoch 41: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.1337e-04 - recall: 1.0000 - val_loss: 2.4704 - val_recall: 0.3151\n",
      "Epoch 42/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.0976e-04 - recall: 1.0000\n",
      "Epoch 42: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.0967e-04 - recall: 1.0000 - val_loss: 2.4708 - val_recall: 0.3014\n",
      "Epoch 43/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.0708e-04 - recall: 1.0000\n",
      "Epoch 43: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.0641e-04 - recall: 1.0000 - val_loss: 2.4809 - val_recall: 0.3151\n",
      "Epoch 44/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 1.0371e-04 - recall: 1.0000\n",
      "Epoch 44: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 1.0323e-04 - recall: 1.0000 - val_loss: 2.4891 - val_recall: 0.3151\n",
      "Epoch 45/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 1.0025e-04 - recall: 1.0000\n",
      "Epoch 45: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 1.0037e-04 - recall: 1.0000 - val_loss: 2.4977 - val_recall: 0.3151\n",
      "Epoch 46/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 9.7195e-05 - recall: 1.0000\n",
      "Epoch 46: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 9.7385e-05 - recall: 1.0000 - val_loss: 2.5039 - val_recall: 0.3151\n",
      "Epoch 47/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 9.3798e-05 - recall: 1.0000\n",
      "Epoch 47: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 9.4613e-05 - recall: 1.0000 - val_loss: 2.5072 - val_recall: 0.3151\n",
      "Epoch 48/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 9.2011e-05 - recall: 1.0000\n",
      "Epoch 48: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 9.2154e-05 - recall: 1.0000 - val_loss: 2.5137 - val_recall: 0.3151\n",
      "Epoch 49/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 8.8780e-05 - recall: 1.0000\n",
      "Epoch 49: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 8.9467e-05 - recall: 1.0000 - val_loss: 2.5212 - val_recall: 0.3288\n",
      "Epoch 50/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 8.6753e-05 - recall: 1.0000\n",
      "Epoch 50: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 8.7120e-05 - recall: 1.0000 - val_loss: 2.5266 - val_recall: 0.3288\n",
      "Epoch 51/500\n",
      "27/27 [==============================] - ETA: 0s - loss: 8.4770e-05 - recall: 1.0000\n",
      "Epoch 51: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 8.4770e-05 - recall: 1.0000 - val_loss: 2.5309 - val_recall: 0.3014\n",
      "Epoch 52/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 8.3997e-05 - recall: 1.0000\n",
      "Epoch 52: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 8.2644e-05 - recall: 1.0000 - val_loss: 2.5381 - val_recall: 0.3014\n",
      "Epoch 53/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 8.0498e-05 - recall: 1.0000\n",
      "Epoch 53: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 8.0454e-05 - recall: 1.0000 - val_loss: 2.5458 - val_recall: 0.3014\n",
      "Epoch 54/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 7.7193e-05 - recall: 1.0000\n",
      "Epoch 54: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 7.8302e-05 - recall: 1.0000 - val_loss: 2.5472 - val_recall: 0.3014\n",
      "Epoch 55/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 7.6897e-05 - recall: 1.0000\n",
      "Epoch 55: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 7.6408e-05 - recall: 1.0000 - val_loss: 2.5542 - val_recall: 0.3288\n",
      "Epoch 56/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 7.5098e-05 - recall: 1.0000\n",
      "Epoch 56: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 7.4410e-05 - recall: 1.0000 - val_loss: 2.5607 - val_recall: 0.3151\n",
      "Epoch 57/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 7.2642e-05 - recall: 1.0000\n",
      "Epoch 57: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 7.2666e-05 - recall: 1.0000 - val_loss: 2.5682 - val_recall: 0.3151\n",
      "Epoch 58/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 7.0339e-05 - recall: 1.0000\n",
      "Epoch 58: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 7.0744e-05 - recall: 1.0000 - val_loss: 2.5687 - val_recall: 0.3151\n",
      "Epoch 59/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 6.7776e-05 - recall: 1.0000\n",
      "Epoch 59: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.9006e-05 - recall: 1.0000 - val_loss: 2.5746 - val_recall: 0.3151\n",
      "Epoch 60/500\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 6.7386e-05 - recall: 1.0000\n",
      "Epoch 60: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.7232e-05 - recall: 1.0000 - val_loss: 2.5788 - val_recall: 0.3014\n",
      "Epoch 61/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 6.5187e-05 - recall: 1.0000\n",
      "Epoch 61: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.5662e-05 - recall: 1.0000 - val_loss: 2.5836 - val_recall: 0.3151\n",
      "Epoch 62/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 6.4702e-05 - recall: 1.0000\n",
      "Epoch 62: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.4086e-05 - recall: 1.0000 - val_loss: 2.5886 - val_recall: 0.3151\n",
      "Epoch 63/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 6.2936e-05 - recall: 1.0000\n",
      "Epoch 63: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.2792e-05 - recall: 1.0000 - val_loss: 2.5977 - val_recall: 0.3151\n",
      "Epoch 64/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 6.1275e-05 - recall: 1.0000\n",
      "Epoch 64: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 6.1072e-05 - recall: 1.0000 - val_loss: 2.6029 - val_recall: 0.3151\n",
      "Epoch 65/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.9745e-05 - recall: 1.0000\n",
      "Epoch 65: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.9639e-05 - recall: 1.0000 - val_loss: 2.6081 - val_recall: 0.3151\n",
      "Epoch 66/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 5.9124e-05 - recall: 1.0000\n",
      "Epoch 66: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 5.8345e-05 - recall: 1.0000 - val_loss: 2.6132 - val_recall: 0.3151\n",
      "Epoch 67/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.6946e-05 - recall: 1.0000\n",
      "Epoch 67: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.6935e-05 - recall: 1.0000 - val_loss: 2.6173 - val_recall: 0.3151\n",
      "Epoch 68/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.4706e-05 - recall: 1.0000\n",
      "Epoch 68: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.5739e-05 - recall: 1.0000 - val_loss: 2.6206 - val_recall: 0.3151\n",
      "Epoch 69/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.3695e-05 - recall: 1.0000\n",
      "Epoch 69: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.4506e-05 - recall: 1.0000 - val_loss: 2.6250 - val_recall: 0.3151\n",
      "Epoch 70/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.3473e-05 - recall: 1.0000\n",
      "Epoch 70: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.3264e-05 - recall: 1.0000 - val_loss: 2.6300 - val_recall: 0.3151\n",
      "Epoch 71/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.1886e-05 - recall: 1.0000\n",
      "Epoch 71: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.2034e-05 - recall: 1.0000 - val_loss: 2.6381 - val_recall: 0.3151\n",
      "Epoch 72/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 5.0722e-05 - recall: 1.0000\n",
      "Epoch 72: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 5.0922e-05 - recall: 1.0000 - val_loss: 2.6438 - val_recall: 0.3151\n",
      "Epoch 73/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.9488e-05 - recall: 1.0000\n",
      "Epoch 73: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.9832e-05 - recall: 1.0000 - val_loss: 2.6416 - val_recall: 0.3151\n",
      "Epoch 74/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 4.8730e-05 - recall: 1.0000\n",
      "Epoch 74: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 4.8703e-05 - recall: 1.0000 - val_loss: 2.6521 - val_recall: 0.3151\n",
      "Epoch 75/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 4.7840e-05 - recall: 1.0000\n",
      "Epoch 75: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.7627e-05 - recall: 1.0000 - val_loss: 2.6551 - val_recall: 0.3151\n",
      "Epoch 76/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.6969e-05 - recall: 1.0000\n",
      "Epoch 76: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.6595e-05 - recall: 1.0000 - val_loss: 2.6592 - val_recall: 0.3151\n",
      "Epoch 77/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.4535e-05 - recall: 1.0000\n",
      "Epoch 77: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.5608e-05 - recall: 1.0000 - val_loss: 2.6666 - val_recall: 0.3151\n",
      "Epoch 78/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 4.3649e-05 - recall: 1.0000\n",
      "Epoch 78: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.4705e-05 - recall: 1.0000 - val_loss: 2.6711 - val_recall: 0.3151\n",
      "Epoch 79/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.3364e-05 - recall: 1.0000\n",
      "Epoch 79: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.3710e-05 - recall: 1.0000 - val_loss: 2.6750 - val_recall: 0.3151\n",
      "Epoch 80/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 4.4100e-05 - recall: 1.0000\n",
      "Epoch 80: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.2824e-05 - recall: 1.0000 - val_loss: 2.6767 - val_recall: 0.3151\n",
      "Epoch 81/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.1844e-05 - recall: 1.0000\n",
      "Epoch 81: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.1906e-05 - recall: 1.0000 - val_loss: 2.6814 - val_recall: 0.3151\n",
      "Epoch 82/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 4.1786e-05 - recall: 1.0000\n",
      "Epoch 82: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.1057e-05 - recall: 1.0000 - val_loss: 2.6879 - val_recall: 0.3151\n",
      "Epoch 83/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 4.0342e-05 - recall: 1.0000\n",
      "Epoch 83: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 4.0262e-05 - recall: 1.0000 - val_loss: 2.6908 - val_recall: 0.3151\n",
      "Epoch 84/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.9591e-05 - recall: 1.0000\n",
      "Epoch 84: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.9504e-05 - recall: 1.0000 - val_loss: 2.6993 - val_recall: 0.3151\n",
      "Epoch 85/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.7783e-05 - recall: 1.0000\n",
      "Epoch 85: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.8584e-05 - recall: 1.0000 - val_loss: 2.7022 - val_recall: 0.3151\n",
      "Epoch 86/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.7925e-05 - recall: 1.0000\n",
      "Epoch 86: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.7792e-05 - recall: 1.0000 - val_loss: 2.7079 - val_recall: 0.3151\n",
      "Epoch 87/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.6758e-05 - recall: 1.0000\n",
      "Epoch 87: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.7032e-05 - recall: 1.0000 - val_loss: 2.7111 - val_recall: 0.3151\n",
      "Epoch 88/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.6062e-05 - recall: 1.0000\n",
      "Epoch 88: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.6289e-05 - recall: 1.0000 - val_loss: 2.7156 - val_recall: 0.3151\n",
      "Epoch 89/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.5231e-05 - recall: 1.0000\n",
      "Epoch 89: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.5577e-05 - recall: 1.0000 - val_loss: 2.7204 - val_recall: 0.3151\n",
      "Epoch 90/500\n",
      "23/27 [========================>.....] - ETA: 0s - loss: 3.5223e-05 - recall: 1.0000\n",
      "Epoch 90: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 3.4878e-05 - recall: 1.0000 - val_loss: 2.7233 - val_recall: 0.3151\n",
      "Epoch 91/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.4536e-05 - recall: 1.0000\n",
      "Epoch 91: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.4202e-05 - recall: 1.0000 - val_loss: 2.7281 - val_recall: 0.3151\n",
      "Epoch 92/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 3.3296e-05 - recall: 1.0000\n",
      "Epoch 92: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.3550e-05 - recall: 1.0000 - val_loss: 2.7352 - val_recall: 0.3151\n",
      "Epoch 93/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.3319e-05 - recall: 1.0000\n",
      "Epoch 93: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.2903e-05 - recall: 1.0000 - val_loss: 2.7374 - val_recall: 0.3151\n",
      "Epoch 94/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.2111e-05 - recall: 1.0000\n",
      "Epoch 94: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.2237e-05 - recall: 1.0000 - val_loss: 2.7414 - val_recall: 0.3151\n",
      "Epoch 95/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.1262e-05 - recall: 1.0000\n",
      "Epoch 95: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.1647e-05 - recall: 1.0000 - val_loss: 2.7464 - val_recall: 0.3151\n",
      "Epoch 96/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.1174e-05 - recall: 1.0000\n",
      "Epoch 96: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.1020e-05 - recall: 1.0000 - val_loss: 2.7512 - val_recall: 0.3151\n",
      "Epoch 97/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 3.0054e-05 - recall: 1.0000\n",
      "Epoch 97: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 3.0446e-05 - recall: 1.0000 - val_loss: 2.7558 - val_recall: 0.3151\n",
      "Epoch 98/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 2.9235e-05 - recall: 1.0000\n",
      "Epoch 98: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.9840e-05 - recall: 1.0000 - val_loss: 2.7608 - val_recall: 0.3151\n",
      "Epoch 99/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.9420e-05 - recall: 1.0000\n",
      "Epoch 99: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.9317e-05 - recall: 1.0000 - val_loss: 2.7631 - val_recall: 0.3151\n",
      "Epoch 100/500\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 2.8833e-05 - recall: 1.0000\n",
      "Epoch 100: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.8739e-05 - recall: 1.0000 - val_loss: 2.7668 - val_recall: 0.3151\n",
      "Epoch 101/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.8625e-05 - recall: 1.0000\n",
      "Epoch 101: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.8213e-05 - recall: 1.0000 - val_loss: 2.7727 - val_recall: 0.3151\n",
      "Epoch 102/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.7891e-05 - recall: 1.0000\n",
      "Epoch 102: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.7665e-05 - recall: 1.0000 - val_loss: 2.7774 - val_recall: 0.3151\n",
      "Epoch 103/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 2.6882e-05 - recall: 1.0000\n",
      "Epoch 103: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 2.7158e-05 - recall: 1.0000 - val_loss: 2.7815 - val_recall: 0.3151\n",
      "Epoch 104/500\n",
      "24/27 [=========================>....] - ETA: 0s - loss: 2.6656e-05 - recall: 1.0000\n",
      "Epoch 104: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 2.6688e-05 - recall: 1.0000 - val_loss: 2.7866 - val_recall: 0.3151\n",
      "Epoch 105/500\n",
      "25/27 [==========================>...] - ETA: 0s - loss: 2.6520e-05 - recall: 1.0000\n",
      "Epoch 105: val_recall did not improve from 0.38356\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 2.6153e-05 - recall: 1.0000 - val_loss: 2.7887 - val_recall: 0.3151\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "\n",
    "model_dense_4 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_4[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_4, y_train_rf_4, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 700)               18900     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 800)               560800    \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1000)              801000    \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1381701 (5.27 MB)\n",
      "Trainable params: 1381701 (5.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dense_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 1.7565 - recall: 0.3436\n",
      "Epoch 1: val_recall improved from -inf to 0.10667, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 38ms/step - loss: 1.6279 - recall: 0.3274 - val_loss: 1.1671 - val_recall: 0.1067\n",
      "Epoch 2/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.8678 - recall: 0.3210\n",
      "Epoch 2: val_recall improved from 0.10667 to 0.14667, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 26ms/step - loss: 0.8654 - recall: 0.3167 - val_loss: 0.8583 - val_recall: 0.1467\n",
      "Epoch 3/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6180 - recall: 0.3333  \n",
      "Epoch 3: val_recall improved from 0.14667 to 0.22667, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 24ms/step - loss: 0.6139 - recall: 0.3381 - val_loss: 0.7529 - val_recall: 0.2267\n",
      "Epoch 4/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.5441 - recall: 0.5354\n",
      "Epoch 4: val_recall improved from 0.22667 to 0.37333, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 23ms/step - loss: 0.5445 - recall: 0.4733 - val_loss: 0.8084 - val_recall: 0.3733\n",
      "Epoch 5/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.4031 - recall: 0.5767\n",
      "Epoch 5: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4217 - recall: 0.5801 - val_loss: 0.7959 - val_recall: 0.3067\n",
      "Epoch 6/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.3501 - recall: 0.7276\n",
      "Epoch 6: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3559 - recall: 0.7295 - val_loss: 0.9015 - val_recall: 0.2400\n",
      "Epoch 7/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.9867 - recall: 0.2932\n",
      "Epoch 7: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9954 - recall: 0.3025 - val_loss: 1.0258 - val_recall: 0.3600\n",
      "Epoch 8/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.8547 - recall: 0.3199\n",
      "Epoch 8: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.8496 - recall: 0.3167 - val_loss: 0.8933 - val_recall: 0.2133\n",
      "Epoch 9/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.7438 - recall: 0.2735\n",
      "Epoch 9: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.7503 - recall: 0.2598 - val_loss: 0.8291 - val_recall: 0.1733\n",
      "Epoch 10/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 1.0648 - recall: 0.3481\n",
      "Epoch 10: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0541 - recall: 0.3488 - val_loss: 0.9450 - val_recall: 0.3200\n",
      "Epoch 11/500\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.8640 - recall: 0.3156\n",
      "Epoch 11: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.8518 - recall: 0.3060 - val_loss: 0.9067 - val_recall: 0.0933\n",
      "Epoch 12/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.7788 - recall: 0.2767\n",
      "Epoch 12: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.7682 - recall: 0.2633 - val_loss: 0.7946 - val_recall: 0.3600\n",
      "Epoch 13/500\n",
      "21/26 [=======================>......] - ETA: 0s - loss: 0.6710 - recall: 0.3241\n",
      "Epoch 13: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.6812 - recall: 0.3096 - val_loss: 0.7723 - val_recall: 0.2533\n",
      "Epoch 14/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6361 - recall: 0.3462\n",
      "Epoch 14: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6413 - recall: 0.3416 - val_loss: 0.7539 - val_recall: 0.1600\n",
      "Epoch 15/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6093 - recall: 0.2721\n",
      "Epoch 15: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6107 - recall: 0.2740 - val_loss: 0.7461 - val_recall: 0.1733\n",
      "Epoch 16/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6124 - recall: 0.3238\n",
      "Epoch 16: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5866 - recall: 0.2847 - val_loss: 0.7441 - val_recall: 0.0933\n",
      "Epoch 17/500\n",
      "17/26 [==================>...........] - ETA: 0s - loss: 0.5879 - recall: 0.4221\n",
      "Epoch 17: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5756 - recall: 0.3452 - val_loss: 0.7432 - val_recall: 0.1333\n",
      "Epoch 18/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6713 - recall: 0.2972\n",
      "Epoch 18: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7198 - recall: 0.2989 - val_loss: 0.8833 - val_recall: 0.1467\n",
      "Epoch 19/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.8227 - recall: 0.2613\n",
      "Epoch 19: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.8219 - recall: 0.2883 - val_loss: 0.8103 - val_recall: 0.1867\n",
      "Epoch 20/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6948 - recall: 0.3069\n",
      "Epoch 20: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7591 - recall: 0.3274 - val_loss: 0.8729 - val_recall: 0.3333\n",
      "Epoch 21/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.8602 - recall: 0.2217\n",
      "Epoch 21: val_recall did not improve from 0.37333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8677 - recall: 0.2384 - val_loss: 0.7826 - val_recall: 0.2800\n",
      "Epoch 22/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.7700 - recall: 0.2151\n",
      "Epoch 22: val_recall improved from 0.37333 to 0.38667, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 23ms/step - loss: 0.7571 - recall: 0.2527 - val_loss: 0.7464 - val_recall: 0.3867\n",
      "Epoch 23/500\n",
      "21/26 [=======================>......] - ETA: 0s - loss: 0.6576 - recall: 0.2064\n",
      "Epoch 23: val_recall improved from 0.38667 to 0.56000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 21ms/step - loss: 0.6648 - recall: 0.2456 - val_loss: 0.7474 - val_recall: 0.5600\n",
      "Epoch 24/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.6251 - recall: 0.3000\n",
      "Epoch 24: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6305 - recall: 0.2883 - val_loss: 0.7167 - val_recall: 0.1867\n",
      "Epoch 25/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6067 - recall: 0.2965  \n",
      "Epoch 25: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6050 - recall: 0.2811 - val_loss: 0.8162 - val_recall: 0.3867\n",
      "Epoch 26/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7653 - recall: 0.2904\n",
      "Epoch 26: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7654 - recall: 0.2918 - val_loss: 0.7423 - val_recall: 0.3333\n",
      "Epoch 27/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6851 - recall: 0.2176\n",
      "Epoch 27: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6896 - recall: 0.1851 - val_loss: 0.7370 - val_recall: 0.1067\n",
      "Epoch 28/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.7328 - recall: 0.2900  \n",
      "Epoch 28: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7480 - recall: 0.2171 - val_loss: 0.7354 - val_recall: 0.1733\n",
      "Epoch 29/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6808 - recall: 0.1800\n",
      "Epoch 29: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6858 - recall: 0.1352 - val_loss: 0.7072 - val_recall: 0.2133\n",
      "Epoch 30/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6675 - recall: 0.2370\n",
      "Epoch 30: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6670 - recall: 0.1922 - val_loss: 0.8329 - val_recall: 0.0800\n",
      "Epoch 31/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.7976 - recall: 0.2300  \n",
      "Epoch 31: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7794 - recall: 0.2064 - val_loss: 0.7336 - val_recall: 0.1600\n",
      "Epoch 32/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.7013 - recall: 0.1637\n",
      "Epoch 32: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.7013 - recall: 0.1637 - val_loss: 0.7199 - val_recall: 0.0400\n",
      "Epoch 33/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6740 - recall: 0.1546  \n",
      "Epoch 33: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6859 - recall: 0.1281 - val_loss: 0.6961 - val_recall: 0.1867\n",
      "Epoch 34/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6580 - recall: 0.1220\n",
      "Epoch 34: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6548 - recall: 0.1139 - val_loss: 0.6891 - val_recall: 0.0400\n",
      "Epoch 35/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6452 - recall: 0.1765\n",
      "Epoch 35: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6433 - recall: 0.1744 - val_loss: 0.6971 - val_recall: 0.0267\n",
      "Epoch 36/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6699 - recall: 0.1815\n",
      "Epoch 36: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6699 - recall: 0.1815 - val_loss: 0.7839 - val_recall: 0.1200\n",
      "Epoch 37/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.7319 - recall: 0.2438  \n",
      "Epoch 37: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7339 - recall: 0.2527 - val_loss: 0.7397 - val_recall: 0.1067\n",
      "Epoch 38/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6859 - recall: 0.2347\n",
      "Epoch 38: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6739 - recall: 0.2491 - val_loss: 0.7892 - val_recall: 0.0400\n",
      "Epoch 39/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6745 - recall: 0.1805\n",
      "Epoch 39: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6766 - recall: 0.2776 - val_loss: 0.7216 - val_recall: 0.1867\n",
      "Epoch 40/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6402 - recall: 0.1815\n",
      "Epoch 40: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6402 - recall: 0.1815 - val_loss: 0.7169 - val_recall: 0.1600\n",
      "Epoch 41/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6142 - recall: 0.1895\n",
      "Epoch 41: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6147 - recall: 0.2028 - val_loss: 0.7137 - val_recall: 0.1600\n",
      "Epoch 42/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6061 - recall: 0.2098\n",
      "Epoch 42: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6097 - recall: 0.2028 - val_loss: 0.7170 - val_recall: 0.2267\n",
      "Epoch 43/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.5936 - recall: 0.2339\n",
      "Epoch 43: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.5989 - recall: 0.2313 - val_loss: 0.7196 - val_recall: 0.1733\n",
      "Epoch 44/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5899 - recall: 0.2268\n",
      "Epoch 44: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5901 - recall: 0.2420 - val_loss: 0.7225 - val_recall: 0.2933\n",
      "Epoch 45/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.5580 - recall: 0.2474\n",
      "Epoch 45: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5761 - recall: 0.2705 - val_loss: 0.7212 - val_recall: 0.1733\n",
      "Epoch 46/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5709 - recall: 0.2454\n",
      "Epoch 46: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5679 - recall: 0.2562 - val_loss: 0.7208 - val_recall: 0.1867\n",
      "Epoch 47/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.5746 - recall: 0.3488\n",
      "Epoch 47: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5746 - recall: 0.3488 - val_loss: 0.7326 - val_recall: 0.1200\n",
      "Epoch 48/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.5549 - recall: 0.3132\n",
      "Epoch 48: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5549 - recall: 0.3132 - val_loss: 0.7420 - val_recall: 0.0667\n",
      "Epoch 49/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.5448 - recall: 0.2989\n",
      "Epoch 49: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5448 - recall: 0.2989 - val_loss: 0.7324 - val_recall: 0.2667\n",
      "Epoch 50/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.5558 - recall: 0.2937\n",
      "Epoch 50: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.5565 - recall: 0.2954 - val_loss: 0.7313 - val_recall: 0.1867\n",
      "Epoch 51/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.7124 - recall: 0.3488\n",
      "Epoch 51: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7124 - recall: 0.3488 - val_loss: 0.8252 - val_recall: 0.1600\n",
      "Epoch 52/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7023 - recall: 0.1704\n",
      "Epoch 52: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6996 - recall: 0.1673 - val_loss: 0.7427 - val_recall: 0.0533\n",
      "Epoch 53/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6715 - recall: 0.1298  \n",
      "Epoch 53: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6641 - recall: 0.1246 - val_loss: 0.7078 - val_recall: 0.0933\n",
      "Epoch 54/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.7089 - recall: 0.1538  \n",
      "Epoch 54: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6981 - recall: 0.1423 - val_loss: 0.7233 - val_recall: 0.1333\n",
      "Epoch 55/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6798 - recall: 0.0913  \n",
      "Epoch 55: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6682 - recall: 0.1103 - val_loss: 0.6941 - val_recall: 0.0800\n",
      "Epoch 56/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6736 - recall: 0.1102\n",
      "Epoch 56: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6763 - recall: 0.1246 - val_loss: 0.6924 - val_recall: 0.1867\n",
      "Epoch 57/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6663 - recall: 0.1284\n",
      "Epoch 57: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6542 - recall: 0.1317 - val_loss: 0.6942 - val_recall: 0.0000e+00\n",
      "Epoch 58/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6840 - recall: 0.1866\n",
      "Epoch 58: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6892 - recall: 0.1779 - val_loss: 0.8685 - val_recall: 0.0133\n",
      "Epoch 59/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7590 - recall: 0.2364\n",
      "Epoch 59: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7531 - recall: 0.2420 - val_loss: 0.7521 - val_recall: 0.0933\n",
      "Epoch 60/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7136 - recall: 0.1704\n",
      "Epoch 60: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7163 - recall: 0.1708 - val_loss: 0.7216 - val_recall: 0.2000\n",
      "Epoch 61/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6598 - recall: 0.1094\n",
      "Epoch 61: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6690 - recall: 0.0961 - val_loss: 0.6906 - val_recall: 0.1467\n",
      "Epoch 62/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6498 - recall: 0.1897\n",
      "Epoch 62: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6595 - recall: 0.2313 - val_loss: 0.6858 - val_recall: 0.1333\n",
      "Epoch 63/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6432 - recall: 0.1024\n",
      "Epoch 63: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6411 - recall: 0.1423 - val_loss: 0.7138 - val_recall: 0.0000e+00\n",
      "Epoch 64/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.7423 - recall: 0.1779\n",
      "Epoch 64: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7423 - recall: 0.1779 - val_loss: 0.6779 - val_recall: 0.1333\n",
      "Epoch 65/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6813 - recall: 0.1993\n",
      "Epoch 65: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6813 - recall: 0.1993 - val_loss: 0.7204 - val_recall: 0.0000e+00\n",
      "Epoch 66/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7210 - recall: 0.2370\n",
      "Epoch 66: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7220 - recall: 0.2278 - val_loss: 0.7429 - val_recall: 0.0000e+00\n",
      "Epoch 67/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.7067 - recall: 0.0600  \n",
      "Epoch 67: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7018 - recall: 0.1566 - val_loss: 0.6502 - val_recall: 0.0000e+00\n",
      "Epoch 68/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6552 - recall: 0.1635  \n",
      "Epoch 68: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6562 - recall: 0.1210 - val_loss: 0.6512 - val_recall: 0.1600\n",
      "Epoch 69/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6397 - recall: 0.1400\n",
      "Epoch 69: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6494 - recall: 0.1139 - val_loss: 0.6444 - val_recall: 0.0933\n",
      "Epoch 70/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6412 - recall: 0.0296  \n",
      "Epoch 70: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6426 - recall: 0.0285 - val_loss: 0.6473 - val_recall: 0.1200\n",
      "Epoch 71/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6495 - recall: 0.1055\n",
      "Epoch 71: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6454 - recall: 0.1032 - val_loss: 0.6889 - val_recall: 0.0000e+00\n",
      "Epoch 72/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6301 - recall: 0.0302  \n",
      "Epoch 72: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6366 - recall: 0.0391 - val_loss: 0.6455 - val_recall: 0.0267\n",
      "Epoch 73/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6769 - recall: 0.0996\n",
      "Epoch 73: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6769 - recall: 0.0996 - val_loss: 0.7003 - val_recall: 0.3733\n",
      "Epoch 74/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6658 - recall: 0.1354\n",
      "Epoch 74: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6676 - recall: 0.1139 - val_loss: 0.6693 - val_recall: 0.0800\n",
      "Epoch 75/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7132 - recall: 0.1573\n",
      "Epoch 75: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7163 - recall: 0.1495 - val_loss: 0.7893 - val_recall: 0.0000e+00\n",
      "Epoch 76/500\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.6840 - recall: 0.1385\n",
      "Epoch 76: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6826 - recall: 0.1281 - val_loss: 0.6762 - val_recall: 0.0000e+00\n",
      "Epoch 77/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6582 - recall: 0.1107\n",
      "Epoch 77: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6586 - recall: 0.1139 - val_loss: 0.6689 - val_recall: 0.0267\n",
      "Epoch 78/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6701 - recall: 0.0641  \n",
      "Epoch 78: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6701 - recall: 0.0641 - val_loss: 0.7291 - val_recall: 0.4667\n",
      "Epoch 79/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6789 - recall: 0.1172\n",
      "Epoch 79: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6739 - recall: 0.1139 - val_loss: 0.6924 - val_recall: 0.0000e+00\n",
      "Epoch 80/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6887 - recall: 0.2015\n",
      "Epoch 80: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6900 - recall: 0.2206 - val_loss: 0.6774 - val_recall: 0.0533\n",
      "Epoch 81/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6519 - recall: 0.0036    \n",
      "Epoch 81: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6519 - recall: 0.0036 - val_loss: 0.6804 - val_recall: 0.0800\n",
      "Epoch 82/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.6467 - recall: 0.0229  \n",
      "Epoch 82: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6449 - recall: 0.0178 - val_loss: 0.6869 - val_recall: 0.0000e+00\n",
      "Epoch 83/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6458 - recall: 0.0036\n",
      "Epoch 83: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6458 - recall: 0.0036 - val_loss: 0.6679 - val_recall: 0.0000e+00\n",
      "Epoch 84/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6488 - recall: 0.0192    \n",
      "Epoch 84: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6459 - recall: 0.0142 - val_loss: 0.6729 - val_recall: 0.0400\n",
      "Epoch 85/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6721 - recall: 0.0507  \n",
      "Epoch 85: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6490 - recall: 0.0391 - val_loss: 0.6668 - val_recall: 0.0000e+00\n",
      "Epoch 86/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6410 - recall: 0.0097    \n",
      "Epoch 86: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6428 - recall: 0.0071 - val_loss: 0.6721 - val_recall: 0.0000e+00\n",
      "Epoch 87/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6393 - recall: 0.0145  \n",
      "Epoch 87: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6371 - recall: 0.0107 - val_loss: 0.6664 - val_recall: 0.0000e+00\n",
      "Epoch 88/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6312 - recall: 0.0048  \n",
      "Epoch 88: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6303 - recall: 0.0036 - val_loss: 0.6672 - val_recall: 0.0000e+00\n",
      "Epoch 89/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.6397 - recall: 0.0651    \n",
      "Epoch 89: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6399 - recall: 0.0534 - val_loss: 0.6693 - val_recall: 0.0267\n",
      "Epoch 90/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6886 - recall: 0.2976\n",
      "Epoch 90: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6957 - recall: 0.2171 - val_loss: 0.6701 - val_recall: 0.0400\n",
      "Epoch 91/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.6453 - recall: 0.1531\n",
      "Epoch 91: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6472 - recall: 0.1139 - val_loss: 0.6668 - val_recall: 0.0000e+00\n",
      "Epoch 92/500\n",
      "20/26 [======================>.......] - ETA: 0s - loss: 0.6608 - recall: 0.1372  \n",
      "Epoch 92: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6703 - recall: 0.1388 - val_loss: 0.7676 - val_recall: 0.0000e+00\n",
      "Epoch 93/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6581 - recall: 0.0927  \n",
      "Epoch 93: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6587 - recall: 0.1495 - val_loss: 0.6683 - val_recall: 0.0000e+00\n",
      "Epoch 94/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6238 - recall: 0.0258\n",
      "Epoch 94: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6395 - recall: 0.0783 - val_loss: 0.6710 - val_recall: 0.0533\n",
      "Epoch 95/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6216 - recall: 0.0151    \n",
      "Epoch 95: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6240 - recall: 0.0320 - val_loss: 0.6707 - val_recall: 0.0000e+00\n",
      "Epoch 96/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6311 - recall: 0.0676\n",
      "Epoch 96: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6311 - recall: 0.0676 - val_loss: 0.6656 - val_recall: 0.0000e+00\n",
      "Epoch 97/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6281 - recall: 0.0534\n",
      "Epoch 97: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6281 - recall: 0.0534 - val_loss: 0.6757 - val_recall: 0.0000e+00\n",
      "Epoch 98/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6264 - recall: 0.0366\n",
      "Epoch 98: val_recall did not improve from 0.56000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6235 - recall: 0.0356 - val_loss: 0.6927 - val_recall: 0.0000e+00\n",
      "Epoch 99/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6988 - recall: 0.1459\n",
      "Epoch 99: val_recall improved from 0.56000 to 0.96000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 26ms/step - loss: 0.6988 - recall: 0.1459 - val_loss: 0.9605 - val_recall: 0.9600\n",
      "Epoch 100/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.7205 - recall: 0.3080\n",
      "Epoch 100: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.7129 - recall: 0.2740 - val_loss: 0.6762 - val_recall: 0.0000e+00\n",
      "Epoch 101/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6553 - recall: 0.0693\n",
      "Epoch 101: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6509 - recall: 0.0676 - val_loss: 0.6724 - val_recall: 0.0000e+00\n",
      "Epoch 102/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6451 - recall: 0.0112\n",
      "Epoch 102: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6490 - recall: 0.0107 - val_loss: 0.6702 - val_recall: 0.0000e+00\n",
      "Epoch 103/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6453 - recall: 0.0320\n",
      "Epoch 103: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6453 - recall: 0.0320 - val_loss: 0.7007 - val_recall: 0.0000e+00\n",
      "Epoch 104/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6522 - recall: 0.0107\n",
      "Epoch 104: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6522 - recall: 0.0107 - val_loss: 0.6955 - val_recall: 0.0000e+00\n",
      "Epoch 105/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6448 - recall: 0.0075\n",
      "Epoch 105: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6476 - recall: 0.0071 - val_loss: 0.6551 - val_recall: 0.0000e+00\n",
      "Epoch 106/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6354 - recall: 0.0074\n",
      "Epoch 106: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6353 - recall: 0.0071 - val_loss: 0.6635 - val_recall: 0.0000e+00\n",
      "Epoch 107/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6480 - recall: 0.0000e+00\n",
      "Epoch 107: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6480 - recall: 0.0000e+00 - val_loss: 0.6590 - val_recall: 0.0000e+00\n",
      "Epoch 108/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6490 - recall: 0.0142  \n",
      "Epoch 108: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6490 - recall: 0.0142 - val_loss: 0.6581 - val_recall: 0.0533\n",
      "Epoch 109/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6477 - recall: 0.0142  \n",
      "Epoch 109: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6477 - recall: 0.0142 - val_loss: 0.6509 - val_recall: 0.0133\n",
      "Epoch 110/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6533 - recall: 0.0819    \n",
      "Epoch 110: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6533 - recall: 0.0819 - val_loss: 0.6872 - val_recall: 0.0000e+00\n",
      "Epoch 111/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6531 - recall: 0.0350  \n",
      "Epoch 111: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6715 - recall: 0.1032 - val_loss: 0.6540 - val_recall: 0.1600\n",
      "Epoch 112/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6602 - recall: 0.0428\n",
      "Epoch 112: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6646 - recall: 0.0320 - val_loss: 0.6737 - val_recall: 0.1600\n",
      "Epoch 113/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6849 - recall: 0.2000\n",
      "Epoch 113: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6743 - recall: 0.1530 - val_loss: 0.6711 - val_recall: 0.0000e+00\n",
      "Epoch 114/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6452 - recall: 0.0112    \n",
      "Epoch 114: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6471 - recall: 0.0107 - val_loss: 0.6750 - val_recall: 0.1067\n",
      "Epoch 115/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6550 - recall: 0.0152  \n",
      "Epoch 115: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6535 - recall: 0.0107 - val_loss: 0.6587 - val_recall: 0.0133\n",
      "Epoch 116/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6482 - recall: 0.0000e+00\n",
      "Epoch 116: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6465 - recall: 0.0036 - val_loss: 0.6661 - val_recall: 0.0000e+00\n",
      "Epoch 117/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6490 - recall: 0.0146    \n",
      "Epoch 117: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6558 - recall: 0.0107 - val_loss: 0.6627 - val_recall: 0.0000e+00\n",
      "Epoch 118/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6685 - recall: 0.0498\n",
      "Epoch 118: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6685 - recall: 0.0498 - val_loss: 0.6601 - val_recall: 0.0000e+00\n",
      "Epoch 119/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6609 - recall: 0.0816    \n",
      "Epoch 119: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6643 - recall: 0.0712 - val_loss: 0.6572 - val_recall: 0.0000e+00\n",
      "Epoch 120/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6630 - recall: 0.0747\n",
      "Epoch 120: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6630 - recall: 0.0747 - val_loss: 0.6679 - val_recall: 0.0000e+00\n",
      "Epoch 121/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6418 - recall: 0.0036\n",
      "Epoch 121: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6418 - recall: 0.0036 - val_loss: 0.6630 - val_recall: 0.0267\n",
      "Epoch 122/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6573 - recall: 0.0534\n",
      "Epoch 122: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6573 - recall: 0.0534 - val_loss: 0.6612 - val_recall: 0.0400\n",
      "Epoch 123/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6809 - recall: 0.1541\n",
      "Epoch 123: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6892 - recall: 0.1459 - val_loss: 0.6864 - val_recall: 0.0000e+00\n",
      "Epoch 124/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6543 - recall: 0.0412  \n",
      "Epoch 124: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6609 - recall: 0.0320 - val_loss: 0.6666 - val_recall: 0.0667\n",
      "Epoch 125/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6492 - recall: 0.0580\n",
      "Epoch 125: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6490 - recall: 0.0427 - val_loss: 0.6580 - val_recall: 0.0133\n",
      "Epoch 126/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6507 - recall: 0.0996\n",
      "Epoch 126: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6507 - recall: 0.0996 - val_loss: 0.6690 - val_recall: 0.0000e+00\n",
      "Epoch 127/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6931 - recall: 0.2051    \n",
      "Epoch 127: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6902 - recall: 0.1744 - val_loss: 0.6970 - val_recall: 0.3867\n",
      "Epoch 128/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6964 - recall: 0.1210\n",
      "Epoch 128: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6964 - recall: 0.1210 - val_loss: 0.7313 - val_recall: 0.8800\n",
      "Epoch 129/500\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.6603 - recall: 0.1128\n",
      "Epoch 129: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6611 - recall: 0.1032 - val_loss: 0.6609 - val_recall: 0.0000e+00\n",
      "Epoch 130/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6435 - recall: 0.0178\n",
      "Epoch 130: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6435 - recall: 0.0178 - val_loss: 0.6783 - val_recall: 0.0000e+00\n",
      "Epoch 131/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6517 - recall: 0.0147\n",
      "Epoch 131: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6494 - recall: 0.0142 - val_loss: 0.6572 - val_recall: 0.0000e+00\n",
      "Epoch 132/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6740 - recall: 0.0444\n",
      "Epoch 132: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6728 - recall: 0.0427 - val_loss: 0.6807 - val_recall: 0.0000e+00\n",
      "Epoch 133/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6291 - recall: 0.0000e+00\n",
      "Epoch 133: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6349 - recall: 0.0000e+00 - val_loss: 0.6616 - val_recall: 0.0000e+00\n",
      "Epoch 134/500\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.6311 - recall: 0.0000e+00\n",
      "Epoch 134: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6358 - recall: 0.0000e+00 - val_loss: 0.6563 - val_recall: 0.0000e+00\n",
      "Epoch 135/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6680 - recall: 0.0712  \n",
      "Epoch 135: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.6680 - recall: 0.0712 - val_loss: 0.7270 - val_recall: 0.0000e+00\n",
      "Epoch 136/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6468 - recall: 0.1134  \n",
      "Epoch 136: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6481 - recall: 0.0996 - val_loss: 0.6683 - val_recall: 0.0000e+00\n",
      "Epoch 137/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6533 - recall: 0.0187    \n",
      "Epoch 137: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6550 - recall: 0.0178 - val_loss: 0.6575 - val_recall: 0.0000e+00\n",
      "Epoch 138/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6425 - recall: 0.0000e+00\n",
      "Epoch 138: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6425 - recall: 0.0000e+00 - val_loss: 0.6612 - val_recall: 0.0000e+00\n",
      "Epoch 139/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6481 - recall: 0.0285\n",
      "Epoch 139: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6481 - recall: 0.0285 - val_loss: 0.6619 - val_recall: 0.0000e+00\n",
      "Epoch 140/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6322 - recall: 0.0000e+00\n",
      "Epoch 140: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6510 - recall: 0.0214 - val_loss: 0.6974 - val_recall: 0.5733\n",
      "Epoch 141/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6551 - recall: 0.0178\n",
      "Epoch 141: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6551 - recall: 0.0178 - val_loss: 0.6986 - val_recall: 0.6000\n",
      "Epoch 142/500\n",
      "18/26 [===================>..........] - ETA: 0s - loss: 0.6737 - recall: 0.1386\n",
      "Epoch 142: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6633 - recall: 0.0996 - val_loss: 0.6771 - val_recall: 0.0000e+00\n",
      "Epoch 143/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6650 - recall: 0.0545  \n",
      "Epoch 143: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6637 - recall: 0.0391 - val_loss: 0.6517 - val_recall: 0.0000e+00\n",
      "Epoch 144/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6561 - recall: 0.0000e+00\n",
      "Epoch 144: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6553 - recall: 0.0000e+00 - val_loss: 0.6751 - val_recall: 0.0000e+00\n",
      "Epoch 145/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6468 - recall: 0.0000e+00\n",
      "Epoch 145: val_recall did not improve from 0.96000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6460 - recall: 0.0000e+00 - val_loss: 0.6587 - val_recall: 0.0000e+00\n",
      "Epoch 146/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6412 - recall: 0.0000e+00\n",
      "Epoch 146: val_recall improved from 0.96000 to 0.97333, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_5)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 23ms/step - loss: 0.6412 - recall: 0.0000e+00 - val_loss: 0.7050 - val_recall: 0.9733\n",
      "Epoch 147/500\n",
      "19/26 [====================>.........] - ETA: 0s - loss: 0.6563 - recall: 0.0808\n",
      "Epoch 147: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6650 - recall: 0.0569 - val_loss: 0.6597 - val_recall: 0.0000e+00\n",
      "Epoch 148/500\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6666 - recall: 0.2135  \n",
      "Epoch 148: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6666 - recall: 0.2135 - val_loss: 0.7193 - val_recall: 0.0000e+00\n",
      "Epoch 149/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.6675 - recall: 0.0294\n",
      "Epoch 149: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6659 - recall: 0.0285 - val_loss: 0.7732 - val_recall: 0.0000e+00\n",
      "Epoch 150/500\n",
      "22/26 [========================>.....] - ETA: 0s - loss: 0.6678 - recall: 0.0571    \n",
      "Epoch 150: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.6757 - recall: 0.1388 - val_loss: 0.6640 - val_recall: 0.0000e+00\n",
      "Epoch 151/500\n",
      "22/26 [========================>.....] - ETA: 0s - loss: 0.6802 - recall: 0.1837  \n",
      "Epoch 151: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.6691 - recall: 0.1601 - val_loss: 0.6527 - val_recall: 0.0000e+00\n",
      "Epoch 152/500\n",
      "22/26 [========================>.....] - ETA: 0s - loss: 0.6457 - recall: 0.0000e+00\n",
      "Epoch 152: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.6455 - recall: 0.0000e+00 - val_loss: 0.6657 - val_recall: 0.0000e+00\n",
      "Epoch 153/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6473 - recall: 0.0000e+00\n",
      "Epoch 153: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6570 - recall: 0.0000e+00 - val_loss: 0.6637 - val_recall: 0.0000e+00\n",
      "Epoch 154/500\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 0.7000 - recall: 0.1926\n",
      "Epoch 154: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6990 - recall: 0.1851 - val_loss: 0.6649 - val_recall: 0.0000e+00\n",
      "Epoch 155/500\n",
      "24/26 [==========================>...] - ETA: 0s - loss: 0.6506 - recall: 0.0000e+00\n",
      "Epoch 155: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6495 - recall: 0.0000e+00 - val_loss: 0.6619 - val_recall: 0.0000e+00\n",
      "Epoch 156/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6468 - recall: 0.0281  \n",
      "Epoch 156: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.6490 - recall: 0.0249 - val_loss: 0.6642 - val_recall: 0.0000e+00\n",
      "Epoch 157/500\n",
      "23/26 [=========================>....] - ETA: 0s - loss: 0.6574 - recall: 0.0000e+00\n",
      "Epoch 157: val_recall did not improve from 0.97333\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.6481 - recall: 0.0000e+00 - val_loss: 0.6891 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "model_dense_5 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_5[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_5, y_train_rf_5, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8803 - recall: 0.4155\n",
      "Epoch 1: val_recall improved from -inf to 0.31250, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 47ms/step - loss: 0.8803 - recall: 0.4155 - val_loss: 0.6954 - val_recall: 0.3125\n",
      "Epoch 2/500\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.7046 - recall: 0.3156\n",
      "Epoch 2: val_recall did not improve from 0.31250\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.6951 - recall: 0.2711 - val_loss: 0.6597 - val_recall: 0.0156\n",
      "Epoch 3/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6263 - recall: 0.2731\n",
      "Epoch 3: val_recall did not improve from 0.31250\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6322 - recall: 0.2535 - val_loss: 0.6587 - val_recall: 0.0312\n",
      "Epoch 4/500\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.6164 - recall: 0.2869\n",
      "Epoch 4: val_recall improved from 0.31250 to 0.53125, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_6)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 28ms/step - loss: 0.6205 - recall: 0.3310 - val_loss: 0.6959 - val_recall: 0.5312\n",
      "Epoch 5/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5886 - recall: 0.3387\n",
      "Epoch 5: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5926 - recall: 0.3239 - val_loss: 0.6621 - val_recall: 0.1875\n",
      "Epoch 6/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5466 - recall: 0.4270\n",
      "Epoch 6: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5474 - recall: 0.4225 - val_loss: 0.6911 - val_recall: 0.1250\n",
      "Epoch 7/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5422 - recall: 0.3603\n",
      "Epoch 7: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5411 - recall: 0.3732 - val_loss: 0.7459 - val_recall: 0.4375\n",
      "Epoch 8/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5445 - recall: 0.4982\n",
      "Epoch 8: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5414 - recall: 0.5035 - val_loss: 0.7127 - val_recall: 0.4062\n",
      "Epoch 9/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4838 - recall: 0.5891\n",
      "Epoch 9: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4831 - recall: 0.5775 - val_loss: 0.7038 - val_recall: 0.0312\n",
      "Epoch 10/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4544 - recall: 0.5642\n",
      "Epoch 10: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4525 - recall: 0.5951 - val_loss: 0.7037 - val_recall: 0.1719\n",
      "Epoch 11/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4194 - recall: 0.6402\n",
      "Epoch 11: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4187 - recall: 0.6444 - val_loss: 0.7262 - val_recall: 0.2656\n",
      "Epoch 12/500\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.3775 - recall: 0.6990\n",
      "Epoch 12: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.3914 - recall: 0.6866 - val_loss: 0.7406 - val_recall: 0.2656\n",
      "Epoch 13/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3857 - recall: 0.6778\n",
      "Epoch 13: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3836 - recall: 0.6831 - val_loss: 0.7770 - val_recall: 0.4062\n",
      "Epoch 14/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3633 - recall: 0.7306\n",
      "Epoch 14: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3638 - recall: 0.7289 - val_loss: 0.7316 - val_recall: 0.2500\n",
      "Epoch 15/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3159 - recall: 0.7658\n",
      "Epoch 15: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3171 - recall: 0.7782 - val_loss: 0.7449 - val_recall: 0.1719\n",
      "Epoch 16/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3018 - recall: 0.8030\n",
      "Epoch 16: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3057 - recall: 0.8063 - val_loss: 0.7934 - val_recall: 0.1406\n",
      "Epoch 17/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3001 - recall: 0.7901\n",
      "Epoch 17: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2973 - recall: 0.7852 - val_loss: 0.8525 - val_recall: 0.0469\n",
      "Epoch 18/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2609 - recall: 0.8491\n",
      "Epoch 18: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2620 - recall: 0.8486 - val_loss: 0.8207 - val_recall: 0.0781\n",
      "Epoch 19/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2358 - recall: 0.8851\n",
      "Epoch 19: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2346 - recall: 0.8803 - val_loss: 0.8550 - val_recall: 0.0625\n",
      "Epoch 20/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.2109 - recall: 0.9055\n",
      "Epoch 20: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2091 - recall: 0.9014 - val_loss: 0.8107 - val_recall: 0.1406\n",
      "Epoch 21/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1765 - recall: 0.9306\n",
      "Epoch 21: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1763 - recall: 0.9296 - val_loss: 0.8506 - val_recall: 0.1719\n",
      "Epoch 22/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1365 - recall: 0.9767\n",
      "Epoch 22: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1356 - recall: 0.9789 - val_loss: 0.8407 - val_recall: 0.1562\n",
      "Epoch 23/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1145 - recall: 0.9920\n",
      "Epoch 23: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1164 - recall: 0.9859 - val_loss: 0.9169 - val_recall: 0.1250\n",
      "Epoch 24/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1025 - recall: 1.0000\n",
      "Epoch 24: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1034 - recall: 1.0000 - val_loss: 0.9136 - val_recall: 0.1406\n",
      "Epoch 25/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0841 - recall: 1.0000\n",
      "Epoch 25: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0888 - recall: 1.0000 - val_loss: 0.9156 - val_recall: 0.2344\n",
      "Epoch 26/500\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.0832 - recall: 1.0000\n",
      "Epoch 26: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0824 - recall: 1.0000 - val_loss: 0.9552 - val_recall: 0.1719\n",
      "Epoch 27/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0681 - recall: 1.0000\n",
      "Epoch 27: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0670 - recall: 1.0000 - val_loss: 0.9513 - val_recall: 0.2656\n",
      "Epoch 28/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0564 - recall: 1.0000\n",
      "Epoch 28: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0564 - recall: 1.0000 - val_loss: 0.9935 - val_recall: 0.2031\n",
      "Epoch 29/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0463 - recall: 1.0000\n",
      "Epoch 29: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0486 - recall: 1.0000 - val_loss: 0.9859 - val_recall: 0.1875\n",
      "Epoch 30/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0416 - recall: 1.0000\n",
      "Epoch 30: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0432 - recall: 1.0000 - val_loss: 1.0395 - val_recall: 0.1406\n",
      "Epoch 31/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0370 - recall: 1.0000\n",
      "Epoch 31: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0373 - recall: 1.0000 - val_loss: 1.0448 - val_recall: 0.1562\n",
      "Epoch 32/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0325 - recall: 1.0000\n",
      "Epoch 32: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0325 - recall: 1.0000 - val_loss: 1.0318 - val_recall: 0.2188\n",
      "Epoch 33/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0271 - recall: 1.0000\n",
      "Epoch 33: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0286 - recall: 1.0000 - val_loss: 1.0967 - val_recall: 0.1406\n",
      "Epoch 34/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0253 - recall: 1.0000\n",
      "Epoch 34: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0263 - recall: 1.0000 - val_loss: 1.0712 - val_recall: 0.2344\n",
      "Epoch 35/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0222 - recall: 1.0000\n",
      "Epoch 35: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0222 - recall: 1.0000 - val_loss: 1.1184 - val_recall: 0.1719\n",
      "Epoch 36/500\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.0196 - recall: 1.0000\n",
      "Epoch 36: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0200 - recall: 1.0000 - val_loss: 1.1023 - val_recall: 0.1875\n",
      "Epoch 37/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0180 - recall: 1.0000\n",
      "Epoch 37: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0180 - recall: 1.0000 - val_loss: 1.1495 - val_recall: 0.1562\n",
      "Epoch 38/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0164 - recall: 1.0000\n",
      "Epoch 38: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0164 - recall: 1.0000 - val_loss: 1.1727 - val_recall: 0.1250\n",
      "Epoch 39/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0146 - recall: 1.0000\n",
      "Epoch 39: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0160 - recall: 1.0000 - val_loss: 1.1345 - val_recall: 0.2344\n",
      "Epoch 40/500\n",
      "19/24 [======================>.......] - ETA: 0s - loss: 0.0147 - recall: 1.0000\n",
      "Epoch 40: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0145 - recall: 1.0000 - val_loss: 1.1981 - val_recall: 0.1250\n",
      "Epoch 41/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0123 - recall: 1.0000\n",
      "Epoch 41: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0123 - recall: 1.0000 - val_loss: 1.1912 - val_recall: 0.1562\n",
      "Epoch 42/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0117 - recall: 1.0000\n",
      "Epoch 42: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0115 - recall: 1.0000 - val_loss: 1.1962 - val_recall: 0.1875\n",
      "Epoch 43/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0105 - recall: 1.0000\n",
      "Epoch 43: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0105 - recall: 1.0000 - val_loss: 1.2180 - val_recall: 0.1562\n",
      "Epoch 44/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0096 - recall: 1.0000\n",
      "Epoch 44: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0096 - recall: 1.0000 - val_loss: 1.2283 - val_recall: 0.1719\n",
      "Epoch 45/500\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.0092 - recall: 1.0000\n",
      "Epoch 45: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0091 - recall: 1.0000 - val_loss: 1.2460 - val_recall: 0.1406\n",
      "Epoch 46/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0083 - recall: 1.0000\n",
      "Epoch 46: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0085 - recall: 1.0000 - val_loss: 1.2395 - val_recall: 0.1875\n",
      "Epoch 47/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0080 - recall: 1.0000\n",
      "Epoch 47: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0078 - recall: 1.0000 - val_loss: 1.2509 - val_recall: 0.1719\n",
      "Epoch 48/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0070 - recall: 1.0000\n",
      "Epoch 48: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0070 - recall: 1.0000 - val_loss: 1.2741 - val_recall: 0.1562\n",
      "Epoch 49/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0065 - recall: 1.0000\n",
      "Epoch 49: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0066 - recall: 1.0000 - val_loss: 1.2680 - val_recall: 0.1875\n",
      "Epoch 50/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0062 - recall: 1.0000\n",
      "Epoch 50: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0063 - recall: 1.0000 - val_loss: 1.2927 - val_recall: 0.1719\n",
      "Epoch 51/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0058 - recall: 1.0000\n",
      "Epoch 51: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0058 - recall: 1.0000 - val_loss: 1.2851 - val_recall: 0.2188\n",
      "Epoch 52/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0055 - recall: 1.0000\n",
      "Epoch 52: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0055 - recall: 1.0000 - val_loss: 1.3281 - val_recall: 0.1406\n",
      "Epoch 53/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0050 - recall: 1.0000\n",
      "Epoch 53: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0051 - recall: 1.0000 - val_loss: 1.3143 - val_recall: 0.1719\n",
      "Epoch 54/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0047 - recall: 1.0000\n",
      "Epoch 54: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0048 - recall: 1.0000 - val_loss: 1.3178 - val_recall: 0.1875\n",
      "Epoch 55/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0045 - recall: 1.0000\n",
      "Epoch 55: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0045 - recall: 1.0000 - val_loss: 1.3214 - val_recall: 0.1875\n",
      "Epoch 56/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0041 - recall: 1.0000\n",
      "Epoch 56: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0043 - recall: 1.0000 - val_loss: 1.3243 - val_recall: 0.2031\n",
      "Epoch 57/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0041 - recall: 1.0000\n",
      "Epoch 57: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0040 - recall: 1.0000 - val_loss: 1.3479 - val_recall: 0.2031\n",
      "Epoch 58/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0038 - recall: 1.0000\n",
      "Epoch 58: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0038 - recall: 1.0000 - val_loss: 1.3611 - val_recall: 0.1719\n",
      "Epoch 59/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0034 - recall: 1.0000\n",
      "Epoch 59: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0035 - recall: 1.0000 - val_loss: 1.3688 - val_recall: 0.1719\n",
      "Epoch 60/500\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 0.0033 - recall: 1.0000\n",
      "Epoch 60: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0034 - recall: 1.0000 - val_loss: 1.3624 - val_recall: 0.1875\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0032 - recall: 1.0000\n",
      "Epoch 61: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0032 - recall: 1.0000 - val_loss: 1.4021 - val_recall: 0.1562\n",
      "Epoch 62/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0029 - recall: 1.0000\n",
      "Epoch 62: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0030 - recall: 1.0000 - val_loss: 1.4042 - val_recall: 0.1562\n",
      "Epoch 63/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0029 - recall: 1.0000\n",
      "Epoch 63: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0029 - recall: 1.0000 - val_loss: 1.4094 - val_recall: 0.1719\n",
      "Epoch 64/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0027 - recall: 1.0000\n",
      "Epoch 64: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0027 - recall: 1.0000 - val_loss: 1.4154 - val_recall: 0.1719\n",
      "Epoch 65/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0026 - recall: 1.0000\n",
      "Epoch 65: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0026 - recall: 1.0000 - val_loss: 1.3980 - val_recall: 0.2969\n",
      "Epoch 66/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0025 - recall: 1.0000\n",
      "Epoch 66: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0025 - recall: 1.0000 - val_loss: 1.4200 - val_recall: 0.1719\n",
      "Epoch 67/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0023 - recall: 1.0000\n",
      "Epoch 67: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0023 - recall: 1.0000 - val_loss: 1.4265 - val_recall: 0.2188\n",
      "Epoch 68/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0022 - recall: 1.0000\n",
      "Epoch 68: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0022 - recall: 1.0000 - val_loss: 1.4462 - val_recall: 0.1719\n",
      "Epoch 69/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0020 - recall: 1.0000\n",
      "Epoch 69: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0021 - recall: 1.0000 - val_loss: 1.4609 - val_recall: 0.1562\n",
      "Epoch 70/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0018 - recall: 1.0000\n",
      "Epoch 70: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0020 - recall: 1.0000 - val_loss: 1.4714 - val_recall: 0.1562\n",
      "Epoch 71/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0019 - recall: 1.0000\n",
      "Epoch 71: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0019 - recall: 1.0000 - val_loss: 1.4702 - val_recall: 0.2188\n",
      "Epoch 72/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0019 - recall: 1.0000\n",
      "Epoch 72: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0018 - recall: 1.0000 - val_loss: 1.4785 - val_recall: 0.2031\n",
      "Epoch 73/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0017 - recall: 1.0000\n",
      "Epoch 73: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0017 - recall: 1.0000 - val_loss: 1.4830 - val_recall: 0.1719\n",
      "Epoch 74/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0016 - recall: 1.0000\n",
      "Epoch 74: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0016 - recall: 1.0000 - val_loss: 1.5043 - val_recall: 0.1562\n",
      "Epoch 75/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0015 - recall: 1.0000\n",
      "Epoch 75: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0016 - recall: 1.0000 - val_loss: 1.5174 - val_recall: 0.1562\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0015 - recall: 1.0000\n",
      "Epoch 76: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0015 - recall: 1.0000 - val_loss: 1.5141 - val_recall: 0.2031\n",
      "Epoch 77/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0014 - recall: 1.0000\n",
      "Epoch 77: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0014 - recall: 1.0000 - val_loss: 1.5300 - val_recall: 0.2188\n",
      "Epoch 78/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0013 - recall: 1.0000  \n",
      "Epoch 78: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0013 - recall: 1.0000 - val_loss: 1.5338 - val_recall: 0.1719\n",
      "Epoch 79/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0013 - recall: 1.0000\n",
      "Epoch 79: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0013 - recall: 1.0000 - val_loss: 1.5223 - val_recall: 0.2344\n",
      "Epoch 80/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0012 - recall: 1.0000\n",
      "Epoch 80: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0012 - recall: 1.0000 - val_loss: 1.5526 - val_recall: 0.2031\n",
      "Epoch 81/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0011 - recall: 1.0000\n",
      "Epoch 81: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0012 - recall: 1.0000 - val_loss: 1.5476 - val_recall: 0.2188\n",
      "Epoch 82/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0011 - recall: 1.0000\n",
      "Epoch 82: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0011 - recall: 1.0000 - val_loss: 1.5835 - val_recall: 0.1250\n",
      "Epoch 83/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0011 - recall: 1.0000\n",
      "Epoch 83: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0011 - recall: 1.0000 - val_loss: 1.5849 - val_recall: 0.1562\n",
      "Epoch 84/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 9.8320e-04 - recall: 1.0000\n",
      "Epoch 84: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0010 - recall: 1.0000 - val_loss: 1.5979 - val_recall: 0.1406\n",
      "Epoch 85/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 9.3750e-04 - recall: 1.0000\n",
      "Epoch 85: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 9.8058e-04 - recall: 1.0000 - val_loss: 1.5784 - val_recall: 0.2344\n",
      "Epoch 86/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 9.1822e-04 - recall: 1.0000\n",
      "Epoch 86: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 9.2987e-04 - recall: 1.0000 - val_loss: 1.5869 - val_recall: 0.2188\n",
      "Epoch 87/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 8.8553e-04 - recall: 1.0000\n",
      "Epoch 87: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 8.9045e-04 - recall: 1.0000 - val_loss: 1.6068 - val_recall: 0.1719\n",
      "Epoch 88/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 8.2663e-04 - recall: 1.0000\n",
      "Epoch 88: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 8.5261e-04 - recall: 1.0000 - val_loss: 1.6191 - val_recall: 0.1875\n",
      "Epoch 89/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 8.1530e-04 - recall: 1.0000\n",
      "Epoch 89: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 8.2060e-04 - recall: 1.0000 - val_loss: 1.6279 - val_recall: 0.1719\n",
      "Epoch 90/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 7.7536e-04 - recall: 1.0000\n",
      "Epoch 90: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 7.9556e-04 - recall: 1.0000 - val_loss: 1.6127 - val_recall: 0.2031\n",
      "Epoch 91/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 7.4590e-04 - recall: 1.0000\n",
      "Epoch 91: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 7.4913e-04 - recall: 1.0000 - val_loss: 1.6123 - val_recall: 0.2500\n",
      "Epoch 92/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 7.1971e-04 - recall: 1.0000\n",
      "Epoch 92: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 7.2757e-04 - recall: 1.0000 - val_loss: 1.6284 - val_recall: 0.2031\n",
      "Epoch 93/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 6.7945e-04 - recall: 1.0000\n",
      "Epoch 93: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 6.9272e-04 - recall: 1.0000 - val_loss: 1.6477 - val_recall: 0.2031\n",
      "Epoch 94/500\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 6.5558e-04 - recall: 1.0000\n",
      "Epoch 94: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 6.7047e-04 - recall: 1.0000 - val_loss: 1.6433 - val_recall: 0.2188\n",
      "Epoch 95/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 6.3595e-04 - recall: 1.0000\n",
      "Epoch 95: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 6.4024e-04 - recall: 1.0000 - val_loss: 1.6444 - val_recall: 0.2500\n",
      "Epoch 96/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 6.2693e-04 - recall: 1.0000\n",
      "Epoch 96: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 6.2693e-04 - recall: 1.0000 - val_loss: 1.6638 - val_recall: 0.1719\n",
      "Epoch 97/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 6.1716e-04 - recall: 1.0000\n",
      "Epoch 97: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 6.0257e-04 - recall: 1.0000 - val_loss: 1.6630 - val_recall: 0.2344\n",
      "Epoch 98/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 5.7522e-04 - recall: 1.0000\n",
      "Epoch 98: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 5.7099e-04 - recall: 1.0000 - val_loss: 1.6837 - val_recall: 0.1562\n",
      "Epoch 99/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 5.4705e-04 - recall: 1.0000\n",
      "Epoch 99: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 5.4974e-04 - recall: 1.0000 - val_loss: 1.6796 - val_recall: 0.2188\n",
      "Epoch 100/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 5.4230e-04 - recall: 1.0000\n",
      "Epoch 100: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 5.3069e-04 - recall: 1.0000 - val_loss: 1.6930 - val_recall: 0.1875\n",
      "Epoch 101/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 4.9559e-04 - recall: 1.0000\n",
      "Epoch 101: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 5.1246e-04 - recall: 1.0000 - val_loss: 1.6860 - val_recall: 0.2188\n",
      "Epoch 102/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 5.0301e-04 - recall: 1.0000\n",
      "Epoch 102: val_recall did not improve from 0.53125\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 4.9502e-04 - recall: 1.0000 - val_loss: 1.7119 - val_recall: 0.1875\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "model_dense_6 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_6[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_6, y_train_rf_6, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6735 - recall: 0.2388\n",
      "Epoch 1: val_recall improved from -inf to 0.00000, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 46ms/step - loss: 0.6735 - recall: 0.2388 - val_loss: 0.7093 - val_recall: 0.0000e+00\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6422 - recall: 0.1082  \n",
      "Epoch 2: val_recall improved from 0.00000 to 0.59459, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 27ms/step - loss: 0.6422 - recall: 0.1082 - val_loss: 0.7742 - val_recall: 0.5946\n",
      "Epoch 3/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.6385 - recall: 0.1211\n",
      "Epoch 3: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6425 - recall: 0.1157 - val_loss: 0.7421 - val_recall: 0.0135\n",
      "Epoch 4/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6182 - recall: 0.1765\n",
      "Epoch 4: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.6172 - recall: 0.1604 - val_loss: 0.7029 - val_recall: 0.0541\n",
      "Epoch 5/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6261 - recall: 0.2839\n",
      "Epoch 5: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6254 - recall: 0.2500 - val_loss: 0.6992 - val_recall: 0.0270\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5866 - recall: 0.2724\n",
      "Epoch 6: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5866 - recall: 0.2724 - val_loss: 0.7803 - val_recall: 0.0270\n",
      "Epoch 7/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5801 - recall: 0.2587\n",
      "Epoch 7: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5811 - recall: 0.2500 - val_loss: 0.7284 - val_recall: 0.0811\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5538 - recall: 0.3246\n",
      "Epoch 8: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5538 - recall: 0.3246 - val_loss: 0.7569 - val_recall: 0.0135\n",
      "Epoch 9/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.5373 - recall: 0.3840\n",
      "Epoch 9: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.5374 - recall: 0.3657 - val_loss: 0.7556 - val_recall: 0.0676\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5050 - recall: 0.4067\n",
      "Epoch 10: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5050 - recall: 0.4067 - val_loss: 0.7835 - val_recall: 0.0676\n",
      "Epoch 11/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.5187 - recall: 0.4751\n",
      "Epoch 11: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5130 - recall: 0.4701 - val_loss: 0.8654 - val_recall: 0.0000e+00\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5160 - recall: 0.5261\n",
      "Epoch 12: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5160 - recall: 0.5261 - val_loss: 0.8220 - val_recall: 0.0405\n",
      "Epoch 13/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.4761 - recall: 0.4818\n",
      "Epoch 13: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4733 - recall: 0.4813 - val_loss: 0.8063 - val_recall: 0.0946\n",
      "Epoch 14/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.4248 - recall: 0.5450\n",
      "Epoch 14: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4145 - recall: 0.5410 - val_loss: 0.8001 - val_recall: 0.2162\n",
      "Epoch 15/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3730 - recall: 0.6529\n",
      "Epoch 15: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3770 - recall: 0.6418 - val_loss: 0.8003 - val_recall: 0.1486\n",
      "Epoch 16/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.3386 - recall: 0.7265\n",
      "Epoch 16: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3436 - recall: 0.7239 - val_loss: 0.8274 - val_recall: 0.2568\n",
      "Epoch 17/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3078 - recall: 0.7364\n",
      "Epoch 17: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3085 - recall: 0.7388 - val_loss: 0.8575 - val_recall: 0.3514\n",
      "Epoch 18/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2802 - recall: 0.8172\n",
      "Epoch 18: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2802 - recall: 0.8172 - val_loss: 0.8761 - val_recall: 0.2568\n",
      "Epoch 19/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2615 - recall: 0.8060\n",
      "Epoch 19: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2615 - recall: 0.8060 - val_loss: 0.8908 - val_recall: 0.4189\n",
      "Epoch 20/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2714 - recall: 0.7910\n",
      "Epoch 20: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2714 - recall: 0.7910 - val_loss: 0.8714 - val_recall: 0.5405\n",
      "Epoch 21/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.2273 - recall: 0.8760\n",
      "Epoch 21: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2327 - recall: 0.8731 - val_loss: 0.9500 - val_recall: 0.3108\n",
      "Epoch 22/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1849 - recall: 0.9067\n",
      "Epoch 22: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1849 - recall: 0.9067 - val_loss: 0.9488 - val_recall: 0.4459\n",
      "Epoch 23/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.1664 - recall: 0.9086\n",
      "Epoch 23: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1768 - recall: 0.9104 - val_loss: 0.9816 - val_recall: 0.3514\n",
      "Epoch 24/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.1370 - recall: 0.9424\n",
      "Epoch 24: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1402 - recall: 0.9478 - val_loss: 0.9423 - val_recall: 0.3649\n",
      "Epoch 25/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1187 - recall: 0.9618\n",
      "Epoch 25: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1189 - recall: 0.9627 - val_loss: 1.0822 - val_recall: 0.2432\n",
      "Epoch 26/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1184 - recall: 0.9580\n",
      "Epoch 26: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1143 - recall: 0.9627 - val_loss: 1.1898 - val_recall: 0.1486\n",
      "Epoch 27/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1067 - recall: 0.9653\n",
      "Epoch 27: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1062 - recall: 0.9664 - val_loss: 1.0764 - val_recall: 0.3378\n",
      "Epoch 28/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0812 - recall: 0.9895\n",
      "Epoch 28: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0791 - recall: 0.9925 - val_loss: 1.1081 - val_recall: 0.3784\n",
      "Epoch 29/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0581 - recall: 0.9960\n",
      "Epoch 29: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0578 - recall: 0.9963 - val_loss: 1.1717 - val_recall: 0.2568\n",
      "Epoch 30/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0499 - recall: 0.9959\n",
      "Epoch 30: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0505 - recall: 0.9963 - val_loss: 1.1630 - val_recall: 0.3514\n",
      "Epoch 31/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0414 - recall: 1.0000\n",
      "Epoch 31: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0423 - recall: 1.0000 - val_loss: 1.1855 - val_recall: 0.2703\n",
      "Epoch 32/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0349 - recall: 1.0000\n",
      "Epoch 32: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0344 - recall: 1.0000 - val_loss: 1.2528 - val_recall: 0.2568\n",
      "Epoch 33/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0299 - recall: 1.0000\n",
      "Epoch 33: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0299 - recall: 1.0000 - val_loss: 1.2635 - val_recall: 0.3108\n",
      "Epoch 34/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0256 - recall: 1.0000\n",
      "Epoch 34: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0256 - recall: 1.0000 - val_loss: 1.2968 - val_recall: 0.2703\n",
      "Epoch 35/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0217 - recall: 1.0000\n",
      "Epoch 35: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0224 - recall: 1.0000 - val_loss: 1.3386 - val_recall: 0.2432\n",
      "Epoch 36/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0202 - recall: 1.0000\n",
      "Epoch 36: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0202 - recall: 1.0000 - val_loss: 1.3473 - val_recall: 0.3243\n",
      "Epoch 37/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0176 - recall: 1.0000\n",
      "Epoch 37: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0177 - recall: 1.0000 - val_loss: 1.3482 - val_recall: 0.2973\n",
      "Epoch 38/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0147 - recall: 1.0000\n",
      "Epoch 38: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0148 - recall: 1.0000 - val_loss: 1.3673 - val_recall: 0.3243\n",
      "Epoch 39/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0135 - recall: 1.0000\n",
      "Epoch 39: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0135 - recall: 1.0000 - val_loss: 1.4415 - val_recall: 0.2432\n",
      "Epoch 40/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0124 - recall: 1.0000\n",
      "Epoch 40: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0123 - recall: 1.0000 - val_loss: 1.4137 - val_recall: 0.2838\n",
      "Epoch 41/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0106 - recall: 1.0000\n",
      "Epoch 41: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0111 - recall: 1.0000 - val_loss: 1.4229 - val_recall: 0.2973\n",
      "Epoch 42/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0108 - recall: 1.0000\n",
      "Epoch 42: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0102 - recall: 1.0000 - val_loss: 1.4176 - val_recall: 0.3378\n",
      "Epoch 43/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0092 - recall: 1.0000\n",
      "Epoch 43: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0092 - recall: 1.0000 - val_loss: 1.4436 - val_recall: 0.2973\n",
      "Epoch 44/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0080 - recall: 1.0000\n",
      "Epoch 44: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0083 - recall: 1.0000 - val_loss: 1.4752 - val_recall: 0.2973\n",
      "Epoch 45/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0075 - recall: 1.0000\n",
      "Epoch 45: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0075 - recall: 1.0000 - val_loss: 1.4880 - val_recall: 0.3243\n",
      "Epoch 46/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0071 - recall: 1.0000\n",
      "Epoch 46: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0069 - recall: 1.0000 - val_loss: 1.5235 - val_recall: 0.2703\n",
      "Epoch 47/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0066 - recall: 1.0000\n",
      "Epoch 47: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0066 - recall: 1.0000 - val_loss: 1.5231 - val_recall: 0.2568\n",
      "Epoch 48/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0059 - recall: 1.0000\n",
      "Epoch 48: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0059 - recall: 1.0000 - val_loss: 1.5577 - val_recall: 0.2703\n",
      "Epoch 49/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0056 - recall: 1.0000\n",
      "Epoch 49: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0056 - recall: 1.0000 - val_loss: 1.5417 - val_recall: 0.2973\n",
      "Epoch 50/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0051 - recall: 1.0000\n",
      "Epoch 50: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0052 - recall: 1.0000 - val_loss: 1.5678 - val_recall: 0.2703\n",
      "Epoch 51/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0048 - recall: 1.0000\n",
      "Epoch 51: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0050 - recall: 1.0000 - val_loss: 1.6188 - val_recall: 0.2162\n",
      "Epoch 52/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0044 - recall: 1.0000\n",
      "Epoch 52: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0044 - recall: 1.0000 - val_loss: 1.6215 - val_recall: 0.2703\n",
      "Epoch 53/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0041 - recall: 1.0000\n",
      "Epoch 53: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0041 - recall: 1.0000 - val_loss: 1.6237 - val_recall: 0.2568\n",
      "Epoch 54/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0038 - recall: 1.0000\n",
      "Epoch 54: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0038 - recall: 1.0000 - val_loss: 1.6058 - val_recall: 0.2973\n",
      "Epoch 55/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 1.1259 - recall: 0.4760\n",
      "Epoch 55: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.1358 - recall: 0.4440 - val_loss: 0.7611 - val_recall: 0.4324\n",
      "Epoch 56/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7132 - recall: 0.3134\n",
      "Epoch 56: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7132 - recall: 0.3134 - val_loss: 0.7086 - val_recall: 0.4595\n",
      "Epoch 57/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.6029 - recall: 0.4016\n",
      "Epoch 57: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6014 - recall: 0.3918 - val_loss: 0.7796 - val_recall: 0.2027\n",
      "Epoch 58/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5304 - recall: 0.4664\n",
      "Epoch 58: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5304 - recall: 0.4664 - val_loss: 0.7580 - val_recall: 0.1081\n",
      "Epoch 59/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5017 - recall: 0.4813\n",
      "Epoch 59: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5017 - recall: 0.4813 - val_loss: 0.7394 - val_recall: 0.3514\n",
      "Epoch 60/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4383 - recall: 0.5784\n",
      "Epoch 60: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4383 - recall: 0.5784 - val_loss: 0.7623 - val_recall: 0.2162\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3991 - recall: 0.6269\n",
      "Epoch 61: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3991 - recall: 0.6269 - val_loss: 0.8518 - val_recall: 0.1892\n",
      "Epoch 62/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3639 - recall: 0.6642\n",
      "Epoch 62: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3639 - recall: 0.6642 - val_loss: 0.8352 - val_recall: 0.3514\n",
      "Epoch 63/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3115 - recall: 0.7695\n",
      "Epoch 63: val_recall did not improve from 0.59459\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3104 - recall: 0.7724 - val_loss: 0.9105 - val_recall: 0.1622\n",
      "Epoch 64/500\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 0.3110 - recall: 0.7431\n",
      "Epoch 64: val_recall improved from 0.59459 to 0.74324, saving model to /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)\n",
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/Parith/Documents/Coding/ml/SSEF_shortlisting/cluster_data_v1/best_dense_models(cluster_7)/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 27ms/step - loss: 0.3167 - recall: 0.7015 - val_loss: 1.0818 - val_recall: 0.7432\n",
      "Epoch 65/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.4204 - recall: 0.6836\n",
      "Epoch 65: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4265 - recall: 0.6642 - val_loss: 1.2052 - val_recall: 0.0405\n",
      "Epoch 66/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.3056 - recall: 0.7769\n",
      "Epoch 66: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3013 - recall: 0.7799 - val_loss: 0.9379 - val_recall: 0.1892\n",
      "Epoch 67/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2144 - recall: 0.8843\n",
      "Epoch 67: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2144 - recall: 0.8843 - val_loss: 0.9995 - val_recall: 0.2432\n",
      "Epoch 68/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.1653 - recall: 0.9300\n",
      "Epoch 68: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1657 - recall: 0.9328 - val_loss: 1.0247 - val_recall: 0.3514\n",
      "Epoch 69/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1407 - recall: 0.9433\n",
      "Epoch 69: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1423 - recall: 0.9478 - val_loss: 1.1386 - val_recall: 0.5946\n",
      "Epoch 70/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.1330 - recall: 0.9637\n",
      "Epoch 70: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1339 - recall: 0.9590 - val_loss: 1.0781 - val_recall: 0.2027\n",
      "Epoch 71/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.1077 - recall: 0.9637\n",
      "Epoch 71: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1075 - recall: 0.9627 - val_loss: 1.1485 - val_recall: 0.2432\n",
      "Epoch 72/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0778 - recall: 0.9959\n",
      "Epoch 72: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0764 - recall: 0.9963 - val_loss: 1.1118 - val_recall: 0.3649\n",
      "Epoch 73/500\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0663 - recall: 0.9914\n",
      "Epoch 73: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0645 - recall: 0.9925 - val_loss: 1.2094 - val_recall: 0.2838\n",
      "Epoch 74/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0510 - recall: 0.9960\n",
      "Epoch 74: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0529 - recall: 0.9963 - val_loss: 1.3046 - val_recall: 0.2297\n",
      "Epoch 75/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0425 - recall: 1.0000\n",
      "Epoch 75: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0425 - recall: 1.0000 - val_loss: 1.3205 - val_recall: 0.2027\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0368 - recall: 1.0000\n",
      "Epoch 76: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0368 - recall: 1.0000 - val_loss: 1.3120 - val_recall: 0.2838\n",
      "Epoch 77/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0283 - recall: 1.0000\n",
      "Epoch 77: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0283 - recall: 1.0000 - val_loss: 1.3160 - val_recall: 0.2838\n",
      "Epoch 78/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0245 - recall: 1.0000\n",
      "Epoch 78: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0256 - recall: 1.0000 - val_loss: 1.3735 - val_recall: 0.2568\n",
      "Epoch 79/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0233 - recall: 1.0000\n",
      "Epoch 79: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0233 - recall: 1.0000 - val_loss: 1.3555 - val_recall: 0.3108\n",
      "Epoch 80/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0199 - recall: 1.0000\n",
      "Epoch 80: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0197 - recall: 1.0000 - val_loss: 1.3585 - val_recall: 0.3243\n",
      "Epoch 81/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0166 - recall: 1.0000\n",
      "Epoch 81: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0167 - recall: 1.0000 - val_loss: 1.4106 - val_recall: 0.2973\n",
      "Epoch 82/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0146 - recall: 1.0000\n",
      "Epoch 82: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0144 - recall: 1.0000 - val_loss: 1.4252 - val_recall: 0.3108\n",
      "Epoch 83/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0124 - recall: 1.0000\n",
      "Epoch 83: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0125 - recall: 1.0000 - val_loss: 1.4317 - val_recall: 0.2973\n",
      "Epoch 84/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0112 - recall: 1.0000\n",
      "Epoch 84: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0117 - recall: 1.0000 - val_loss: 1.4494 - val_recall: 0.3514\n",
      "Epoch 85/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0104 - recall: 1.0000\n",
      "Epoch 85: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0104 - recall: 1.0000 - val_loss: 1.4945 - val_recall: 0.2568\n",
      "Epoch 86/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0096 - recall: 1.0000\n",
      "Epoch 86: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0096 - recall: 1.0000 - val_loss: 1.5132 - val_recall: 0.2703\n",
      "Epoch 87/500\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 0.0088 - recall: 1.0000\n",
      "Epoch 87: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0088 - recall: 1.0000 - val_loss: 1.5226 - val_recall: 0.2973\n",
      "Epoch 88/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0079 - recall: 1.0000\n",
      "Epoch 88: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0082 - recall: 1.0000 - val_loss: 1.5345 - val_recall: 0.2973\n",
      "Epoch 89/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0073 - recall: 1.0000\n",
      "Epoch 89: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0075 - recall: 1.0000 - val_loss: 1.5558 - val_recall: 0.2703\n",
      "Epoch 90/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0069 - recall: 1.0000\n",
      "Epoch 90: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0069 - recall: 1.0000 - val_loss: 1.5703 - val_recall: 0.2973\n",
      "Epoch 91/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0066 - recall: 1.0000\n",
      "Epoch 91: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0066 - recall: 1.0000 - val_loss: 1.5909 - val_recall: 0.2703\n",
      "Epoch 92/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0060 - recall: 1.0000\n",
      "Epoch 92: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0060 - recall: 1.0000 - val_loss: 1.6187 - val_recall: 0.2432\n",
      "Epoch 93/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0053 - recall: 1.0000\n",
      "Epoch 93: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0056 - recall: 1.0000 - val_loss: 1.6224 - val_recall: 0.2973\n",
      "Epoch 94/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0052 - recall: 1.0000\n",
      "Epoch 94: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0053 - recall: 1.0000 - val_loss: 1.6590 - val_recall: 0.2432\n",
      "Epoch 95/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0050 - recall: 1.0000\n",
      "Epoch 95: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0050 - recall: 1.0000 - val_loss: 1.6587 - val_recall: 0.2432\n",
      "Epoch 96/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0048 - recall: 1.0000\n",
      "Epoch 96: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0048 - recall: 1.0000 - val_loss: 1.6663 - val_recall: 0.2838\n",
      "Epoch 97/500\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.0044 - recall: 1.0000\n",
      "Epoch 97: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0044 - recall: 1.0000 - val_loss: 1.7006 - val_recall: 0.2432\n",
      "Epoch 98/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0041 - recall: 1.0000\n",
      "Epoch 98: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0041 - recall: 1.0000 - val_loss: 1.6948 - val_recall: 0.2973\n",
      "Epoch 99/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0038 - recall: 1.0000\n",
      "Epoch 99: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0040 - recall: 1.0000 - val_loss: 1.7224 - val_recall: 0.2432\n",
      "Epoch 100/500\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 0.0037 - recall: 1.0000\n",
      "Epoch 100: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0038 - recall: 1.0000 - val_loss: 1.7398 - val_recall: 0.2432\n",
      "Epoch 101/500\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0036 - recall: 1.0000\n",
      "Epoch 101: val_recall did not improve from 0.74324\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0036 - recall: 1.0000 - val_loss: 1.7337 - val_recall: 0.2703\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "\n",
    "model_dense_7 = Sequential([\n",
    "    Dense(input_shape = X_train_rf_7[0].shape, units = 700, activation = 'tanh'),\n",
    "    Dense(units = 800, activation = 'tanh'),\n",
    "    Dense(units = 1000, activation = 'tanh'),\n",
    "    Flatten(),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=100, restore_best_weights=True)\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint(verbose=1, filepath = model_path, monitor = 'val_recall', save_best_only = True, mode = 'max')\n",
    "model_dense_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(name = 'recall')])\n",
    "history = model_dense_1.fit(X_train_rf_7, y_train_rf_7, epochs = 500, validation_split = 0.2,callbacks=[early_stopping,best_model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
